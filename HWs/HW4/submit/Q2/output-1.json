[
  {
    "title": "tf.TensorShape  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/TensorShape",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nArgs\nRaises\nAttributes\nMethods\nas_list\nas_proto\nassert_has_rank\nassert_is_compatible_with\nassert_is_fully_defined\nassert_same_rank\nconcatenate\nexperimental_as_proto\nexperimental_from_proto\nexperimental_type_proto\nis_compatible_with\nis_fully_defined\nis_subtype_of\nmerge_with\nmost_specific_common_supertype\nmost_specific_compatible_shape\nnum_elements\nwith_rank\nwith_rank_at_least\nwith_rank_at_most\n__add__\n__bool__\n__concat__\n__eq__\n__getitem__\n__iter__\n__len__\n__nonzero__\n__radd__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\ntf.TensorShape \nbookmark_border\n\nView source on GitHub\n\nRepresents the shape of a Tensor.\n\nInherits From: TraceType\n\nView aliases\ntf.TensorShape(\n    dims\n)\n\nUsed in the notebooks\nUsed in the guide\tUsed in the tutorials\n\nTensorFlow 1.x vs TensorFlow 2 - Behaviors and APIs\nDTensor concepts\nWorking with RNNs\n\t\nDistributed Input\nTensorflow datasets from MongoDB collections\nt = tf.constant([[1,2,3],[4,5,6]])\nt.shape\nTensorShape([2, 3])\n\n\nTensorShape is the static shape representation of a Tensor. During eager execution a Tensor always has a fully specified shape but when tracing a tf.function it may be one of the following:\n\nFully-known shape: has a known number of dimensions and a known size for each dimension. e.g. TensorShape([16, 256])\nPartially-known shape: has a known number of dimensions, and an unknown size for one or more dimension. e.g. TensorShape([None, 256])\nUnknown shape: has an unknown number of dimensions, and an unknown size in all dimensions. e.g. TensorShape(None)\n\nDuring function tracing t.shape will return a TensorShape object representing the shape of Tensor as it is known during tracing. This static representation will be partially defined in cases where the exact shape depends on the values within the tensors. To get the dynamic representation, please use tf.shape(t) which will return Tensor representing the fully defined shape of t. This way, you can express logic that manipulates the shapes of tensors by building other tensors that depend on the dynamic shape of t.\n\nNote: tf.RaggedTensor.shape also returns a tf.TensorShape, the lengths of any ragged dimensions are unknown (None).\n\nFor example, this function prints the TensorShape' (t.shape), when you trace the function, and returns a tensor <a href=\"../tf/shape\"><code>tf.shape(t)</code></a> for given inputt`:\n\n@tf.function\ndef get_dynamic_shape(t):\n  print(\"tracing...\")\n  print(f\"static shape is {t.shape}\")\n  return tf.shape(t)\n\n\nJust calling the function traces it with a fully-specified static shape:\n\nresult = get_dynamic_shape(tf.constant([[1, 1, 1], [0, 0, 0]]))\ntracing...\nstatic shape is (2, 3)\nresult.numpy()\narray([2, 3], dtype=int32)\n\n\nBut tf.function can also trace the function with a partially specified (or even unspecified) shape:\n\ncf1 = get_dynamic_shape.get_concrete_function(tf.TensorSpec(\n                                              shape=[None, 2]))\ntracing...\nstatic shape is (None, 2)\ncf1(tf.constant([[1., 0],[1, 0],[1, 0]])).numpy()\narray([3, 2], dtype=int32)\n\ncf2 = get_dynamic_shape.get_concrete_function(tf.TensorSpec(shape=None))\ntracing...\nstatic shape is <unknown>\ncf2(tf.constant([[[[[1., 0]]]]])).numpy()\narray([1, 1, 1, 1, 2], dtype=int32)\n\n\nIf a tensor is produced by an operation of type \"Foo\", its shape may be inferred if there is a registered shape function for \"Foo\". See Shape functions for details of shape functions and how to register them. Alternatively, you may set the shape explicitly using tf.Tensor.ensure_shape.\n\nArgs\n\ndims\tA list of Dimensions, or None if the shape is unspecified.\n\nRaises\n\nTypeError\tIf dims cannot be converted to a list of dimensions.\n\nAttributes\n\ndims\tDeprecated. Returns list of dimensions for this shape.\n\nSuggest TensorShape.as_list instead.\n\n\nndims\tDeprecated accessor for rank.\nrank\tReturns the rank of this shape, or None if it is unspecified.\n\nMethods\nas_list\n\nView source\n\nas_list()\n\n\nReturns a list of integers or None for each dimension.\n\nReturns\nA list of integers or None for each dimension.\n\nRaises\nValueError\tIf self is an unknown shape with an unknown rank.\n\nas_proto\n\nView source\n\nas_proto()\n\n\nReturns this shape as a TensorShapeProto.\n\nassert_has_rank\n\nView source\n\nassert_has_rank(\n    rank\n)\n\n\nRaises an exception if self is not compatible with the given rank.\n\nArgs\nrank\tAn integer.\n\nRaises\nValueError\tIf self does not represent a shape with the given rank.\n\nassert_is_compatible_with\n\nView source\n\nassert_is_compatible_with(\n    other\n)\n\n\nRaises exception if self and other do not represent the same shape.\n\nThis method can be used to assert that there exists a shape that both self and other represent.\n\nArgs\nother\tAnother TensorShape.\n\nRaises\nValueError\tIf self and other do not represent the same shape.\n\nassert_is_fully_defined\n\nView source\n\nassert_is_fully_defined()\n\n\nRaises an exception if self is not fully defined in every dimension.\n\nRaises\nValueError\tIf self does not have a known value for every dimension.\n\nassert_same_rank\n\nView source\n\nassert_same_rank(\n    other\n)\n\n\nRaises an exception if self and other do not have compatible ranks.\n\nArgs\nother\tAnother TensorShape.\n\nRaises\nValueError\tIf self and other do not represent shapes with the same rank.\n\nconcatenate\n\nView source\n\nconcatenate(\n    other\n)\n\n\nReturns the concatenation of the dimension in self and other.\n\nNote: If either self or other is completely unknown, concatenation will discard information about the other shape. In future, we might support concatenation that preserves this information for use with slicing.\n\nArgs\nother\tAnother TensorShape.\n\nReturns\nA TensorShape whose dimensions are the concatenation of the dimensions in self and other.\n\nexperimental_as_proto\n\nView source\n\nexperimental_as_proto() -> tensor_shape_pb2.TensorShapeProto\n\n\nReturns a proto representation of the TensorShape instance.\n\nexperimental_from_proto\n\nView source\n\n@classmethod\nexperimental_from_proto(\n    proto: tensor_shape_pb2.TensorShapeProto\n) -> 'TensorShape'\n\n\nReturns a TensorShape instance based on the serialized proto.\n\nexperimental_type_proto\n\nView source\n\n@classmethod\nexperimental_type_proto() -> Type[tensor_shape_pb2.TensorShapeProto]\n\n\nReturns the type of proto associated with TensorShape serialization.\n\nis_compatible_with\n\nView source\n\nis_compatible_with(\n    other\n)\n\n\nReturns True iff self is compatible with other.\n\nTwo possibly-partially-defined shapes are compatible if there exists a fully-defined shape that both shapes can represent. Thus, compatibility allows the shape inference code to reason about partially-defined shapes. For example:\n\nTensorShape(None) is compatible with all shapes.\n\nTensorShape([None, None]) is compatible with all two-dimensional shapes, such as TensorShape([32, 784]), and also TensorShape(None). It is not compatible with, for example, TensorShape([None]) or TensorShape([None, None, None]).\n\nTensorShape([32, None]) is compatible with all two-dimensional shapes with size 32 in the 0th dimension, and also TensorShape([None, None]) and TensorShape(None). It is not compatible with, for example, TensorShape([32]), TensorShape([32, None, 1]) or TensorShape([64, None]).\n\nTensorShape([32, 784]) is compatible with itself, and also TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None, None]) and TensorShape(None). It is not compatible with, for example, TensorShape([32, 1, 784]) or TensorShape([None]).\n\nThe compatibility relation is reflexive and symmetric, but not transitive. For example, TensorShape([32, 784]) is compatible with TensorShape(None), and TensorShape(None) is compatible with TensorShape([4, 4]), but TensorShape([32, 784]) is not compatible with TensorShape([4, 4]).\n\nArgs\nother\tAnother TensorShape.\n\nReturns\nTrue iff self is compatible with other.\n\nis_fully_defined\n\nView source\n\nis_fully_defined()\n\n\nReturns True iff self is fully defined in every dimension.\n\nis_subtype_of\n\nView source\n\nis_subtype_of(\n    other: tf.types.experimental.TraceType\n) -> bool\n\n\nReturns True iff self is subtype of other.\n\nShape A is a subtype of shape B if shape B can successfully represent it:\n\nA TensorShape of any rank is a subtype of TensorShape(None).\n\nTensorShapes of equal ranks are covariant, i.e. TensorShape([A1, A2, ..]) is a subtype of TensorShape([B1, B2, ..]) iff An is a subtype of Bn.\n\nAn is subtype of Bn iff An == Bn or Bn is None.\n\nTensorShapes of different defined ranks have no subtyping relation.\n\nThe subtyping relation is reflexive and transitive, but not symmetric.\n\nSome examples:\n\nTensorShape([32, 784]) is a subtype of TensorShape(None), and TensorShape([4, 4]) is also a subtype of TensorShape(None) but TensorShape([32, 784]) and TensorShape([4, 4]) are not subtypes of each other.\n\nAll two-dimensional shapes are subtypes of TensorShape([None, None]), such as TensorShape([32, 784]). There is no subtype relationship with, for example, TensorShape([None]) or TensorShape([None, None, None]).\n\nTensorShape([32, None]) is also a subtype of TensorShape([None, None]) and TensorShape(None). It is not a subtype of, for example, TensorShape([32]), TensorShape([32, None, 1]), TensorShape([64, None]) or TensorShape([None, 32]).\n\nTensorShape([32, 784]) is a subtype of itself, and also TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None, None]) and TensorShape(None). It has no subtype relation with, for example, TensorShape([32, 1, 784]) or TensorShape([None]).\n\nArgs\nother\tAnother TensorShape.\n\nReturns\nTrue iff self is subtype of other.\n\nmerge_with\n\nView source\n\nmerge_with(\n    other\n)\n\n\nReturns a TensorShape combining the information in self and other.\n\nThe dimensions in self and other are merged element-wise, according to the rules below:\n\nDimension(n).merge_with(Dimension(None)) == Dimension(n)\nDimension(None).merge_with(Dimension(n)) == Dimension(n)\nDimension(None).merge_with(Dimension(None)) == Dimension(None)\n# raises ValueError for n != m\nDimension(n).merge_with(Dimension(m))\n\n\nts = tf.TensorShape([1,2]) ot1 = tf.TensorShape([1,2]) ts.merge_with(ot).as_list() [1,2]\n\not2 = tf.TensorShape([1,None]) ts.merge_with(ot2).as_list() [1,2]\n\not3 = tf.TensorShape([None, None]) ot3.merge_with(ot2).as_list() [1, None]\n\nArgs\nother\tAnother TensorShape.\n\nReturns\nA TensorShape containing the combined information of self and other.\n\nRaises\nValueError\tIf self and other are not compatible.\n\nmost_specific_common_supertype\n\nView source\n\nmost_specific_common_supertype(\n    others: Sequence[tf.types.experimental.TraceType]\n) -> Optional['TensorShape']\n\n\nReturns the most specific supertype TensorShape of self and others.\n\nTensorShape([None, 1]) is the most specific TensorShape supertyping both TensorShape([2, 1]) and TensorShape([5, 1]). Note that TensorShape(None) is also a supertype but it is not \"most specific\".\n\nTensorShape([1, 2, 3]) is the most specific TensorShape supertyping both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are other less specific TensorShapes that supertype above mentioned TensorShapes, e.g. TensorShape([1, 2, None]), TensorShape(None).\n\nTensorShape([None, None]) is the most specific TensorShape supertyping both TensorShape([2, None]) and TensorShape([None, 3]). As always, TensorShape(None) is also a supertype but not the most specific one.\n\nTensorShape(None) is the only TensorShape supertyping both TensorShape([1, 2, 3]) and TensorShape([1, 2]). In general, any two shapes that have different ranks will only have TensorShape(None) as a common supertype.\n\nTensorShape(None) is the only TensorShape supertyping both TensorShape([1, 2, 3]) and TensorShape(None). In general, the common supertype of any shape with TensorShape(None) is TensorShape(None).\n\nArgs\nothers\tSequence of TensorShape.\n\nReturns\nA TensorShape which is the most specific supertype shape of self and others. None if it does not exist.\n\nmost_specific_compatible_shape\n\nView source\n\nmost_specific_compatible_shape(\n    other\n) -> 'TensorShape'\n\n\nReturns the most specific TensorShape compatible with self and other.\n\nTensorShape([None, 1]) is the most specific TensorShape compatible with both TensorShape([2, 1]) and TensorShape([5, 1]). Note that TensorShape(None) is also compatible with above mentioned TensorShapes.\n\nTensorShape([1, 2, 3]) is the most specific TensorShape compatible with both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more less specific TensorShapes compatible with above mentioned TensorShapes, e.g. TensorShape([1, 2, None]), TensorShape(None).\n\nArgs\nother\tAnother TensorShape.\n\nReturns\nA TensorShape which is the most specific compatible shape of self and other.\n\nnum_elements\n\nView source\n\nnum_elements()\n\n\nReturns the total number of elements, or none for incomplete shapes.\n\nwith_rank\n\nView source\n\nwith_rank(\n    rank\n)\n\n\nReturns a shape based on self with the given rank.\n\nThis method promotes a completely unknown shape to one with a known rank.\n\nArgs\nrank\tAn integer.\n\nReturns\nA shape that is at least as specific as self with the given rank.\n\nRaises\nValueError\tIf self does not represent a shape with the given rank.\n\nwith_rank_at_least\n\nView source\n\nwith_rank_at_least(\n    rank\n)\n\n\nReturns a shape based on self with at least the given rank.\n\nArgs\nrank\tAn integer.\n\nReturns\nA shape that is at least as specific as self with at least the given rank.\n\nRaises\nValueError\tIf self does not represent a shape with at least the given rank.\n\nwith_rank_at_most\n\nView source\n\nwith_rank_at_most(\n    rank\n)\n\n\nReturns a shape based on self with at most the given rank.\n\nArgs\nrank\tAn integer.\n\nReturns\nA shape that is at least as specific as self with at most the given rank.\n\nRaises\nValueError\tIf self does not represent a shape with at most the given rank.\n\n__add__\n\nView source\n\n__add__(\n    other\n)\n\n__bool__\n\nView source\n\n__bool__()\n\n\nReturns True if this shape contains non-zero information.\n\n__concat__\n\nView source\n\n__concat__(\n    other\n)\n\n__eq__\n\nView source\n\n__eq__(\n    other\n)\n\n\nReturns True if self is equivalent to other.\n\nIt first tries to convert other to TensorShape. TypeError is thrown when the conversion fails. Otherwise, it compares each element in the TensorShape dimensions.\n\nTwo Fully known shapes, return True iff each element is equal.\n>>> t_a = tf.TensorShape([1,2])\n>>> a = [1, 2]\n>>> t_b = tf.TensorShape([1,2])\n>>> t_c = tf.TensorShape([1,2,3])\n>>> t_a.__eq__(a)\nTrue\n>>> t_a.__eq__(t_b)\nTrue\n>>> t_a.__eq__(t_c)\nFalse\n\nTwo Partially-known shapes, return True iff each element is equal.\n>>> p_a = tf.TensorShape([1,None])\n>>> p_b = tf.TensorShape([1,None])\n>>> p_c = tf.TensorShape([2,None])\n>>> p_a.__eq__(p_b)\nTrue\n>>> t_a.__eq__(p_a)\nFalse\n>>> p_a.__eq__(p_c)\nFalse\n\nTwo Unknown shape, return True.\n>>> unk_a = tf.TensorShape(None)\n>>> unk_b = tf.TensorShape(None)\n>>> unk_a.__eq__(unk_b)\nTrue\n>>> unk_a.__eq__(t_a)\nFalse\n\n\nArgs\nother\tA TensorShape or type that can be converted to TensorShape.\n\nReturns\nTrue if the dimensions are all equal.\n\nRaises\nTypeError if other can not be converted to TensorShape.\n\n__getitem__\n\nView source\n\n__getitem__(\n    key\n)\n\n\nReturns the value of a dimension or a shape, depending on the key.\n\nArgs\nkey\tIf key is an integer, returns the dimension at that index; otherwise if key is a slice, returns a TensorShape whose dimensions are those selected by the slice from self.\n\nReturns\nAn integer if key is an integer, or a TensorShape if key is a slice.\n\nRaises\nValueError\tIf key is a slice and self is completely unknown and the step is set.\n\n__iter__\n\nView source\n\n__iter__()\n\n\nReturns self.dims if the rank is known, otherwise raises ValueError.\n\n__len__\n\nView source\n\n__len__()\n\n\nReturns the rank of this shape, or raises ValueError if unspecified.\n\n__nonzero__\n\nView source\n\n__nonzero__()\n\n\nReturns True if this shape contains non-zero information.\n\n__radd__\n\nView source\n\n__radd__(\n    other\n)\n\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.TensorArraySpec  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/TensorArraySpec",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nArgs\nAttributes\nMethods\nexperimental_as_proto\nexperimental_from_proto\nexperimental_type_proto\nfrom_value\nis_compatible_with\nis_subtype_of\nmost_specific_common_supertype\nmost_specific_compatible_type\n__eq__\n__ne__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\ntf.TensorArraySpec \nbookmark_border\n\nView source on GitHub\n\nType specification for a tf.TensorArray.\n\nInherits From: TypeSpec, TraceType\n\nView aliases\ntf.TensorArraySpec(\n    element_shape=None,\n    dtype=tf.dtypes.float32,\n    dynamic_size=False,\n    infer_shape=True\n)\n\n\nArgs\n\nelement_shape\tThe shape of each element in the TensorArray.\ndtype\tData type of the TensorArray.\ndynamic_size\tWhether the TensorArray can grow past its initial size.\ninfer_shape\tWhether shape inference is enabled.\n\nAttributes\n\nvalue_type\t\n\nMethods\nexperimental_as_proto\n\nView source\n\nexperimental_as_proto() -> struct_pb2.TypeSpecProto\n\n\nReturns a proto representation of the TypeSpec instance.\n\nDo NOT override for custom non-TF types.\n\nexperimental_from_proto\n\nView source\n\n@classmethod\nexperimental_from_proto(\n    proto: struct_pb2.TypeSpecProto\n) -> 'TypeSpec'\n\n\nReturns a TypeSpec instance based on the serialized proto.\n\nDo NOT override for custom non-TF types.\n\nArgs\nproto\tProto generated using 'experimental_as_proto'.\n\nexperimental_type_proto\n\nView source\n\n@classmethod\nexperimental_type_proto() -> Type[struct_pb2.TypeSpecProto]\n\n\nReturns the type of proto associated with TypeSpec serialization.\n\nDo NOT override for custom non-TF types.\n\nfrom_value\n\nView source\n\n@staticmethod\nfrom_value(\n    value\n)\n\nis_compatible_with\n\nView source\n\nis_compatible_with(\n    other\n)\n\n\nReturns true if spec_or_value is compatible with this TypeSpec.\n\nPrefer using \"is_subtype_of\" and \"most_specific_common_supertype\" wherever possible.\n\nArgs\nspec_or_value\tA TypeSpec or TypeSpec associated value to compare against.\n\nis_subtype_of\n\nView source\n\nis_subtype_of(\n    other\n)\n\n\nReturns True if self is a subtype of other.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nother\tA TraceType object.\n\nmost_specific_common_supertype\n\nView source\n\nmost_specific_common_supertype(\n    others\n)\n\n\nReturns the most specific supertype of self and others.\n\nArgs\nothers\tA Sequence of TypeSpec.\n\nReturns None if a supertype does not exist.\n\nmost_specific_compatible_type\n\nView source\n\nmost_specific_compatible_type(\n    other: 'TypeSpec'\n) -> 'TypeSpec'\n\n\nReturns the most specific TypeSpec compatible with self and other. (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use most_specific_common_supertype instead.\n\nDeprecated. Please use most_specific_common_supertype instead. Do not override this function.\n\nArgs\nother\tA TypeSpec.\n\nRaises\nValueError\tIf there is no TypeSpec that is compatible with both self and other.\n\n__eq__\n\nView source\n\n__eq__(\n    other\n) -> bool\n\n\nReturn self==value.\n\n__ne__\n\nView source\n\n__ne__(\n    other\n) -> bool\n\n\nReturn self!=value.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.TensorArray  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/TensorArray",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nArgs\nRaises\nAttributes\nMethods\nclose\nconcat\ngather\ngrad\nidentity\nread\nscatter\nsize\nsplit\nstack\nunstack\nwrite\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.TensorArray \nbookmark_border\n\nView source on GitHub\n\nClass wrapping dynamic-sized, per-time-step, Tensor arrays.\n\nView aliases\ntf.TensorArray(\n    dtype,\n    size=None,\n    dynamic_size=None,\n    clear_after_read=None,\n    tensor_array_name=None,\n    handle=None,\n    flow=None,\n    infer_shape=True,\n    element_shape=None,\n    colocate_with_first_write_call=True,\n    name=None\n)\n\nUsed in the notebooks\nUsed in the guide\tUsed in the tutorials\n\nEffective Tensorflow 2\nBetter performance with tf.function\n\t\nPlaying CartPole with the Actor-Critic method\nModeling COVID-19 spread in Europe and the effect of interventions\nNeural machine translation with attention\nNeural machine translation with a Transformer and Keras\n\nThis class is meant to be used with dynamic iteration primitives such as while_loop and map_fn. It supports gradient back-propagation via special \"flow\" control flow dependencies.\n\nNote that although the array can be read multiple times and positions can be overwritten, behavior may be undefined when storing multiple references to the same array and clear_after_read is False. In particular, avoid using methods like concat() to convert an intermediate TensorArray to a Tensor, then further modifying the TensorArray, particularly if you need to backprop through it later.\n\nExample 1: Plain reading and writing.\n\nta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\nta = ta.write(0, 10)\nta = ta.write(1, 20)\nta = ta.write(2, 30)\n\nta.read(0)\n<tf.Tensor: shape=(), dtype=float32, numpy=10.0>\nta.read(1)\n<tf.Tensor: shape=(), dtype=float32, numpy=20.0>\nta.read(2)\n<tf.Tensor: shape=(), dtype=float32, numpy=30.0>\nta.stack()\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],\ndtype=float32)>\n\n\nExample 2: Fibonacci sequence algorithm that writes in a loop then returns.\n\n@tf.function\ndef fibonacci(n):\n  ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n  ta = ta.unstack([0., 1.])\n\n  for i in range(2, n):\n    ta = ta.write(i, ta.read(i - 1) + ta.read(i - 2))\n\n  return ta.stack()\n\nfibonacci(7)\n<tf.Tensor: shape=(7,), dtype=float32,\nnumpy=array([0., 1., 1., 2., 3., 5., 8.], dtype=float32)>\n\n\nExample 3: A simple loop interacting with a tf.Variable.\n\nv = tf.Variable(1)\n@tf.function\ndef f(x):\n  ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n  for i in tf.range(x):\n    v.assign_add(i)\n    ta = ta.write(i, v)\n  return ta.stack()\nf(5)\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 1,  2,  4,  7, 11],\ndtype=int32)>\n\n\nArgs\n\ndtype\t(required) data type of the TensorArray.\nsize\t(optional) int32 scalar Tensor: the size of the TensorArray. Required if handle is not provided.\ndynamic_size\t(optional) Python bool: If true, writes to the TensorArray can grow the TensorArray past its initial size. Default: False.\nclear_after_read\tBoolean (optional, default: True). If True, clear TensorArray values after reading them. This disables read-many semantics, but allows early release of memory.\ntensor_array_name\t(optional) Python string: the name of the TensorArray. This is used when creating the TensorArray handle. If this value is set, handle should be None.\nhandle\t(optional) A Tensor handle to an existing TensorArray. If this is set, tensor_array_name should be None. Only supported in graph mode.\nflow\t(optional) A float Tensor scalar coming from an existing TensorArray.flow. Only supported in graph mode.\ninfer_shape\t(optional, default: True) If True, shape inference is enabled. In this case, all elements must have the same shape.\nelement_shape\t(optional, default: None) A TensorShape object specifying the shape constraints of each of the elements of the TensorArray. Need not be fully defined.\ncolocate_with_first_write_call\tIf True, the TensorArray will be colocated on the same device as the Tensor used on its first write (write operations include write, unstack, and split). If False, the TensorArray will be placed on the device determined by the device context available during its initialization.\nname\tA name for the operation (optional).\n\nRaises\n\nValueError\tif both handle and tensor_array_name are provided.\nTypeError\tif handle is provided but is not a Tensor.\n\nAttributes\n\ndtype\tThe data type of this TensorArray.\ndynamic_size\tPython bool; if True the TensorArray can grow dynamically.\nelement_shape\tThe tf.TensorShape of elements in this TensorArray.\nflow\tThe flow Tensor forcing ops leading to this TensorArray state.\nhandle\tThe reference to the TensorArray.\n\nMethods\nclose\n\nView source\n\nclose(\n    name=None\n)\n\n\nClose the current TensorArray.\n\nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\nconcat\n\nView source\n\nconcat(\n    name=None\n)\n\n\nReturn the values in the TensorArray as a concatenated Tensor.\n\nAll of the values must have been written, their ranks must match, and and their shapes must all match for all dimensions except the first.\n\nArgs\nname\tA name for the operation (optional).\n\nReturns\nAll the tensors in the TensorArray concatenated into one tensor.\n\ngather\n\nView source\n\ngather(\n    indices, name=None\n)\n\n\nReturn selected values in the TensorArray as a packed Tensor.\n\nAll of selected values must have been written and their shapes must all match.\n\nArgs\nindices\tA 1-D Tensor taking values in [0, max_value). If the TensorArray is not dynamic, max_value=size().\nname\tA name for the operation (optional).\n\nReturns\nThe tensors in the TensorArray selected by indices, packed into one tensor.\n\ngrad\n\nView source\n\ngrad(\n    source, flow=None, name=None\n)\n\nidentity\n\nView source\n\nidentity()\n\n\nReturns a TensorArray with the same content and properties.\n\nReturns\nA new TensorArray object with flow that ensures the control dependencies from the contexts will become control dependencies for writes, reads, etc. Use this object for all subsequent operations.\n\nread\n\nView source\n\nread(\n    index, name=None\n)\n\n\nRead the value at location index in the TensorArray.\n\nArgs\nindex\t0-D. int32 tensor with the index to read from.\nname\tA name for the operation (optional).\n\nReturns\nThe tensor at index index.\n\nscatter\n\nView source\n\nscatter(\n    indices, value, name=None\n)\n\n\nScatter the values of a Tensor in specific indices of a TensorArray.\n\nArgs\nindices\tA 1-D Tensor taking values in [0, max_value). If the TensorArray is not dynamic, max_value=size().\nvalue\t(N+1)-D. Tensor of type dtype. The Tensor to unpack.\nname\tA name for the operation (optional).\n\nReturns\nA new TensorArray object with flow that ensures the scatter occurs. Use this object for all subsequent operations.\n\nRaises\nValueError\tif the shape inference fails.\n\nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\nsize\n\nView source\n\nsize(\n    name=None\n)\n\n\nReturn the size of the TensorArray.\n\nsplit\n\nView source\n\nsplit(\n    value, lengths, name=None\n)\n\n\nSplit the values of a Tensor into the TensorArray.\n\nArgs\nvalue\t(N+1)-D. Tensor of type dtype. The Tensor to split.\nlengths\t1-D. int32 vector with the lengths to use when splitting value along its first dimension.\nname\tA name for the operation (optional).\n\nReturns\nA new TensorArray object with flow that ensures the split occurs. Use this object for all subsequent operations.\n\nRaises\nValueError\tif the shape inference fails.\n\nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\nstack\n\nView source\n\nstack(\n    name=None\n)\n\n\nReturn the values in the TensorArray as a stacked Tensor.\n\nAll of the values must have been written and their shapes must all match. If input shapes have rank-R, then output shape will have rank-(R+1).\n\nFor example:\nta = tf.TensorArray(tf.int32, size=3)\nta = ta.write(0, tf.constant([1, 2]))\nta = ta.write(1, tf.constant([3, 4]))\nta = ta.write(2, tf.constant([5, 6]))\nta.stack()\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [5, 6]], dtype=int32)>\n\n\nArgs\nname\tA name for the operation (optional).\n\nReturns\nAll the tensors in the TensorArray stacked into one tensor.\n\nunstack\n\nView source\n\nunstack(\n    value, name=None\n)\n\n\nUnstack the values of a Tensor in the TensorArray.\n\nIf input value shapes have rank-R, then the output TensorArray will contain elements whose shapes are rank-(R-1).\n\nArgs\nvalue\t(N+1)-D. Tensor of type dtype. The Tensor to unstack.\nname\tA name for the operation (optional).\n\nReturns\nA new TensorArray object with flow that ensures the unstack occurs. Use this object for all subsequent operations.\n\nRaises\nValueError\tif the shape inference fails.\n\nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\nwrite\n\nView source\n\nwrite(\n    index, value, name=None\n)\n\n\nWrite value into index index of the TensorArray.\n\nArgs\nindex\t0-D. int32 scalar with the index to write to.\nvalue\tN-D. Tensor of type dtype. The Tensor to write to this index.\nname\tA name for the operation (optional).\n\nReturns\nA new TensorArray object with flow that ensures the write occurs. Use this object for all subsequent operations.\n\nRaises\nValueError\tif there are more writers than specified.\n\nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.Tensor  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/Tensor",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nAttributes\nMethods\neval\nexperimental_ref\nget_shape\nref\nset_shape\n__abs__\n__add__\n__and__\n__array__\n__bool__\n__div__\n__eq__\n__floordiv__\n__ge__\n__getitem__\n__gt__\n__invert__\n__iter__\n__le__\n__len__\n__lt__\n__matmul__\n__mod__\n__mul__\n__ne__\n__neg__\n__nonzero__\n__or__\n__pow__\n__radd__\n__rand__\n__rdiv__\n__rfloordiv__\n__rmatmul__\n__rmod__\n__rmul__\n__ror__\n__rpow__\n__rsub__\n__rtruediv__\n__rxor__\n__sub__\n__truediv__\n__xor__\nClass Variables\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\ntf.Tensor \nbookmark_border\n\nView source on GitHub\n\nA tf.Tensor represents a multidimensional array of elements.\n\nView aliases\n\nAll elements are of a single known data type.\n\nWhen writing a TensorFlow program, the main object that is manipulated and passed around is the tf.Tensor.\n\nA tf.Tensor has the following properties:\n\na single data type (float32, int32, or string, for example)\na shape\n\nTensorFlow supports eager execution and graph execution. In eager execution, operations are evaluated immediately. In graph execution, a computational graph is constructed for later evaluation.\n\nTensorFlow defaults to eager execution. In the example below, the matrix multiplication results are calculated immediately.\n\n# Compute some values using a Tensor\nc = tf.constant([[1.0, 2.0], [3.0, 4.0]])\nd = tf.constant([[1.0, 1.0], [0.0, 1.0]])\ne = tf.matmul(c, d)\nprint(e)\ntf.Tensor(\n[[1. 3.]\n [3. 7.]], shape=(2, 2), dtype=float32)\n\n\nNote that during eager execution, you may discover your Tensors are actually of type EagerTensor. This is an internal detail, but it does give you access to a useful function, numpy:\n\ntype(e)\n<class '...ops.EagerTensor'>\nprint(e.numpy())\n  [[1. 3.]\n   [3. 7.]]\n\n\nIn TensorFlow, tf.functions are a common way to define graph execution.\n\nA Tensor's shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known. In tf.function definitions, the shape may only be partially known.\n\nMost operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it's only possible to find the shape of a tensor at execution time.\n\nA number of specialized tensors are available: see tf.Variable, tf.constant, tf.placeholder, tf.sparse.SparseTensor, and tf.RaggedTensor.\n\nCaution: when constructing a tensor from a numpy array or pandas dataframe the underlying buffer may be re-used:\na = np.array([1, 2, 3])\nb = tf.constant(a)\na[0] = 4\nprint(b)  # tf.Tensor([4 2 3], shape=(3,), dtype=int64)\n\nNote: this is an implementation detail that is subject to change and users should not rely on this behaviour.\n\nFor more on Tensors, see the guide.\n\nAttributes\n\ndtype\tThe DType of elements in this tensor.\nname\t\n\n\nndim\t\n\n\nshape\tReturns a tf.TensorShape that represents the shape of this tensor.\n\nt = tf.constant([1,2,3,4,5])\nt.shape\nTensorShape([5])\n\n\ntf.Tensor.shape is equivalent to tf.Tensor.get_shape().\n\nIn a tf.function or when building a model using tf.keras.Input, they return the build-time shape of the tensor, which may be partially unknown.\n\nA tf.TensorShape is not a tensor. Use tf.shape(t) to get a tensor containing the shape, calculated at runtime.\n\nSee tf.Tensor.get_shape(), and tf.TensorShape for details and examples.\n\nMethods\neval\n\nView source\n\neval(\n    feed_dict=None, session=None\n)\n\n\nEvaluates this tensor in a Session.\n\nNote: If you are not using compat.v1 libraries, you should not need this, (or feed_dict or Session). In eager execution (or within tf.function) you do not need to call eval.\n\nCalling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.\n\nNote: Before invoking Tensor.eval(), its graph must have been launched in a session, and either a default session must be available, or session must be specified explicitly.\n\nArgs\nfeed_dict\tA dictionary that maps Tensor objects to feed values. See tf.Session.run for a description of the valid feed values.\nsession\t(Optional.) The Session to be used to evaluate this tensor. If none, the default session will be used.\n\nReturns\nA numpy array corresponding to the value of this tensor.\n\nexperimental_ref\n\nView source\n\nexperimental_ref()\n\n\nDEPRECATED FUNCTION\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use ref() instead.\nget_shape\n\nView source\n\nget_shape() -> tf.TensorShape\n\n\nReturns a tf.TensorShape that represents the shape of this tensor.\n\nIn eager execution the shape is always fully-known.\n\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nprint(a.shape)\n(2, 3)\n\n\ntf.Tensor.get_shape() is equivalent to tf.Tensor.shape.\n\nWhen executing in a tf.function or building a model using tf.keras.Input, Tensor.shape may return a partial shape (including None for unknown dimensions). See tf.TensorShape for more details.\n\ninputs = tf.keras.Input(shape = [10])\n# Unknown batch size\nprint(inputs.shape)\n(None, 10)\n\n\nThe shape is computed using shape inference functions that are registered for each tf.Operation.\n\nThe returned tf.TensorShape is determined at build time, without executing the underlying kernel. It is not a tf.Tensor. If you need a shape tensor, either convert the tf.TensorShape to a tf.constant, or use the tf.shape(tensor) function, which returns the tensor's shape at execution time.\n\nThis is useful for debugging and providing early errors. For example, when tracing a tf.function, no ops are being executed, shapes may be unknown (See the Concrete Functions Guide for details).\n\n@tf.function\ndef my_matmul(a, b):\n  result = a@b\n  # the `print` executes during tracing.\n  print(\"Result shape: \", result.shape)\n  return result\n\n\nThe shape inference functions propagate shapes to the extent possible:\n\nf = my_matmul.get_concrete_function(\n  tf.TensorSpec([None,3]),\n  tf.TensorSpec([3,5]))\nResult shape: (None, 5)\n\n\nTracing may fail if a shape missmatch can be detected:\n\ncf = my_matmul.get_concrete_function(\n  tf.TensorSpec([None,3]),\n  tf.TensorSpec([4,5]))\nTraceback (most recent call last):\n\nValueError: Dimensions must be equal, but are 3 and 4 for 'matmul' (op:\n'MatMul') with input shapes: [?,3], [4,5].\n\n\nIn some cases, the inferred shape may have unknown dimensions. If the caller has additional information about the values of these dimensions, tf.ensure_shape or Tensor.set_shape() can be used to augment the inferred shape.\n\n@tf.function\ndef my_fun(a):\n  a = tf.ensure_shape(a, [5, 5])\n  # the `print` executes during tracing.\n  print(\"Result shape: \", a.shape)\n  return a\n\ncf = my_fun.get_concrete_function(\n  tf.TensorSpec([None, None]))\nResult shape: (5, 5)\n\n\nReturns\nA tf.TensorShape representing the shape of this tensor.\n\nref\n\nView source\n\nref()\n\n\nReturns a hashable reference object to this Tensor.\n\nThe primary use case for this API is to put tensors in a set/dictionary. We can't put tensors in a set/dictionary as tensor.__hash__() is no longer available starting Tensorflow 2.0.\n\nThe following will raise an exception starting 2.0\n\nx = tf.constant(5)\ny = tf.constant(10)\nz = tf.constant(10)\ntensor_set = {x, y, z}\nTraceback (most recent call last):\n\nTypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\ntensor_dict = {x: 'five', y: 'ten'}\nTraceback (most recent call last):\n\nTypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n\n\nInstead, we can use tensor.ref().\n\ntensor_set = {x.ref(), y.ref(), z.ref()}\nx.ref() in tensor_set\nTrue\ntensor_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\ntensor_dict[y.ref()]\n'ten'\n\n\nAlso, the reference object provides .deref() function that returns the original Tensor.\n\nx = tf.constant(5)\nx.ref().deref()\n<tf.Tensor: shape=(), dtype=int32, numpy=5>\n\nset_shape\n\nView source\n\nset_shape(\n    shape\n)\n\n\nUpdates the shape of this tensor.\n\nNote: It is recommended to use tf.ensure_shape instead of Tensor.set_shape, because tf.ensure_shape provides better checking for programming errors and can create guarantees for compiler optimization.\n\nWith eager execution this operates as a shape assertion. Here the shapes match:\n\nt = tf.constant([[1,2,3]])\nt.set_shape([1, 3])\n\n\nPassing a None in the new shape allows any value for that axis:\n\nt.set_shape([1,None])\n\n\nAn error is raised if an incompatible shape is passed.\n\nt.set_shape([1,5])\nTraceback (most recent call last):\n\nValueError: Tensor's shape (1, 3) is not compatible with supplied\nshape [1, 5]\n\n\nWhen executing in a tf.function, or building a model using tf.keras.Input, Tensor.set_shape will merge the given shape with the current shape of this tensor, and set the tensor's shape to the merged value (see tf.TensorShape.merge_with for details):\n\nt = tf.keras.Input(shape=[None, None, 3])\nprint(t.shape)\n(None, None, None, 3)\n\n\nDimensions set to None are not updated:\n\nt.set_shape([None, 224, 224, None])\nprint(t.shape)\n(None, 224, 224, 3)\n\n\nThe main use case for this is to provide additional shape information that cannot be inferred from the graph alone.\n\nFor example if you know all the images in a dataset have shape [28,28,3] you can set it with tf.set_shape:\n\n@tf.function\ndef load_image(filename):\n  raw = tf.io.read_file(filename)\n  image = tf.image.decode_png(raw, channels=3)\n  # the `print` executes during tracing.\n  print(\"Initial shape: \", image.shape)\n  image.set_shape([28, 28, 3])\n  print(\"Final shape: \", image.shape)\n  return image\n\n\nTrace the function, see the Concrete Functions Guide for details.\n\ncf = load_image.get_concrete_function(\n    tf.TensorSpec([], dtype=tf.string))\nInitial shape:  (None, None, 3)\nFinal shape: (28, 28, 3)\n\n\nSimilarly the tf.io.parse_tensor function could return a tensor with any shape, even the tf.rank is unknown. If you know that all your serialized tensors will be 2d, set it with set_shape:\n\n@tf.function\ndef my_parse(string_tensor):\n  result = tf.io.parse_tensor(string_tensor, out_type=tf.float32)\n  # the `print` executes during tracing.\n  print(\"Initial shape: \", result.shape)\n  result.set_shape([None, None])\n  print(\"Final shape: \", result.shape)\n  return result\n\n\nTrace the function\n\nconcrete_parse = my_parse.get_concrete_function(\n    tf.TensorSpec([], dtype=tf.string))\nInitial shape:  <unknown>\nFinal shape:  (None, None)\n\nMake sure it works:\nt = tf.ones([5,3], dtype=tf.float32)\nserialized = tf.io.serialize_tensor(t)\nprint(serialized.dtype)\n<dtype: 'string'>\nprint(serialized.shape)\n()\nt2 = concrete_parse(serialized)\nprint(t2.shape)\n(5, 3)\n\nCaution: set_shape ensures that the applied shape is compatible with the existing shape, but it does not check at runtime. Setting incorrect shapes can result in inconsistencies between the statically-known graph and the runtime value of tensors. For runtime validation of the shape, use tf.ensure_shape instead. It also modifies the shape of the tensor.\n# Serialize a rank-3 tensor\nt = tf.ones([5,5,5], dtype=tf.float32)\nserialized = tf.io.serialize_tensor(t)\n# The function still runs, even though it `set_shape([None,None])`\nt2 = concrete_parse(serialized)\nprint(t2.shape)\n(5, 5, 5)\n\n\nArgs\nshape\tA TensorShape representing the shape of this tensor, a TensorShapeProto, a list, a tuple, or None.\n\nRaises\nValueError\tIf shape is not compatible with the current shape of this tensor.\n\n__abs__\n\nView source\n\n__abs__(\n    name=None\n)\n\n__add__\n\nView source\n\n__add__(\n    y\n)\n\n__and__\n\nView source\n\n__and__(\n    y\n)\n\n__array__\n\nView source\n\n__array__(\n    dtype=None\n)\n\n__bool__\n\nView source\n\n__bool__()\n\n\nDummy method to prevent a tensor from being used as a Python bool.\n\nThis overload raises a TypeError when the user inadvertently treats a Tensor as a boolean (most commonly in an if or while statement), in code that was not converted by AutoGraph. For example:\n\nif tf.constant(True):  # Will raise.\n  # ...\n\nif tf.constant(5) < tf.constant(7):  # Will raise.\n  # ...\n\n\nRaises\nTypeError.\n\n__div__\n\nView source\n\n__div__(\n    y\n)\n\n__eq__\n\nView source\n\n__eq__(\n    other\n)\n\n__floordiv__\n\nView source\n\n__floordiv__(\n    y\n)\n\n__ge__\n__ge__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x >= y) element-wise.\n\nNote: math.greater_equal supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5, 2, 5, 10])\ntf.math.greater_equal(x, y) ==> [True, True, True, False]\n\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5])\ntf.math.greater_equal(x, y) ==> [True, False, True, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__getitem__\n\nView source\n\n__getitem__(\n    slice_spec, var=None\n)\n\n\nOverload for Tensor.getitem.\n\nThis operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.\n\nSome useful examples:\n# Strip leading and trailing 2 elements\nfoo = tf.constant([1,2,3,4,5,6])\nprint(foo[2:-2])  # => [3,4]\n\n# Skip every other row and reverse the order of the columns\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[::2,::-1])  # => [[3,2,1], [9,8,7]]\n\n# Use scalar tensors as indices on both dimensions\nprint(foo[tf.constant(0), tf.constant(2)])  # => 3\n\n# Insert another dimension\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[tf.newaxis, :, :]) # => [[[1,2,3], [4,5,6], [7,8,9]]]\nprint(foo[:, tf.newaxis, :]) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\nprint(foo[:, :, tf.newaxis]) # => [[[1],[2],[3]], [[4],[5],[6]],\n[[7],[8],[9]]]\n\n# Ellipses (3 equivalent operations)\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[tf.newaxis, :, :])  # => [[[1,2,3], [4,5,6], [7,8,9]]]\nprint(foo[tf.newaxis, ...])  # => [[[1,2,3], [4,5,6], [7,8,9]]]\nprint(foo[tf.newaxis])  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n\n# Masks\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[foo > 2])  # => [3, 4, 5, 6, 7, 8, 9]\n\n\nNotes\n\n\ntf.newaxis is None as in NumPy.\nAn implicit ellipsis is placed at the end of the slice_spec\nNumPy advanced indexing is currently not supported.\n\nPurpose in the API\nThis method is exposed in TensorFlow's API so that library developers can register dispatching for Tensor.getitem to allow it to handle custom composite tensors & other custom objects.\n\nThe API symbol is not intended to be called by users directly and does appear in TensorFlow's generated documentation.\n\nArgs\ntensor\tAn tensor.Tensor object.\nslice_spec\tThe arguments to Tensor.getitem.\nvar\tIn the case of variable slice assignment, the Variable object to slice (i.e. tensor is the read-only view of this variable).\n\nReturns\nThe appropriate slice of \"tensor\", based on \"slice_spec\".\n\nRaises\nValueError\tIf a slice range is negative size.\nTypeError\tIf the slice indices aren't int, slice, ellipsis, tf.newaxis or scalar int32/int64 tensors.\n\n__gt__\n__gt__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x > y) element-wise.\n\nNote: math.greater supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==> [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==> [False, False, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__invert__\n\nView source\n\n__invert__(\n    name=None\n)\n\n__iter__\n\nView source\n\n__iter__()\n\n__le__\n__le__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x <= y) element-wise.\n\nNote: math.less_equal supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==> [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==> [True, True, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__len__\n\nView source\n\n__len__()\n\n__lt__\n__lt__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x < y) element-wise.\n\nNote: math.less supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==> [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==> [False, True, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__matmul__\n\nView source\n\n__matmul__(\n    y\n)\n\n__mod__\n\nView source\n\n__mod__(\n    y\n)\n\n__mul__\n\nView source\n\n__mul__(\n    y\n)\n\n__ne__\n\nView source\n\n__ne__(\n    other\n)\n\n__neg__\n__neg__(\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nComputes numerical negative value element-wise.\n\nI.e., \n𝑦\n=\n−\n𝑥\n.\n\nArgs\nx\tA Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor. Has the same type as x.\n\nIf x is a SparseTensor, returns SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)\n\n__nonzero__\n\nView source\n\n__nonzero__()\n\n\nDummy method to prevent a tensor from being used as a Python bool.\n\nThis is the Python 2.x counterpart to __bool__() above.\n\nRaises\nTypeError.\n\n__or__\n\nView source\n\n__or__(\n    y\n)\n\n__pow__\n\nView source\n\n__pow__(\n    y\n)\n\n__radd__\n\nView source\n\n__radd__(\n    x\n)\n\n__rand__\n\nView source\n\n__rand__(\n    x\n)\n\n__rdiv__\n\nView source\n\n__rdiv__(\n    x\n)\n\n__rfloordiv__\n\nView source\n\n__rfloordiv__(\n    x\n)\n\n__rmatmul__\n\nView source\n\n__rmatmul__(\n    x\n)\n\n__rmod__\n\nView source\n\n__rmod__(\n    x\n)\n\n__rmul__\n\nView source\n\n__rmul__(\n    x\n)\n\n__ror__\n\nView source\n\n__ror__(\n    x\n)\n\n__rpow__\n\nView source\n\n__rpow__(\n    x\n)\n\n__rsub__\n\nView source\n\n__rsub__(\n    x\n)\n\n__rtruediv__\n\nView source\n\n__rtruediv__(\n    x\n)\n\n__rxor__\n\nView source\n\n__rxor__(\n    x\n)\n\n__sub__\n\nView source\n\n__sub__(\n    y\n)\n\n__truediv__\n\nView source\n\n__truediv__(\n    y\n)\n\n__xor__\n\nView source\n\n__xor__(\n    y\n)\n\n\nClass Variables\n\nOVERLOADABLE_OPERATORS\t\n\n{\n '__abs__',\n '__add__',\n '__and__',\n '__div__',\n '__eq__',\n '__floordiv__',\n '__ge__',\n '__getitem__',\n '__gt__',\n '__invert__',\n '__le__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__or__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdiv__',\n '__rfloordiv__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__sub__',\n '__truediv__',\n '__xor__'\n}\n\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.SparseTensorSpec  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/SparseTensorSpec",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nArgs\nAttributes\nMethods\nexperimental_as_proto\nexperimental_from_proto\nexperimental_type_proto\nfrom_value\nis_compatible_with\nis_subtype_of\nmost_specific_common_supertype\nmost_specific_compatible_type\n__eq__\n__ne__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.SparseTensorSpec \nbookmark_border\n\nView source on GitHub\n\nType specification for a tf.sparse.SparseTensor.\n\nInherits From: TypeSpec, TraceType\n\nView aliases\ntf.SparseTensorSpec(\n    shape=None,\n    dtype=tf.dtypes.float32\n)\n\n\nArgs\n\nshape\tThe dense shape of the SparseTensor, or None to allow any dense shape.\ndtype\ttf.DType of values in the SparseTensor.\n\nAttributes\n\ndtype\tThe tf.dtypes.DType specified by this type for the SparseTensor.\nshape\tThe tf.TensorShape specified by this type for the SparseTensor.\nvalue_type\t\n\nMethods\nexperimental_as_proto\n\nView source\n\nexperimental_as_proto() -> struct_pb2.TypeSpecProto\n\n\nReturns a proto representation of the TypeSpec instance.\n\nDo NOT override for custom non-TF types.\n\nexperimental_from_proto\n\nView source\n\n@classmethod\nexperimental_from_proto(\n    proto: struct_pb2.TypeSpecProto\n) -> 'TypeSpec'\n\n\nReturns a TypeSpec instance based on the serialized proto.\n\nDo NOT override for custom non-TF types.\n\nArgs\nproto\tProto generated using 'experimental_as_proto'.\n\nexperimental_type_proto\n\nView source\n\n@classmethod\nexperimental_type_proto() -> Type[struct_pb2.TypeSpecProto]\n\n\nReturns the type of proto associated with TypeSpec serialization.\n\nDo NOT override for custom non-TF types.\n\nfrom_value\n\nView source\n\n@classmethod\nfrom_value(\n    value\n)\n\nis_compatible_with\n\nView source\n\nis_compatible_with(\n    spec_or_value\n)\n\n\nReturns true if spec_or_value is compatible with this TypeSpec.\n\nPrefer using \"is_subtype_of\" and \"most_specific_common_supertype\" wherever possible.\n\nArgs\nspec_or_value\tA TypeSpec or TypeSpec associated value to compare against.\n\nis_subtype_of\n\nView source\n\nis_subtype_of(\n    other: tf.types.experimental.TraceType\n) -> bool\n\n\nReturns True if self is a subtype of other.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nother\tA TraceType object.\n\nmost_specific_common_supertype\n\nView source\n\nmost_specific_common_supertype(\n    others: Sequence[tf.types.experimental.TraceType]\n) -> Optional['TypeSpec']\n\n\nReturns the most specific supertype TypeSpec of self and others.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nothers\tA sequence of TraceTypes.\n\nmost_specific_compatible_type\n\nView source\n\nmost_specific_compatible_type(\n    other: 'TypeSpec'\n) -> 'TypeSpec'\n\n\nReturns the most specific TypeSpec compatible with self and other. (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use most_specific_common_supertype instead.\n\nDeprecated. Please use most_specific_common_supertype instead. Do not override this function.\n\nArgs\nother\tA TypeSpec.\n\nRaises\nValueError\tIf there is no TypeSpec that is compatible with both self and other.\n\n__eq__\n\nView source\n\n__eq__(\n    other\n) -> bool\n\n\nReturn self==value.\n\n__ne__\n\nView source\n\n__ne__(\n    other\n) -> bool\n\n\nReturn self!=value.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.RegisterGradient  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/RegisterGradient",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nArgs\nRaises\nMethods\n__call__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\ntf.RegisterGradient \nbookmark_border\n\nView source on GitHub\n\nA decorator for registering the gradient function for an op type.\n\nView aliases\ntf.RegisterGradient(\n    op_type\n)\n\n\nThis decorator is only used when defining a new op type. For an op with m inputs and n outputs, the gradient function is a function that takes the original Operation and n Tensor objects (representing the gradients with respect to each output of the op), and returns m Tensor objects (representing the partial gradients with respect to each input of the op).\n\nFor example, assuming that operations of type \"Sub\" take two inputs x and y, and return a single output x - y, the following gradient function would be registered:\n\n@tf.RegisterGradient(\"Sub\")\ndef _sub_grad(unused_op, grad):\n  return grad, tf.negative(grad)\n\n\nThe decorator argument op_type is the string type of an operation. This corresponds to the OpDef.name field for the proto that defines the operation.\n\nArgs\n\nop_type\tThe string type of an operation. This corresponds to the OpDef.name field for the proto that defines the operation.\n\nRaises\n\nTypeError\tIf op_type is not string.\n\nMethods\n__call__\n\nView source\n\n__call__(\n    f: _T\n) -> _T\n\n\nRegisters the function f as gradient function for op_type.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.RaggedTensorSpec  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/RaggedTensorSpec",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nArgs\nAttributes\nMethods\nexperimental_as_proto\nexperimental_from_proto\nexperimental_type_proto\nfrom_value\nis_compatible_with\nis_subtype_of\nmost_specific_common_supertype\nmost_specific_compatible_type\n__eq__\n__ne__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.RaggedTensorSpec \nbookmark_border\n\nView source on GitHub\n\nType specification for a tf.RaggedTensor.\n\nInherits From: TypeSpec, TraceType\n\nView aliases\ntf.RaggedTensorSpec(\n    shape=None,\n    dtype=tf.dtypes.float32,\n    ragged_rank=None,\n    row_splits_dtype=tf.dtypes.int64,\n    flat_values_spec=None\n)\n\nUsed in the notebooks\nUsed in the guide\n\nRagged tensors\nSubword tokenizers\n\nArgs\n\nshape\tThe shape of the RaggedTensor, or None to allow any shape. If a shape is specified, then all ragged dimensions must have size None.\ndtype\ttf.DType of values in the RaggedTensor.\nragged_rank\tPython integer, the number of times the RaggedTensor's flat_values is partitioned. Defaults to shape.ndims - 1.\nrow_splits_dtype\tdtype for the RaggedTensor's row_splits tensor. One of tf.int32 or tf.int64.\nflat_values_spec\tTypeSpec for flat_value of the RaggedTensor. It shall be provided when the flat_values is a CompositeTensor rather then Tensor. If both dtype and flat_values_spec and are provided, dtype must be the same as flat_values_spec.dtype. (experimental)\n\nAttributes\n\ndtype\tThe tf.dtypes.DType specified by this type for the RaggedTensor.\n\nrt = tf.ragged.constant([[\"a\"], [\"b\", \"c\"]], dtype=tf.string)\ntf.type_spec_from_value(rt).dtype\ntf.string\n\n\n\nflat_values_spec\tThe TypeSpec of the flat_values of RaggedTensor.\nragged_rank\tThe number of times the RaggedTensor's flat_values is partitioned.\n\nDefaults to shape.ndims - 1.\n\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\ntf.type_spec_from_value(values).ragged_rank\n1\n\nrt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)\ntf.type_spec_from_value(rt1).ragged_rank\n2\n\n\n\nrow_splits_dtype\tThe tf.dtypes.DType of the RaggedTensor's row_splits.\n\nrt = tf.ragged.constant([[1, 2, 3], [4]], row_splits_dtype=tf.int64)\ntf.type_spec_from_value(rt).row_splits_dtype\ntf.int64\n\n\n\nshape\tThe statically known shape of the RaggedTensor.\n\nrt = tf.ragged.constant([[0], [1, 2]])\ntf.type_spec_from_value(rt).shape\nTensorShape([2, None])\n\nrt = tf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1)\ntf.type_spec_from_value(rt).shape\nTensorShape([2, None, 2])\n\n\n\nvalue_type\tThe Python type for values that are compatible with this TypeSpec.\n\nIn particular, all values that are compatible with this TypeSpec must be an instance of this type.\n\nMethods\nexperimental_as_proto\n\nView source\n\nexperimental_as_proto() -> struct_pb2.TypeSpecProto\n\n\nReturns a proto representation of the TypeSpec instance.\n\nDo NOT override for custom non-TF types.\n\nexperimental_from_proto\n\nView source\n\n@classmethod\nexperimental_from_proto(\n    proto: struct_pb2.TypeSpecProto\n) -> 'TypeSpec'\n\n\nReturns a TypeSpec instance based on the serialized proto.\n\nDo NOT override for custom non-TF types.\n\nArgs\nproto\tProto generated using 'experimental_as_proto'.\n\nexperimental_type_proto\n\nView source\n\n@classmethod\nexperimental_type_proto() -> Type[struct_pb2.TypeSpecProto]\n\n\nReturns the type of proto associated with TypeSpec serialization.\n\nDo NOT override for custom non-TF types.\n\nfrom_value\n\nView source\n\n@classmethod\nfrom_value(\n    value\n)\n\nis_compatible_with\n\nView source\n\nis_compatible_with(\n    spec_or_value\n)\n\n\nReturns true if spec_or_value is compatible with this TypeSpec.\n\nPrefer using \"is_subtype_of\" and \"most_specific_common_supertype\" wherever possible.\n\nArgs\nspec_or_value\tA TypeSpec or TypeSpec associated value to compare against.\n\nis_subtype_of\n\nView source\n\nis_subtype_of(\n    other: tf.types.experimental.TraceType\n) -> bool\n\n\nReturns True if self is a subtype of other.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nother\tA TraceType object.\n\nmost_specific_common_supertype\n\nView source\n\nmost_specific_common_supertype(\n    others: Sequence[tf.types.experimental.TraceType]\n) -> Optional['TypeSpec']\n\n\nReturns the most specific supertype TypeSpec of self and others.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nothers\tA sequence of TraceTypes.\n\nmost_specific_compatible_type\n\nView source\n\nmost_specific_compatible_type(\n    other: 'TypeSpec'\n) -> 'TypeSpec'\n\n\nReturns the most specific TypeSpec compatible with self and other. (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use most_specific_common_supertype instead.\n\nDeprecated. Please use most_specific_common_supertype instead. Do not override this function.\n\nArgs\nother\tA TypeSpec.\n\nRaises\nValueError\tIf there is no TypeSpec that is compatible with both self and other.\n\n__eq__\n\nView source\n\n__eq__(\n    other\n) -> bool\n\n\nReturn self==value.\n\n__ne__\n\nView source\n\n__ne__(\n    other\n) -> bool\n\n\nReturn self!=value.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.RaggedTensor  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/RaggedTensor",
    "html": "Loading [MathJax]/jax/output/SVG/jax.js\nInstall\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nPotentially Ragged Tensors\nDocumenting RaggedTensor Shapes\nComponent Tensors\nAlternative Row-Partitioning Schemes\nMultiple Ragged Dimensions\nUniform Inner Dimensions\nUniform Outer Dimensions\nAttributes\nMethods\nbounding_shape\nconsumers\nfrom_nested_row_lengths\nfrom_nested_row_splits\nfrom_nested_value_rowids\nfrom_row_lengths\nfrom_row_limits\nfrom_row_splits\nfrom_row_starts\nfrom_sparse\nfrom_tensor\nfrom_uniform_row_length\nfrom_value_rowids\nget_shape\nmerge_dims\nnested_row_lengths\nnested_value_rowids\nnrows\nnumpy\nrow_lengths\nrow_limits\nrow_starts\nto_list\nto_sparse\nto_tensor\nvalue_rowids\nwith_flat_values\nwith_row_splits_dtype\nwith_values\n__abs__\n__add__\n__and__\n__bool__\n__div__\n__eq__\n__floordiv__\n__ge__\n__getitem__\n__gt__\n__invert__\n__le__\n__lt__\n__mod__\n__mul__\n__ne__\n__neg__\n__nonzero__\n__or__\n__pow__\n__radd__\n__rand__\n__rdiv__\n__rfloordiv__\n__rmod__\n__rmul__\n__ror__\n__rpow__\n__rsub__\n__rtruediv__\n__rxor__\n__sub__\n__truediv__\n__xor__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.RaggedTensor \nbookmark_border\n\nView source on GitHub\n\nRepresents a ragged tensor.\n\nView aliases\nUsed in the notebooks\nUsed in the guide\n\nRagged tensors\nUnicode strings\n\nA RaggedTensor is a tensor with one or more ragged dimensions, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []] is ragged, since the column slices (rt[0, :], ..., rt[4, :]) have different lengths. Dimensions whose slices all have the same length are called uniform dimensions. The outermost dimension of a RaggedTensor is always uniform, since it consists of a single slice (and so there is no possibility for differing slice lengths).\n\nThe total number of dimensions in a RaggedTensor is called its rank, and the number of ragged dimensions in a RaggedTensor is called its ragged-rank. A RaggedTensor's ragged-rank is fixed at graph creation time: it can't depend on the runtime values of Tensors, and can't vary dynamically for different session runs.\n\nNote that the __init__ constructor is private. Please use one of the following methods to construct a RaggedTensor:\n\ntf.RaggedTensor.from_row_lengths\ntf.RaggedTensor.from_value_rowids\ntf.RaggedTensor.from_row_splits\ntf.RaggedTensor.from_row_starts\ntf.RaggedTensor.from_row_limits\ntf.RaggedTensor.from_nested_row_splits\ntf.RaggedTensor.from_nested_row_lengths\ntf.RaggedTensor.from_nested_value_rowids\nPotentially Ragged Tensors\n\nMany ops support both Tensors and RaggedTensors (see tf.ragged for a full listing). The term \"potentially ragged tensor\" may be used to refer to a tensor that might be either a Tensor or a RaggedTensor. The ragged-rank of a Tensor is zero.\n\nDocumenting RaggedTensor Shapes\n\nWhen documenting the shape of a RaggedTensor, ragged dimensions can be indicated by enclosing them in parentheses. For example, the shape of a 3-D RaggedTensor that stores the fixed-size word embedding for each word in a sentence, for each sentence in a batch, could be written as [num_sentences, (num_words), embedding_size]. The parentheses around (num_words) indicate that dimension is ragged, and that the length of each element list in that dimension may vary for each item.\n\nComponent Tensors\n\nInternally, a RaggedTensor consists of a concatenated list of values that are partitioned into variable-length rows. In particular, each RaggedTensor consists of:\n\nA values tensor, which concatenates the variable-length rows into a flattened list. For example, the values tensor for [[3, 1, 4, 1], [], [5, 9, 2], [6], []] is [3, 1, 4, 1, 5, 9, 2, 6].\n\nA row_splits vector, which indicates how those flattened values are divided into rows. In particular, the values for row rt[i] are stored in the slice rt.values[rt.row_splits[i]:rt.row_splits[i+1]].\n\nExample:\nprint(tf.RaggedTensor.from_row_splits(\n      values=[3, 1, 4, 1, 5, 9, 2, 6],\n      row_splits=[0, 4, 4, 7, 8, 8]))\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n\nAlternative Row-Partitioning Schemes\n\nIn addition to row_splits, ragged tensors provide support for five other row-partitioning schemes:\n\nrow_lengths: a vector with shape [nrows], which specifies the length of each row.\n\nvalue_rowids and nrows: value_rowids is a vector with shape [nvals], corresponding one-to-one with values, which specifies each value's row index. In particular, the row rt[row] consists of the values rt.values[j] where value_rowids[j]==row. nrows is an integer scalar that specifies the number of rows in the RaggedTensor. (nrows is used to indicate trailing empty rows.)\n\nrow_starts: a vector with shape [nrows], which specifies the start offset of each row. Equivalent to row_splits[:-1].\n\nrow_limits: a vector with shape [nrows], which specifies the stop offset of each row. Equivalent to row_splits[1:].\n\nuniform_row_length: A scalar tensor, specifying the length of every row. This row-partitioning scheme may only be used if all rows have the same length.\n\nExample: The following ragged tensors are equivalent, and all represent the nested list [[3, 1, 4, 1], [], [5, 9, 2], [6], []].\n\nvalues = [3, 1, 4, 1, 5, 9, 2, 6]\nRaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\nRaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\nRaggedTensor.from_value_rowids(\n    values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\nRaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\nRaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\nRaggedTensor.from_uniform_row_length(values, uniform_row_length=2)\n<tf.RaggedTensor [[3, 1], [4, 1], [5, 9], [2, 6]]>\n\nMultiple Ragged Dimensions\n\nRaggedTensors with multiple ragged dimensions can be defined by using a nested RaggedTensor for the values tensor. Each nested RaggedTensor adds a single ragged dimension.\n\ninner_rt = RaggedTensor.from_row_splits(  # =rt1 from above\n    values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\nouter_rt = RaggedTensor.from_row_splits(\n    values=inner_rt, row_splits=[0, 3, 3, 5])\nprint(outer_rt.to_list())\n[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]\nprint(outer_rt.ragged_rank)\n2\n\n\nThe factory function RaggedTensor.from_nested_row_splits may be used to construct a RaggedTensor with multiple ragged dimensions directly, by providing a list of row_splits tensors:\n\nRaggedTensor.from_nested_row_splits(\n    flat_values=[3, 1, 4, 1, 5, 9, 2, 6],\n    nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8])).to_list()\n[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]\n\nUniform Inner Dimensions\n\nRaggedTensors with uniform inner dimensions can be defined by using a multidimensional Tensor for values.\n\nrt = RaggedTensor.from_row_splits(values=tf.ones([5, 3], tf.int32),\n                                  row_splits=[0, 2, 5])\nprint(rt.to_list())\n[[[1, 1, 1], [1, 1, 1]],\n [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]\nprint(rt.shape)\n(2, None, 3)\n\nUniform Outer Dimensions\n\nRaggedTensors with uniform outer dimensions can be defined by using one or more RaggedTensor with a uniform_row_length row-partitioning tensor. For example, a RaggedTensor with shape [2, 2, None] can be constructed with this method from a RaggedTensor values with shape [4, None]:\n\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nprint(values.shape)\n(4, None)\nrt6 = tf.RaggedTensor.from_uniform_row_length(values, 2)\nprint(rt6)\n<tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>\nprint(rt6.shape)\n(2, 2, None)\n\n\nNote that rt6 only contains one ragged dimension (the innermost dimension). In contrast, if from_row_splits is used to construct a similar RaggedTensor, then that RaggedTensor will have two ragged dimensions:\n\nrt7 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])\nprint(rt7.shape)\n(2, None, None)\n\n\nUniform and ragged outer dimensions may be interleaved, meaning that a tensor with any combination of ragged and uniform dimensions may be created. For example, a RaggedTensor t4 with shape [3, None, 4, 8, None, 2] could be constructed as follows:\n\nt0 = tf.zeros([1000, 2])                           # Shape:         [1000, 2]\nt1 = RaggedTensor.from_row_lengths(t0, [...])      #           [160, None, 2]\nt2 = RaggedTensor.from_uniform_row_length(t1, 8)   #         [20, 8, None, 2]\nt3 = RaggedTensor.from_uniform_row_length(t2, 4)   #       [5, 4, 8, None, 2]\nt4 = RaggedTensor.from_row_lengths(t3, [...])      # [3, None, 4, 8, None, 2]\n\n\nAttributes\n\ndtype\tThe DType of values in this tensor.\nflat_values\tThe innermost values tensor for this ragged tensor.\n\nConcretely, if rt.values is a Tensor, then rt.flat_values is rt.values; otherwise, rt.flat_values is rt.values.flat_values.\n\nConceptually, flat_values is the tensor formed by flattening the outermost dimension and all of the ragged dimensions into a single dimension.\n\nrt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:] (where nvals is the number of items in the flattened dimensions).\n\nExample:\nrt = tf.ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\nprint(rt.flat_values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n\n\n\nnested_row_splits\tA tuple containing the row_splits for all ragged dimensions.\n\nrt.nested_row_splits is a tuple containing the row_splits tensors for all ragged dimensions in rt, ordered from outermost to innermost. In particular, rt.nested_row_splits = (rt.row_splits,) + value_splits where:\n\n* `value_splits = ()` if `rt.values` is a `Tensor`.\n* `value_splits = rt.values.nested_row_splits` otherwise.\n\nExample:\nrt = tf.ragged.constant(\n    [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])\nfor i, splits in enumerate(rt.nested_row_splits):\n  print('Splits for dimension %d: %s' % (i+1, splits.numpy()))\nSplits for dimension 1: [0 3]\nSplits for dimension 2: [0 3 3 5]\nSplits for dimension 3: [0 4 4 7 8 8]\n\n\n\nragged_rank\tThe number of times the RaggedTensor's flat_values is partitioned.\n\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nvalues.ragged_rank\n1\n\nrt = tf.RaggedTensor.from_uniform_row_length(values, 2)\nrt.ragged_rank\n2\n\n\n\nrow_splits\tThe row-split indices for this ragged tensor's values.\n\nrt.row_splits specifies where the values for each row begin and end in rt.values. In particular, the values for row rt[i] are stored in the slice rt.values[rt.row_splits[i]:rt.row_splits[i+1]].\n\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.row_splits)  # indices of row splits in rt.values\ntf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)\n\n\n\nshape\tThe statically known shape of this ragged tensor.\n\ntf.ragged.constant([[0], [1, 2]]).shape\nTensorShape([2, None])\n\ntf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1).shape\nTensorShape([2, None, 2])\n\n\n\nuniform_row_length\tThe length of each row in this ragged tensor, or None if rows are ragged.\n\nrt1 = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nprint(rt1.uniform_row_length)  # rows are ragged.\nNone\n\nrt2 = tf.RaggedTensor.from_uniform_row_length(\n    values=rt1, uniform_row_length=2)\nprint(rt2)\n<tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>\nprint(rt2.uniform_row_length)  # rows are not ragged (all have size 2).\ntf.Tensor(2, shape=(), dtype=int64)\n\n\nA RaggedTensor's rows are only considered to be uniform (i.e. non-ragged) if it can be determined statically (at graph construction time) that the rows all have the same length.\n\n\nvalues\tThe concatenated rows for this ragged tensor.\n\nrt.values is a potentially ragged tensor formed by flattening the two outermost dimensions of rt into a single dimension.\n\nrt.values.shape = [nvals] + rt.shape[2:] (where nvals is the number of items in the outer two dimensions of rt).\n\nrt.ragged_rank = self.ragged_rank - 1\n\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n\n\nMethods\nbounding_shape\n\nView source\n\nbounding_shape(\n    axis=None, name=None, out_type=None\n)\n\n\nReturns the tight bounding box shape for this RaggedTensor.\n\nArgs\naxis\tAn integer scalar or vector indicating which axes to return the bounding box for. If not specified, then the full bounding box is returned.\nname\tA name prefix for the returned tensor (optional).\nout_type\tdtype for the returned tensor. Defaults to self.row_splits.dtype.\n\nReturns\nAn integer Tensor (dtype=self.row_splits.dtype). If axis is not specified, then output is a vector with output.shape=[self.shape.ndims]. If axis is a scalar, then the output is a scalar. If axis is a vector, then output is a vector, where output[i] is the bounding size for dimension axis[i].\n\nExample:\nrt = tf.ragged.constant([[1, 2, 3, 4], [5], [], [6, 7, 8, 9], [10]])\nrt.bounding_shape().numpy()\narray([5, 4])\n\nconsumers\n\nView source\n\nconsumers()\n\nfrom_nested_row_lengths\n\nView source\n\n@classmethod\nfrom_nested_row_lengths(\n    flat_values, nested_row_lengths, name=None, validate=True\n)\n\n\nCreates a RaggedTensor from a nested list of row_lengths tensors.\n\nEquivalent to:\nresult = flat_values\nfor row_lengths in reversed(nested_row_lengths):\n  result = from_row_lengths(result, row_lengths)\n\n\nArgs\nflat_values\tA potentially ragged tensor.\nnested_row_lengths\tA list of 1-D integer tensors. The ith tensor is used as the row_lengths for the ith ragged dimension.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor (or flat_values if nested_row_lengths is empty).\n\nfrom_nested_row_splits\n\nView source\n\n@classmethod\nfrom_nested_row_splits(\n    flat_values, nested_row_splits, name=None, validate=True\n)\n\n\nCreates a RaggedTensor from a nested list of row_splits tensors.\n\nEquivalent to:\nresult = flat_values\nfor row_splits in reversed(nested_row_splits):\n  result = from_row_splits(result, row_splits)\n\n\nArgs\nflat_values\tA potentially ragged tensor.\nnested_row_splits\tA list of 1-D integer tensors. The ith tensor is used as the row_splits for the ith ragged dimension.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor (or flat_values if nested_row_splits is empty).\n\nfrom_nested_value_rowids\n\nView source\n\n@classmethod\nfrom_nested_value_rowids(\n    flat_values,\n    nested_value_rowids,\n    nested_nrows=None,\n    name=None,\n    validate=True\n)\n\n\nCreates a RaggedTensor from a nested list of value_rowids tensors.\n\nEquivalent to:\nresult = flat_values\nfor (rowids, nrows) in reversed(zip(nested_value_rowids, nested_nrows)):\n  result = from_value_rowids(result, rowids, nrows)\n\n\nArgs\nflat_values\tA potentially ragged tensor.\nnested_value_rowids\tA list of 1-D integer tensors. The ith tensor is used as the value_rowids for the ith ragged dimension.\nnested_nrows\tA list of integer scalars. The ith scalar is used as the nrows for the ith ragged dimension.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor (or flat_values if nested_value_rowids is empty).\n\nRaises\nValueError\tIf len(nested_values_rowids) != len(nested_nrows).\n\nfrom_row_lengths\n\nView source\n\n@classmethod\nfrom_row_lengths(\n    values, row_lengths, name=None, validate=True\n)\n\n\nCreates a RaggedTensor with rows partitioned by row_lengths.\n\nThe returned RaggedTensor corresponds with the python list defined by:\n\nresult = [[values.pop(0) for i in range(length)]\n          for length in row_lengths]\n\n\nArgs\nvalues\tA potentially ragged tensor with shape [nvals, ...].\nrow_lengths\tA 1-D integer tensor with shape [nrows]. Must be nonnegative. sum(row_lengths) must be nvals.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor. result.rank = values.rank + 1. result.ragged_rank = values.ragged_rank + 1.\n\nExample:\nprint(tf.RaggedTensor.from_row_lengths(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_lengths=[4, 0, 3, 1, 0]))\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n\nfrom_row_limits\n\nView source\n\n@classmethod\nfrom_row_limits(\n    values, row_limits, name=None, validate=True\n)\n\n\nCreates a RaggedTensor with rows partitioned by row_limits.\n\nEquivalent to: from_row_splits(values, concat([0, row_limits])).\n\nArgs\nvalues\tA potentially ragged tensor with shape [nvals, ...].\nrow_limits\tA 1-D integer tensor with shape [nrows]. Must be sorted in ascending order. If nrows>0, then row_limits[-1] must be nvals.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor. result.rank = values.rank + 1. result.ragged_rank = values.ragged_rank + 1.\n\nExample:\nprint(tf.RaggedTensor.from_row_limits(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_limits=[4, 4, 7, 8, 8]))\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n\nfrom_row_splits\n\nView source\n\n@classmethod\nfrom_row_splits(\n    values, row_splits, name=None, validate=True\n)\n\n\nCreates a RaggedTensor with rows partitioned by row_splits.\n\nThe returned RaggedTensor corresponds with the python list defined by:\n\nresult = [values[row_splits[i]:row_splits[i + 1]]\n          for i in range(len(row_splits) - 1)]\n\n\nArgs\nvalues\tA potentially ragged tensor with shape [nvals, ...].\nrow_splits\tA 1-D integer tensor with shape [nrows+1]. Must not be empty, and must be sorted in ascending order. row_splits[0] must be zero and row_splits[-1] must be nvals.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor. result.rank = values.rank + 1. result.ragged_rank = values.ragged_rank + 1.\n\nRaises\nValueError\tIf row_splits is an empty list.\n\nExample:\nprint(tf.RaggedTensor.from_row_splits(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_splits=[0, 4, 4, 7, 8, 8]))\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n\nfrom_row_starts\n\nView source\n\n@classmethod\nfrom_row_starts(\n    values, row_starts, name=None, validate=True\n)\n\n\nCreates a RaggedTensor with rows partitioned by row_starts.\n\nEquivalent to: from_row_splits(values, concat([row_starts, nvals])).\n\nArgs\nvalues\tA potentially ragged tensor with shape [nvals, ...].\nrow_starts\tA 1-D integer tensor with shape [nrows]. Must be nonnegative and sorted in ascending order. If nrows>0, then row_starts[0] must be zero.\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor. result.rank = values.rank + 1. result.ragged_rank = values.ragged_rank + 1.\n\nExample:\nprint(tf.RaggedTensor.from_row_starts(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_starts=[0, 4, 4, 7, 8]))\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n\nfrom_sparse\n\nView source\n\n@classmethod\nfrom_sparse(\n    st_input,\n    name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n\n\nConverts a 2D tf.sparse.SparseTensor to a RaggedTensor.\n\nEach row of the output RaggedTensor will contain the explicit values from the same row in st_input. st_input must be ragged-right. If not it is not ragged-right, then an error will be generated.\n\nExample:\nindices = [[0, 0], [0, 1], [0, 2], [1, 0], [3, 0]]\nst = tf.sparse.SparseTensor(indices=indices,\n                            values=[1, 2, 3, 4, 5],\n                            dense_shape=[4, 3])\ntf.RaggedTensor.from_sparse(st).to_list()\n[[1, 2, 3], [4], [], [5]]\n\n\nCurrently, only two-dimensional SparseTensors are supported.\n\nArgs\nst_input\tThe sparse tensor to convert. Must have rank 2.\nname\tA name prefix for the returned tensors (optional).\nrow_splits_dtype\tdtype for the returned RaggedTensor's row_splits tensor. One of tf.int32 or tf.int64.\n\nReturns\nA RaggedTensor with the same values as st_input. output.ragged_rank = rank(st_input) - 1. output.shape = [st_input.dense_shape[0], None].\n\nRaises\nValueError\tIf the number of dimensions in st_input is not known statically, or is not two.\n\nfrom_tensor\n\nView source\n\n@classmethod\nfrom_tensor(\n    tensor,\n    lengths=None,\n    padding=None,\n    ragged_rank=1,\n    name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n\n\nConverts a tf.Tensor into a RaggedTensor.\n\nThe set of absent/default values may be specified using a vector of lengths or a padding value (but not both). If lengths is specified, then the output tensor will satisfy output[row] = tensor[row][:lengths[row]]. If 'lengths' is a list of lists or tuple of lists, those lists will be used as nested row lengths. If padding is specified, then any row suffix consisting entirely of padding will be excluded from the returned RaggedTensor. If neither lengths nor padding is specified, then the returned RaggedTensor will have no absent/default values.\n\nExamples:\ndt = tf.constant([[5, 7, 0], [0, 3, 0], [6, 0, 0]])\ntf.RaggedTensor.from_tensor(dt)\n<tf.RaggedTensor [[5, 7, 0], [0, 3, 0], [6, 0, 0]]>\ntf.RaggedTensor.from_tensor(dt, lengths=[1, 0, 3])\n<tf.RaggedTensor [[5], [], [6, 0, 0]]>\n\ntf.RaggedTensor.from_tensor(dt, padding=0)\n<tf.RaggedTensor [[5, 7], [0, 3], [6]]>\n\ndt = tf.constant([[[5, 0], [7, 0], [0, 0]],\n                  [[0, 0], [3, 0], [0, 0]],\n                  [[6, 0], [0, 0], [0, 0]]])\ntf.RaggedTensor.from_tensor(dt, lengths=([2, 0, 3], [1, 1, 2, 0, 1]))\n<tf.RaggedTensor [[[5], [7]], [], [[6, 0], [], [0]]]>\n\n\nArgs\ntensor\tThe Tensor to convert. Must have rank ragged_rank + 1 or higher.\nlengths\tAn optional set of row lengths, specified using a 1-D integer Tensor whose length is equal to tensor.shape[0] (the number of rows in tensor). If specified, then output[row] will contain tensor[row][:lengths[row]]. Negative lengths are treated as zero. You may optionally pass a list or tuple of lengths to this argument, which will be used as nested row lengths to construct a ragged tensor with multiple ragged dimensions.\npadding\tAn optional padding value. If specified, then any row suffix consisting entirely of padding will be excluded from the returned RaggedTensor. padding is a Tensor with the same dtype as tensor and with shape=tensor.shape[ragged_rank + 1:].\nragged_rank\tInteger specifying the ragged rank for the returned RaggedTensor. Must be greater than zero.\nname\tA name prefix for the returned tensors (optional).\nrow_splits_dtype\tdtype for the returned RaggedTensor's row_splits tensor. One of tf.int32 or tf.int64.\n\nReturns\nA RaggedTensor with the specified ragged_rank. The shape of the returned ragged tensor is compatible with the shape of tensor.\n\nRaises\nValueError\tIf both lengths and padding are specified.\nValueError\tIf the rank of tensor is 0 or 1.\n\nfrom_uniform_row_length\n\nView source\n\n@classmethod\nfrom_uniform_row_length(\n    values, uniform_row_length, nrows=None, validate=True, name=None\n)\n\n\nCreates a RaggedTensor with rows partitioned by uniform_row_length.\n\nThis method can be used to create RaggedTensors with multiple uniform outer dimensions. For example, a RaggedTensor with shape [2, 2, None] can be constructed with this method from a RaggedTensor values with shape [4, None]:\n\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nprint(values.shape)\n(4, None)\nrt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)\nprint(rt1)\n<tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>\nprint(rt1.shape)\n(2, 2, None)\n\n\nNote that rt1 only contains one ragged dimension (the innermost dimension). In contrast, if from_row_splits is used to construct a similar RaggedTensor, then that RaggedTensor will have two ragged dimensions:\n\nrt2 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])\nprint(rt2.shape)\n(2, None, None)\n\n\nArgs\nvalues\tA potentially ragged tensor with shape [nvals, ...].\nuniform_row_length\tA scalar integer tensor. Must be nonnegative. The size of the outer axis of values must be evenly divisible by uniform_row_length.\nnrows\tThe number of rows in the constructed RaggedTensor. If not specified, then it defaults to nvals/uniform_row_length (or 0 if uniform_row_length==0). nrows only needs to be specified if uniform_row_length might be zero. uniform_row_length*nrows must be nvals.\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\nname\tA name prefix for the RaggedTensor (optional).\n\nReturns\nA RaggedTensor that corresponds with the python list defined by:\n\nresult = [[values.pop(0) for i in range(uniform_row_length)]\n          for _ in range(nrows)]\n\n\nresult.rank = values.rank + 1. result.ragged_rank = values.ragged_rank + 1.\n\nfrom_value_rowids\n\nView source\n\n@classmethod\nfrom_value_rowids(\n    values, value_rowids, nrows=None, name=None, validate=True\n)\n\n\nCreates a RaggedTensor with rows partitioned by value_rowids.\n\nThe returned RaggedTensor corresponds with the python list defined by:\n\nresult = [[values[i] for i in range(len(values)) if value_rowids[i] == row]\n          for row in range(nrows)]\n\n\nArgs\nvalues\tA potentially ragged tensor with shape [nvals, ...].\nvalue_rowids\tA 1-D integer tensor with shape [nvals], which corresponds one-to-one with values, and specifies each value's row index. Must be nonnegative, and must be sorted in ascending order.\nnrows\tAn integer scalar specifying the number of rows. This should be specified if the RaggedTensor may containing empty training rows. Must be greater than value_rowids[-1] (or zero if value_rowids is empty). Defaults to value_rowids[-1] + 1 (or zero if value_rowids is empty).\nname\tA name prefix for the RaggedTensor (optional).\nvalidate\tIf true, then use assertions to check that the arguments form a valid RaggedTensor. Note: these assertions incur a runtime cost, since they must be checked for each tensor value.\n\nReturns\nA RaggedTensor. result.rank = values.rank + 1. result.ragged_rank = values.ragged_rank + 1.\n\nRaises\nValueError\tIf nrows is incompatible with value_rowids.\n\nExample:\nprint(tf.RaggedTensor.from_value_rowids(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    value_rowids=[0, 0, 0, 0, 2, 2, 2, 3],\n    nrows=5))\n<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n\nget_shape\n\nView source\n\nget_shape() -> tf.TensorShape\n\n\nThe statically known shape of this ragged tensor.\n\nReturns\nA TensorShape containing the statically known shape of this ragged tensor. Ragged dimensions have a size of None.\n\nAlias for shape property.\n\nExamples:\ntf.ragged.constant([[0], [1, 2]]).get_shape()\nTensorShape([2, None])\n\ntf.ragged.constant(\n   [[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1).get_shape()\nTensorShape([2, None, 2])\n\nmerge_dims\n\nView source\n\nmerge_dims(\n    outer_axis, inner_axis\n)\n\n\nMerges outer_axis...inner_axis into a single dimension.\n\nReturns a copy of this RaggedTensor with the specified range of dimensions flattened into a single dimension, with elements in row-major order.\n\nExamples:\nrt = tf.ragged.constant([[[1, 2], [3]], [[4, 5, 6]]])\nprint(rt.merge_dims(0, 1))\n<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>\nprint(rt.merge_dims(1, 2))\n<tf.RaggedTensor [[1, 2, 3], [4, 5, 6]]>\nprint(rt.merge_dims(0, 2))\ntf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)\n\n\nTo mimic the behavior of np.flatten (which flattens all dimensions), use rt.merge_dims(0, -1). To mimic the behavior oftf.layers.Flatten(which flattens all dimensions except the outermost batch dimension), usert.merge_dims(1, -1)`.\n\nArgs\nouter_axis\tint: The first dimension in the range of dimensions to merge. May be negative if self.shape.rank is statically known.\ninner_axis\tint: The last dimension in the range of dimensions to merge. May be negative if self.shape.rank is statically known.\n\nReturns\nA copy of this tensor, with the specified dimensions merged into a single dimension. The shape of the returned tensor will be self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:], where N is the total number of slices in the merged dimensions.\n\nnested_row_lengths\n\nView source\n\nnested_row_lengths(\n    name=None\n)\n\n\nReturns a tuple containing the row_lengths for all ragged dimensions.\n\nrt.nested_row_lengths() is a tuple containing the row_lengths tensors for all ragged dimensions in rt, ordered from outermost to innermost.\n\nArgs\nname\tA name prefix for the returned tensors (optional).\n\nReturns\nA tuple of 1-D integer Tensors. The length of the tuple is equal to self.ragged_rank.\n\nnested_value_rowids\n\nView source\n\nnested_value_rowids(\n    name=None\n)\n\n\nReturns a tuple containing the value_rowids for all ragged dimensions.\n\nrt.nested_value_rowids is a tuple containing the value_rowids tensors for all ragged dimensions in rt, ordered from outermost to innermost. In particular, rt.nested_value_rowids = (rt.value_rowids(),) + value_ids where:\n\nvalue_ids = () if rt.values is a Tensor.\nvalue_ids = rt.values.nested_value_rowids otherwise.\n\nArgs\nname\tA name prefix for the returned tensors (optional).\n\nReturns\nA tuple of 1-D integer Tensors.\n\nExample:\nrt = tf.ragged.constant(\n    [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])\nfor i, ids in enumerate(rt.nested_value_rowids()):\n  print('row ids for dimension %d: %s' % (i+1, ids.numpy()))\nrow ids for dimension 1: [0 0 0]\nrow ids for dimension 2: [0 0 0 2 2]\nrow ids for dimension 3: [0 0 0 0 2 2 2 3]\n\nnrows\n\nView source\n\nnrows(\n    out_type=None, name=None\n)\n\n\nReturns the number of rows in this ragged tensor.\n\nI.e., the size of the outermost dimension of the tensor.\n\nArgs\nout_type\tdtype for the returned tensor. Defaults to self.row_splits.dtype.\nname\tA name prefix for the returned tensor (optional).\n\nReturns\nA scalar Tensor with dtype out_type.\n\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.nrows())  # rt has 5 rows.\ntf.Tensor(5, shape=(), dtype=int64)\n\nnumpy\n\nView source\n\nnumpy()\n\n\nReturns a numpy array with the values for this RaggedTensor.\n\nRequires that this RaggedTensor was constructed in eager execution mode.\n\nRagged dimensions are encoded using numpy arrays with dtype=object and rank=1, where each element is a single row.\n\nExamples\n\nIn the following example, the value returned by RaggedTensor.numpy() contains three numpy array objects: one for each row (with rank=1 and dtype=int64), and one to combine them (with rank=1 and dtype=object):\n\ntf.ragged.constant([[1, 2, 3], [4, 5]], dtype=tf.int64).numpy()\narray([array([1, 2, 3]), array([4, 5])], dtype=object)\n\n\nUniform dimensions are encoded using multidimensional numpy arrays. In the following example, the value returned by RaggedTensor.numpy() contains a single numpy array object, with rank=2 and dtype=int64:\n\ntf.ragged.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int64).numpy()\narray([[1, 2, 3], [4, 5, 6]])\n\n\nReturns\nA numpy array.\n\nrow_lengths\n\nView source\n\nrow_lengths(\n    axis=1, name=None\n)\n\n\nReturns the lengths of the rows in this ragged tensor.\n\nrt.row_lengths()[i] indicates the number of values in the ith row of rt.\n\nArgs\naxis\tAn integer constant indicating the axis whose row lengths should be returned.\nname\tA name prefix for the returned tensor (optional).\n\nReturns\nA potentially ragged integer Tensor with shape self.shape[:axis].\n\nRaises\nValueError\tIf axis is out of bounds.\n\nExample:\nrt = tf.ragged.constant(\n    [[[3, 1, 4], [1]], [], [[5, 9], [2]], [[6]], []])\nprint(rt.row_lengths())  # lengths of rows in rt\ntf.Tensor([2 0 2 1 0], shape=(5,), dtype=int64)\nprint(rt.row_lengths(axis=2))  # lengths of axis=2 rows.\n<tf.RaggedTensor [[3, 1], [], [2, 1], [1], []]>\n\nrow_limits\n\nView source\n\nrow_limits(\n    name=None\n)\n\n\nReturns the limit indices for rows in this ragged tensor.\n\nThese indices specify where the values for each row end in self.values. rt.row_limits(self) is equal to rt.row_splits[:-1].\n\nArgs\nname\tA name prefix for the returned tensor (optional).\n\nReturns\nA 1-D integer Tensor with shape [nrows]. The returned tensor is nonnegative, and is sorted in ascending order.\n\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\nprint(rt.row_limits())  # indices of row limits in rt.values\ntf.Tensor([4 4 7 8 8], shape=(5,), dtype=int64)\n\nrow_starts\n\nView source\n\nrow_starts(\n    name=None\n)\n\n\nReturns the start indices for rows in this ragged tensor.\n\nThese indices specify where the values for each row begin in self.values. rt.row_starts() is equal to rt.row_splits[:-1].\n\nArgs\nname\tA name prefix for the returned tensor (optional).\n\nReturns\nA 1-D integer Tensor with shape [nrows]. The returned tensor is nonnegative, and is sorted in ascending order.\n\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\nprint(rt.row_starts())  # indices of row starts in rt.values\ntf.Tensor([0 4 4 7 8], shape=(5,), dtype=int64)\n\nto_list\n\nView source\n\nto_list()\n\n\nReturns a nested Python list with the values for this RaggedTensor.\n\nRequires that rt was constructed in eager execution mode.\n\nReturns\nA nested Python list.\n\nto_sparse\n\nView source\n\nto_sparse(\n    name=None\n)\n\n\nConverts this RaggedTensor into a tf.sparse.SparseTensor.\n\nExample:\nrt = tf.ragged.constant([[1, 2, 3], [4], [], [5, 6]])\nprint(rt.to_sparse())\nSparseTensor(indices=tf.Tensor(\n                 [[0 0] [0 1] [0 2] [1 0] [3 0] [3 1]],\n                 shape=(6, 2), dtype=int64),\n             values=tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32),\n             dense_shape=tf.Tensor([4 3], shape=(2,), dtype=int64))\n\n\nArgs\nname\tA name prefix for the returned tensors (optional).\n\nReturns\nA SparseTensor with the same values as self.\n\nto_tensor\n\nView source\n\nto_tensor(\n    default_value=None, name=None, shape=None\n)\n\n\nConverts this RaggedTensor into a tf.Tensor.\n\nIf shape is specified, then the result is padded and/or truncated to the specified shape.\n\nExamples:\nrt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])\nprint(rt.to_tensor())\ntf.Tensor(\n    [[9 8 7] [0 0 0] [6 5 0] [4 0 0]], shape=(4, 3), dtype=int32)\nprint(rt.to_tensor(shape=[5, 2]))\ntf.Tensor(\n    [[9 8] [0 0] [6 5] [4 0] [0 0]], shape=(5, 2), dtype=int32)\n\n\nArgs\ndefault_value\tValue to set for indices not specified in self. Defaults to zero. default_value must be broadcastable to self.shape[self.ragged_rank + 1:].\nname\tA name prefix for the returned tensors (optional).\nshape\tThe shape of the resulting dense tensor. In particular, result.shape[i] is shape[i] (if shape[i] is not None), or self.bounding_shape(i) (otherwise).shape.rank must be None or equal to self.rank.\n\nReturns\nA Tensor with shape ragged.bounding_shape(self) and the values specified by the non-empty values in self. Empty values are assigned default_value.\n\nvalue_rowids\n\nView source\n\nvalue_rowids(\n    name=None\n)\n\n\nReturns the row indices for the values in this ragged tensor.\n\nrt.value_rowids() corresponds one-to-one with the outermost dimension of rt.values, and specifies the row containing each value. In particular, the row rt[row] consists of the values rt.values[j] where rt.value_rowids()[j] == row.\n\nArgs\nname\tA name prefix for the returned tensor (optional).\n\nReturns\nA 1-D integer Tensor with shape self.values.shape[:1]. The returned tensor is nonnegative, and is sorted in ascending order.\n\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\nprint(rt.value_rowids())  # corresponds 1:1 with rt.values\ntf.Tensor([0 0 0 0 2 2 2 3], shape=(8,), dtype=int64)\n\nwith_flat_values\n\nView source\n\nwith_flat_values(\n    new_values\n)\n\n\nReturns a copy of self with flat_values replaced by new_value.\n\nPreserves cached row-partitioning tensors such as self.cached_nrows and self.cached_value_rowids if they have values.\n\nArgs\nnew_values\tPotentially ragged tensor that should replace self.flat_values. Must have rank > 0, and must have the same number of rows as self.flat_values.\n\nReturns\nA RaggedTensor. result.rank = self.ragged_rank + new_values.rank. result.ragged_rank = self.ragged_rank + new_values.ragged_rank.\n\nwith_row_splits_dtype\n\nView source\n\nwith_row_splits_dtype(\n    dtype\n)\n\n\nReturns a copy of this RaggedTensor with the given row_splits dtype.\n\nFor RaggedTensors with multiple ragged dimensions, the row_splits for all nested RaggedTensor objects are cast to the given dtype.\n\nArgs\ndtype\tThe dtype for row_splits. One of tf.int32 or tf.int64.\n\nReturns\nA copy of this RaggedTensor, with the row_splits cast to the given type.\n\nwith_values\n\nView source\n\nwith_values(\n    new_values\n)\n\n\nReturns a copy of self with values replaced by new_value.\n\nPreserves cached row-partitioning tensors such as self.cached_nrows and self.cached_value_rowids if they have values.\n\nArgs\nnew_values\tPotentially ragged tensor to use as the values for the returned RaggedTensor. Must have rank > 0, and must have the same number of rows as self.values.\n\nReturns\nA RaggedTensor. result.rank = 1 + new_values.rank. result.ragged_rank = 1 + new_values.ragged_rank\n\n__abs__\n\nView source\n\n__abs__(\n    name=None\n)\n\n\nComputes the absolute value of a ragged tensor.\n\nGiven a ragged tensor of integer or floating-point values, this operation returns a ragged tensor of the same type, where each element contains the absolute value of the corresponding element in the input.\n\nGiven a ragged tensor x of complex numbers, this operation returns a tensor of type float32 or float64 that is the absolute value of each element in x. For a complex number a+bj, its absolute value is computed as \n√\na2+b2\n.\n\nFor example:\n# real number\nx = tf.ragged.constant([[-2.2, 3.2], [-4.2]])\ntf.abs(x)\n<tf.RaggedTensor [[2.2, 3.2], [4.2]]>\n\n# complex number\nx = tf.ragged.constant([[-2.2 + 4.7j], [-3.2 + 5.7j], [-4.2 + 6.7j]])\ntf.abs(x)\n<tf.RaggedTensor [[5.189412298131649],\n [6.536818798161687],\n [7.907591289387685]]>\n\n\nArgs\nname\tA name for the operation (optional).\n\nReturns\nA RaggedTensor of the same size and type as x, with absolute values. Note, for complex64 or complex128 input, the returned RaggedTensor will be of type float32 or float64, respectively.\n\n__add__\n\nView source\n\n__add__(\n    y, name=None\n)\n\n\nReturns x + y element-wise.\n\nExample usages below.\n\nAdd a scalar and a list:\n\nx = [1, 2, 3, 4, 5]\ny = 1\ntf.add(x, y)\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],\ndtype=int32)>\n\n\nNote that binary + operator can be used instead:\n\nx = tf.convert_to_tensor([1, 2, 3, 4, 5])\ny = tf.convert_to_tensor(1)\nx + y\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],\ndtype=int32)>\n\n\nAdd a tensor and a list of same shape:\n\nx = [1, 2, 3, 4, 5]\ny = tf.constant([1, 2, 3, 4, 5])\ntf.add(x, y)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 2,  4,  6,  8, 10], dtype=int32)>\n\nWarning: If one of the inputs (x or y) is a tensor and the other is a non-tensor, the non-tensor input will adopt (or get casted to) the data type of the tensor input. This can potentially cause unwanted overflow or underflow conversion.\n\nFor example,\n\nx = tf.constant([1, 2], dtype=tf.int8)\ny = [2**7 + 1, 2**7 + 2]\ntf.add(x, y)\n<tf.Tensor: shape=(2,), dtype=int8, numpy=array([-126, -124], dtype=int8)>\n\n\nWhen adding two input values of different shapes, Add follows NumPy broadcasting rules. The two input array shapes are compared element-wise. Starting with the trailing dimensions, the two dimensions either have to be equal or one of them needs to be 1.\n\nFor example,\n\nx = np.ones(6).reshape(1, 2, 1, 3)\ny = np.ones(6).reshape(2, 1, 3, 1)\ntf.add(x, y).shape.as_list()\n[2, 2, 3, 3]\n\n\nAnother example with two arrays of different dimension.\n\nx = np.ones([1, 2, 1, 4])\ny = np.ones([3, 4])\ntf.add(x, y).shape.as_list()\n[1, 2, 3, 4]\n\n\nThe reduction version of this elementwise operation is tf.math.reduce_sum\n\nArgs\nx\tA tf.Tensor. Must be one of the following types: bfloat16, half, float16, float32, float64, uint8, uint16, uint32, uint64, int8, int16, int32, int64, complex64, complex128, string.\ny\tA tf.Tensor. Must have the same type as x.\nname\tA name for the operation (optional)\n\n__and__\n\nView source\n\n__and__(\n    y, name=None\n)\n\n\nReturns the truth value of elementwise x & y.\n\nLogical AND function.\n\nRequires that x and y have the same shape or have broadcast-compatible shapes. For example, y can be:\n\nA single Python boolean, where the result will be calculated by applying logical AND with the single element to each element in x.\nA tf.Tensor object of dtype tf.bool of the same shape or broadcast-compatible shape. In this case, the result will be the element-wise logical AND of x and y.\nA tf.RaggedTensor object of dtype tf.bool of the same shape or broadcast-compatible shape. In this case, the result will be the element-wise logical AND of x and y.\nFor example:\n# `y` is a Python boolean\nx = tf.ragged.constant([[True, False], [True]])\ny = True\nx & y\n<tf.RaggedTensor [[True, False], [True]]>\ntf.math.logical_and(x, y)  # Equivalent of x & y\n<tf.RaggedTensor [[True, False], [True]]>\ny & x\n<tf.RaggedTensor [[True, False], [True]]>\ntf.math.reduce_all(x & y)  # Reduce to a scalar bool Tensor.\n<tf.Tensor: shape=(), dtype=bool, numpy=False>\n\n# `y` is a tf.Tensor of the same shape.\nx = tf.ragged.constant([[True, False], [True, False]])\ny = tf.constant([[True, False], [False, True]])\nx & y\n<tf.RaggedTensor [[True, False], [False, False]]>\n\n# `y` is a tf.Tensor of a broadcast-compatible shape.\nx = tf.ragged.constant([[True, False], [True]])\ny = tf.constant([[True], [False]])\nx & y\n<tf.RaggedTensor [[True, False], [False]]>\n\n# `y` is a `tf.RaggedTensor` of the same shape.\nx = tf.ragged.constant([[True, False], [True]])\ny = tf.ragged.constant([[False, True], [True]])\nx & y\n<tf.RaggedTensor [[False, False], [True]]>\n\n# `y` is a `tf.RaggedTensor` of a broadcast-compatible shape.\nx = tf.ragged.constant([[[True, True, False]], [[]], [[True, False]]])\ny = tf.ragged.constant([[[True]], [[True]], [[False]]], ragged_rank=1)\nx & y\n<tf.RaggedTensor [[[True, True, False]], [[]], [[False, False]]]>\n\n\nArgs\ny\tA Python boolean or a tf.Tensor or tf.RaggedTensor of dtype tf.bool.\nname\tA name for the operation (optional).\n\nReturns\nA tf.RaggedTensor of dtype tf.bool with the shape that x and y broadcast to.\n\n__bool__\n\nView source\n\n__bool__()\n\n\nRaises TypeError when a RaggedTensor is used as a Python bool.\n\nTo prevent RaggedTensor from being used as a bool, this function always raise TypeError when being called.\n\nFor example:\nx = tf.ragged.constant([[1, 2], [3]])\nresult = True if x else False  # Evaluate x as a bool value.\nTraceback (most recent call last):\n\nTypeError: RaggedTensor may not be used as a boolean.\n\nx = tf.ragged.constant([[1]])\nr = (x == 1)  # tf.RaggedTensor [[True]]\nif r:  # Evaluate r as a bool value.\n  pass\nTraceback (most recent call last):\n\nTypeError: RaggedTensor may not be used as a boolean.\n\n__div__\n\nView source\n\n__div__(\n    y, name=None\n)\n\n\nDivides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide.\n\nThis function divides x and y, forcing Python 2 semantics. That is, if x and y are both integers then the result will be an integer. This is in contrast to Python 3, where division with / is always a float while division with // is always an integer.\n\nArgs\nx\tTensor numerator of real numeric type.\ny\tTensor denominator of real numeric type.\nname\tA name for the operation (optional).\n\nReturns\nx / y returns the quotient of x and y.\n\nMigrate to TF2\n\n__eq__\n\nView source\n\n__eq__(\n    other\n)\n\n\nReturns result of elementwise == or False if not broadcast-compatible.\n\nCompares two ragged tensors elemewise for equality if they are broadcast-compatible; or returns False if they are not broadcast-compatible.\n\nNote that this behavior differs from tf.math.equal, which raises an exception if the two ragged tensors are not broadcast-compatible.\n\nFor example:\nrt1 = tf.ragged.constant([[1, 2], [3]])\nrt1 == rt1\n<tf.RaggedTensor [[True, True], [True]]>\n\nrt2 = tf.ragged.constant([[1, 2], [4]])\nrt1 == rt2\n<tf.RaggedTensor [[True, True], [False]]>\n\nrt3 = tf.ragged.constant([[1, 2], [3, 4]])\n# rt1 and rt3 are not broadcast-compatible.\nrt1 == rt3\nFalse\n\n# You can also compare a `tf.RaggedTensor` to a `tf.Tensor`.\nt = tf.constant([[1, 2], [3, 4]])\nrt1 == t\nFalse\nt == rt1\nFalse\nrt4 = tf.ragged.constant([[1, 2], [3, 4]])\nrt4 == t\n<tf.RaggedTensor [[True, True], [True, True]]>\nt == rt4\n<tf.RaggedTensor [[True, True], [True, True]]>\n\n\nArgs\nother\tThe right-hand side of the == operator.\n\nReturns\nThe ragged tensor result of the elementwise == operation, or False if the arguments are not broadcast-compatible.\n\n__floordiv__\n\nView source\n\n__floordiv__(\n    y, name=None\n)\n\n\nDivides x / y elementwise, rounding toward the most negative integer.\n\nMathematically, this is equivalent to floor(x / y). For example: floor(8.4 / 4.0) = floor(2.1) = 2.0 floor(-8.4 / 4.0) = floor(-2.1) = -3.0 This is equivalent to the '//' operator in Python 3.0 and above.\n\nNote: x and y must have the same type, and the result will have the same type as well.\n\nArgs\nx\tTensor numerator of real numeric type.\ny\tTensor denominator of real numeric type.\nname\tA name for the operation (optional).\n\nReturns\nx / y rounded toward -infinity.\n\nRaises\nTypeError\tIf the inputs are complex.\n\n__ge__\n\nView source\n\n__ge__(\n    other\n)\n\n\nElementwise >= comparison of two convertible-to-ragged-tensor values.\n\nComputes the elemewise >= comparison of two values that are convertible to ragged tenors, with broadcasting support. Raises an exception if two values are not broadcast-compatible.\n\nFor example:\nrt1 = tf.ragged.constant([[1, 2], [3]])\nrt1 >= rt1\n<tf.RaggedTensor [[True, True], [True]]>\n\nrt2 = tf.ragged.constant([[2, 1], [3]])\nrt1 >= rt2\n<tf.RaggedTensor [[False, True], [True]]>\n\nrt3 = tf.ragged.constant([[1, 2], [3, 4]])\n# rt1 and rt3 are not broadcast-compatible.\nrt1 >= rt3\nTraceback (most recent call last):\n\nInvalidArgumentError: ...\n\n# You can also compare a `tf.RaggedTensor` to a `tf.Tensor`.\nrt4 = tf.ragged.constant([[1, 2],[3, 4]])\nt1 = tf.constant([[2, 1], [4, 3]])\nrt4 >= t1\n<tf.RaggedTensor [[False, True],\n [False, True]]>\nt1 >= rt4\n<tf.RaggedTensor [[True, False],\n [True, False]]>\n\n# Compares a `tf.RaggedTensor` to a `tf.Tensor` with broadcasting.\nt2 = tf.constant([[2]])\nrt4 >= t2\n<tf.RaggedTensor [[False, True],\n [True, True]]>\nt2 >= rt4\n<tf.RaggedTensor [[True, True],\n [False, False]]>\n\n\nArgs\nother\tThe right-hand side of the >= operator.\n\nReturns\nA tf.RaggedTensor of dtype tf.bool with the shape that self and other broadcast to.\n\nRaises\nInvalidArgumentError\tIf self and other are not broadcast-compatible.\n\n__getitem__\n\nView source\n\n__getitem__(\n    key\n)\n\n\nReturns the specified piece of this RaggedTensor.\n\nSupports multidimensional indexing and slicing, with one restriction: indexing into a ragged inner dimension is not allowed. This case is problematic because the indicated value may exist in some rows but not others. In such cases, it's not obvious whether we should (1) report an IndexError; (2) use a default value; or (3) skip that value and return a tensor with fewer rows than we started with. Following the guiding principles of Python (\"In the face of ambiguity, refuse the temptation to guess\"), we simply disallow this operation.\n\nArgs\nrt_input\tThe RaggedTensor to slice.\nkey\tIndicates which piece of the RaggedTensor to return, using standard Python semantics (e.g., negative values index from the end). key may have any of the following types:\n\nint constant\nScalar integer Tensor\n\nslice containing integer constants and/or scalar integer Tensors\n\nEllipsis\n\ntf.newaxis\n\ntuple containing any of the above (for multidimensional indexing)\n\nReturns\nA Tensor or RaggedTensor object. Values that include at least one ragged dimension are returned as RaggedTensor. Values that include no ragged dimensions are returned as Tensor. See above for examples of expressions that return Tensors vs RaggedTensors.\n\nRaises\nValueError\tIf key is out of bounds.\nValueError\tIf key is not supported.\nTypeError\tIf the indices in key have an unsupported type.\n\nExamples:\n# A 2-D ragged tensor with 1 ragged dimension.\nrt = tf.ragged.constant([['a', 'b', 'c'], ['d', 'e'], ['f'], ['g']])\nrt[0].numpy()                 # First row (1-D `Tensor`)\narray([b'a', b'b', b'c'], dtype=object)\nrt[:3].to_list()              # First three rows (2-D RaggedTensor)\n[[b'a', b'b', b'c'], [b'd', b'e'], [b'f']]\nrt[3, 0].numpy()              # 1st element of 4th row (scalar)\nb'g'\n\n# A 3-D ragged tensor with 2 ragged dimensions.\nrt = tf.ragged.constant([[[1, 2, 3], [4]],\n                         [[5], [], [6]],\n                         [[7]],\n                         [[8, 9], [10]]])\nrt[1].to_list()               # Second row (2-D RaggedTensor)\n[[5], [], [6]]\nrt[3, 0].numpy()              # First element of fourth row (1-D Tensor)\narray([8, 9], dtype=int32)\nrt[:, 1:3].to_list()          # Items 1-3 of each row (3-D RaggedTensor)\n[[[4]], [[], [6]], [], [[10]]]\nrt[:, -1:].to_list()          # Last item of each row (3-D RaggedTensor)\n[[[4]], [[6]], [[7]], [[10]]]\n\n__gt__\n__gt__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x > y) element-wise.\n\nNote: math.greater supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==> [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==> [False, False, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__invert__\n__invert__(\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of NOT x element-wise.\n\nExample:\ntf.math.logical_not(tf.constant([True, False]))\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n\n\nArgs\nx\tA Tensor of type bool. A Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__le__\n__le__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x <= y) element-wise.\n\nNote: math.less_equal supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==> [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==> [True, True, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__lt__\n__lt__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of (x < y) element-wise.\n\nNote: math.less supports broadcasting. More about broadcasting here\nExample:\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==> [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==> [False, True, True]\n\n\nArgs\nx\tA Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__mod__\n__mod__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns element-wise remainder of division.\n\nThis follows Python semantics in that the result here is consistent with a flooring divide. E.g. floor(x / y) * y + floormod(x, y) = x, regardless of the signs of x and y.\n\nNote: math.floormod supports broadcasting. More about broadcasting here\n\nArgs\nx\tA Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64, bfloat16, half, float32, float64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor. Has the same type as x.\n\n__mul__\n\nView source\n\n__mul__(\n    y, name=None\n)\n\n\nReturns an element-wise x * y.\n\nFor example:\nx = tf.constant(([1, 2, 3, 4]))\ntf.math.multiply(x, x)\n<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>\n\n\nSince tf.math.multiply will convert its arguments to Tensors, you can also pass in non-Tensor arguments:\n\ntf.math.multiply(7,6)\n<tf.Tensor: shape=(), dtype=int32, numpy=42>\n\n\nIf x.shape is not the same as y.shape, they will be broadcast to a compatible shape. (More about broadcasting here.)\n\nFor example:\nx = tf.ones([1, 2]);\ny = tf.ones([2, 1]);\nx * y  # Taking advantage of operator overriding\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1., 1.],\n     [1., 1.]], dtype=float32)>\n\n\nThe reduction version of this elementwise operation is tf.math.reduce_prod\n\nArgs\nx\tA Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\n\nA Tensor. Has the same type as x.\n\nRaises\n\n\nInvalidArgumentError: When x and y have incompatible shapes or types.\n\n__ne__\n\nView source\n\n__ne__(\n    other\n)\n\n\nThe operation invoked by the Tensor.ne operator.\n\nCompares two tensors element-wise for inequality if they are broadcast-compatible; or returns True if they are not broadcast-compatible. (Note that this behavior differs from tf.math.not_equal, which raises an exception if the two tensors are not broadcast-compatible.)\n\nPurpose in the API\nThis method is exposed in TensorFlow's API so that library developers can register dispatching for Tensor.ne to allow it to handle custom composite tensors & other custom objects.\n\nThe API symbol is not intended to be called by users directly and does appear in TensorFlow's generated documentation.\n\nArgs\nself\tThe left-hand side of the != operator.\nother\tThe right-hand side of the != operator.\n\nReturns\nThe result of the elementwise != operation, or True if the arguments are not broadcast-compatible.\n\n__neg__\n__neg__(\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nComputes numerical negative value element-wise.\n\nI.e., y=−x.\n\nArgs\nx\tA Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor. Has the same type as x.\n\nIf x is a SparseTensor, returns SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)\n\n__nonzero__\n\nView source\n\n__nonzero__()\n\n\nRaises TypeError when a RaggedTensor is used as a Python bool.\n\nTo prevent RaggedTensor from being used as a bool, this function always raise TypeError when being called.\n\nFor example:\nx = tf.ragged.constant([[1, 2], [3]])\nresult = True if x else False  # Evaluate x as a bool value.\nTraceback (most recent call last):\n\nTypeError: RaggedTensor may not be used as a boolean.\n\nx = tf.ragged.constant([[1]])\nr = (x == 1)  # tf.RaggedTensor [[True]]\nif r:  # Evaluate r as a bool value.\n  pass\nTraceback (most recent call last):\n\nTypeError: RaggedTensor may not be used as a boolean.\n\n__or__\n__or__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of x OR y element-wise.\n\nLogical OR function.\n\nRequires that x and y have the same shape or have broadcast-compatible shapes. For example, x and y can be:\n\nTwo single elements of type bool.\nOne tf.Tensor of type bool and one single bool, where the result will be calculated by applying logical OR with the single element to each element in the larger Tensor.\nTwo tf.Tensor objects of type bool of the same shape. In this case, the result will be the element-wise logical OR of the two input tensors.\n\nYou can also use the | operator instead.\n\nUsage\n\n\n>>> a = tf.constant([True])\n>>> b = tf.constant([False])\n>>> tf.math.logical_or(a, b)\n<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n>>> a | b\n<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n\nc = tf.constant([False])\nx = tf.constant([False, True, True, False])\ntf.math.logical_or(c, x)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>\nc | x\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>\n\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_or(y, z)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>\ny | z\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>\n\n\nThis op also supports broadcasting\n\ntf.logical_or([[True, False]], [[True], [False]])\n<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\narray([[ True,  True],\n     [ True, False]])>\n\n\nThe reduction version of this elementwise operation is tf.math.reduce_any.\n\nArgs\nx\tA tf.Tensor of type bool.\ny\tA tf.Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA tf.Tensor of type bool with the shape that x and y broadcast to.\n\nArgs\nx\tA Tensor of type bool.\ny\tA Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__pow__\n\nView source\n\n__pow__(\n    y, name=None\n)\n\n\nComputes the power of one value to another.\n\nGiven a tensor x and a tensor y, this operation computes xy for corresponding elements in x and y. For example:\n\nx = tf.constant([[2, 2], [3, 3]])\ny = tf.constant([[8, 16], [2, 3]])\ntf.pow(x, y)  # [[256, 65536], [9, 27]]\n\n\nArgs\nx\tA Tensor of type float16, float32, float64, int32, int64, complex64, or complex128.\ny\tA Tensor of type float16, float32, float64, int32, int64, complex64, or complex128.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor.\n\n__radd__\n\nView source\n\n__radd__(\n    y, name=None\n)\n\n\nReturns x + y element-wise.\n\nExample usages below.\n\nAdd a scalar and a list:\n\nx = [1, 2, 3, 4, 5]\ny = 1\ntf.add(x, y)\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],\ndtype=int32)>\n\n\nNote that binary + operator can be used instead:\n\nx = tf.convert_to_tensor([1, 2, 3, 4, 5])\ny = tf.convert_to_tensor(1)\nx + y\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],\ndtype=int32)>\n\n\nAdd a tensor and a list of same shape:\n\nx = [1, 2, 3, 4, 5]\ny = tf.constant([1, 2, 3, 4, 5])\ntf.add(x, y)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 2,  4,  6,  8, 10], dtype=int32)>\n\nWarning: If one of the inputs (x or y) is a tensor and the other is a non-tensor, the non-tensor input will adopt (or get casted to) the data type of the tensor input. This can potentially cause unwanted overflow or underflow conversion.\n\nFor example,\n\nx = tf.constant([1, 2], dtype=tf.int8)\ny = [2**7 + 1, 2**7 + 2]\ntf.add(x, y)\n<tf.Tensor: shape=(2,), dtype=int8, numpy=array([-126, -124], dtype=int8)>\n\n\nWhen adding two input values of different shapes, Add follows NumPy broadcasting rules. The two input array shapes are compared element-wise. Starting with the trailing dimensions, the two dimensions either have to be equal or one of them needs to be 1.\n\nFor example,\n\nx = np.ones(6).reshape(1, 2, 1, 3)\ny = np.ones(6).reshape(2, 1, 3, 1)\ntf.add(x, y).shape.as_list()\n[2, 2, 3, 3]\n\n\nAnother example with two arrays of different dimension.\n\nx = np.ones([1, 2, 1, 4])\ny = np.ones([3, 4])\ntf.add(x, y).shape.as_list()\n[1, 2, 3, 4]\n\n\nThe reduction version of this elementwise operation is tf.math.reduce_sum\n\nArgs\nx\tA tf.Tensor. Must be one of the following types: bfloat16, half, float16, float32, float64, uint8, uint16, uint32, uint64, int8, int16, int32, int64, complex64, complex128, string.\ny\tA tf.Tensor. Must have the same type as x.\nname\tA name for the operation (optional)\n\n__rand__\n\nView source\n\n__rand__(\n    y, name=None\n)\n\n\nReturns the truth value of elementwise x & y.\n\nLogical AND function.\n\nRequires that x and y have the same shape or have broadcast-compatible shapes. For example, y can be:\n\nA single Python boolean, where the result will be calculated by applying logical AND with the single element to each element in x.\nA tf.Tensor object of dtype tf.bool of the same shape or broadcast-compatible shape. In this case, the result will be the element-wise logical AND of x and y.\nA tf.RaggedTensor object of dtype tf.bool of the same shape or broadcast-compatible shape. In this case, the result will be the element-wise logical AND of x and y.\nFor example:\n# `y` is a Python boolean\nx = tf.ragged.constant([[True, False], [True]])\ny = True\nx & y\n<tf.RaggedTensor [[True, False], [True]]>\ntf.math.logical_and(x, y)  # Equivalent of x & y\n<tf.RaggedTensor [[True, False], [True]]>\ny & x\n<tf.RaggedTensor [[True, False], [True]]>\ntf.math.reduce_all(x & y)  # Reduce to a scalar bool Tensor.\n<tf.Tensor: shape=(), dtype=bool, numpy=False>\n\n# `y` is a tf.Tensor of the same shape.\nx = tf.ragged.constant([[True, False], [True, False]])\ny = tf.constant([[True, False], [False, True]])\nx & y\n<tf.RaggedTensor [[True, False], [False, False]]>\n\n# `y` is a tf.Tensor of a broadcast-compatible shape.\nx = tf.ragged.constant([[True, False], [True]])\ny = tf.constant([[True], [False]])\nx & y\n<tf.RaggedTensor [[True, False], [False]]>\n\n# `y` is a `tf.RaggedTensor` of the same shape.\nx = tf.ragged.constant([[True, False], [True]])\ny = tf.ragged.constant([[False, True], [True]])\nx & y\n<tf.RaggedTensor [[False, False], [True]]>\n\n# `y` is a `tf.RaggedTensor` of a broadcast-compatible shape.\nx = tf.ragged.constant([[[True, True, False]], [[]], [[True, False]]])\ny = tf.ragged.constant([[[True]], [[True]], [[False]]], ragged_rank=1)\nx & y\n<tf.RaggedTensor [[[True, True, False]], [[]], [[False, False]]]>\n\n\nArgs\ny\tA Python boolean or a tf.Tensor or tf.RaggedTensor of dtype tf.bool.\nname\tA name for the operation (optional).\n\nReturns\nA tf.RaggedTensor of dtype tf.bool with the shape that x and y broadcast to.\n\n__rdiv__\n\nView source\n\n__rdiv__(\n    y, name=None\n)\n\n\nDivides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide.\n\nThis function divides x and y, forcing Python 2 semantics. That is, if x and y are both integers then the result will be an integer. This is in contrast to Python 3, where division with / is always a float while division with // is always an integer.\n\nArgs\nx\tTensor numerator of real numeric type.\ny\tTensor denominator of real numeric type.\nname\tA name for the operation (optional).\n\nReturns\nx / y returns the quotient of x and y.\n\nMigrate to TF2\n\n__rfloordiv__\n\nView source\n\n__rfloordiv__(\n    y, name=None\n)\n\n\nDivides x / y elementwise, rounding toward the most negative integer.\n\nMathematically, this is equivalent to floor(x / y). For example: floor(8.4 / 4.0) = floor(2.1) = 2.0 floor(-8.4 / 4.0) = floor(-2.1) = -3.0 This is equivalent to the '//' operator in Python 3.0 and above.\n\nNote: x and y must have the same type, and the result will have the same type as well.\n\nArgs\nx\tTensor numerator of real numeric type.\ny\tTensor denominator of real numeric type.\nname\tA name for the operation (optional).\n\nReturns\nx / y rounded toward -infinity.\n\nRaises\nTypeError\tIf the inputs are complex.\n\n__rmod__\n__rmod__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns element-wise remainder of division.\n\nThis follows Python semantics in that the result here is consistent with a flooring divide. E.g. floor(x / y) * y + floormod(x, y) = x, regardless of the signs of x and y.\n\nNote: math.floormod supports broadcasting. More about broadcasting here\n\nArgs\nx\tA Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64, bfloat16, half, float32, float64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor. Has the same type as x.\n\n__rmul__\n\nView source\n\n__rmul__(\n    y, name=None\n)\n\n\nReturns an element-wise x * y.\n\nFor example:\nx = tf.constant(([1, 2, 3, 4]))\ntf.math.multiply(x, x)\n<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>\n\n\nSince tf.math.multiply will convert its arguments to Tensors, you can also pass in non-Tensor arguments:\n\ntf.math.multiply(7,6)\n<tf.Tensor: shape=(), dtype=int32, numpy=42>\n\n\nIf x.shape is not the same as y.shape, they will be broadcast to a compatible shape. (More about broadcasting here.)\n\nFor example:\nx = tf.ones([1, 2]);\ny = tf.ones([2, 1]);\nx * y  # Taking advantage of operator overriding\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1., 1.],\n     [1., 1.]], dtype=float32)>\n\n\nThe reduction version of this elementwise operation is tf.math.reduce_prod\n\nArgs\nx\tA Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\n\nA Tensor. Has the same type as x.\n\nRaises\n\n\nInvalidArgumentError: When x and y have incompatible shapes or types.\n\n__ror__\n__ror__(\n    y: Annotated[Any, tf.raw_ops.Any],\n    name=None\n) -> Annotated[Any, tf.raw_ops.Any]\n\n\nReturns the truth value of x OR y element-wise.\n\nLogical OR function.\n\nRequires that x and y have the same shape or have broadcast-compatible shapes. For example, x and y can be:\n\nTwo single elements of type bool.\nOne tf.Tensor of type bool and one single bool, where the result will be calculated by applying logical OR with the single element to each element in the larger Tensor.\nTwo tf.Tensor objects of type bool of the same shape. In this case, the result will be the element-wise logical OR of the two input tensors.\n\nYou can also use the | operator instead.\n\nUsage\n\n\n>>> a = tf.constant([True])\n>>> b = tf.constant([False])\n>>> tf.math.logical_or(a, b)\n<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n>>> a | b\n<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n\nc = tf.constant([False])\nx = tf.constant([False, True, True, False])\ntf.math.logical_or(c, x)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>\nc | x\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>\n\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_or(y, z)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>\ny | z\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>\n\n\nThis op also supports broadcasting\n\ntf.logical_or([[True, False]], [[True], [False]])\n<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\narray([[ True,  True],\n     [ True, False]])>\n\n\nThe reduction version of this elementwise operation is tf.math.reduce_any.\n\nArgs\nx\tA tf.Tensor of type bool.\ny\tA tf.Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA tf.Tensor of type bool with the shape that x and y broadcast to.\n\nArgs\nx\tA Tensor of type bool.\ny\tA Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor of type bool.\n\n__rpow__\n\nView source\n\n__rpow__(\n    y, name=None\n)\n\n\nComputes the power of one value to another.\n\nGiven a tensor x and a tensor y, this operation computes xy for corresponding elements in x and y. For example:\n\nx = tf.constant([[2, 2], [3, 3]])\ny = tf.constant([[8, 16], [2, 3]])\ntf.pow(x, y)  # [[256, 65536], [9, 27]]\n\n\nArgs\nx\tA Tensor of type float16, float32, float64, int32, int64, complex64, or complex128.\ny\tA Tensor of type float16, float32, float64, int32, int64, complex64, or complex128.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor.\n\n__rsub__\n\nView source\n\n__rsub__(\n    y, name=None\n)\n\n\nReturns x - y element-wise.\n\nNote: tf.subtract supports broadcasting. More about broadcasting here\n\nBoth input and output have a range (-inf, inf).\n\nExample usages below.\n\nSubtract operation between an array and a scalar:\n\nx = [1, 2, 3, 4, 5]\ny = 1\ntf.subtract(x, y)\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>\ntf.subtract(y, x)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 0, -1, -2, -3, -4], dtype=int32)>\n\n\nNote that binary - operator can be used instead:\n\nx = tf.convert_to_tensor([1, 2, 3, 4, 5])\ny = tf.convert_to_tensor(1)\nx - y\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>\n\n\nSubtract operation between an array and a tensor of same shape:\n\nx = [1, 2, 3, 4, 5]\ny = tf.constant([5, 4, 3, 2, 1])\ntf.subtract(y, x)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 4,  2,  0, -2, -4], dtype=int32)>\n\nWarning: If one of the inputs (x or y) is a tensor and the other is a non-tensor, the non-tensor input will adopt (or get casted to) the data type of the tensor input. This can potentially cause unwanted overflow or underflow conversion.\n\nFor example,\n\nx = tf.constant([1, 2], dtype=tf.int8)\ny = [2**8 + 1, 2**8 + 2]\ntf.subtract(x, y)\n<tf.Tensor: shape=(2,), dtype=int8, numpy=array([0, 0], dtype=int8)>\n\n\nWhen subtracting two input values of different shapes, tf.subtract follows the general broadcasting rules . The two input array shapes are compared element-wise. Starting with the trailing dimensions, the two dimensions either have to be equal or one of them needs to be 1.\n\nFor example,\n\nx = np.ones(6).reshape(2, 3, 1)\ny = np.ones(6).reshape(2, 1, 3)\ntf.subtract(x, y)\n<tf.Tensor: shape=(2, 3, 3), dtype=float64, numpy=\narray([[[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]],\n       [[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]])>\n\n\nExample with inputs of different dimensions:\n\nx = np.ones(6).reshape(2, 3, 1)\ny = np.ones(6).reshape(1, 6)\ntf.subtract(x, y)\n<tf.Tensor: shape=(2, 3, 6), dtype=float64, numpy=\narray([[[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]],\n       [[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]]])>\n\n\nArgs\nx\tA Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor. Has the same type as x.\n\n__rtruediv__\n\nView source\n\n__rtruediv__(\n    y, name=None\n)\n\n\nDivides x / y elementwise (using Python 3 division operator semantics).\n\nNote: Prefer using the Tensor operator or tf.divide which obey Python division operator semantics.\n\nThis function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal x / y division in Python 3 and in Python 2.7 with from __future__ import division. If you want integer division that rounds down, use x // y or tf.math.floordiv.\n\nx and y must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to float32 for int8 and int16 and float64 for int32 and int64 (matching the behavior of Numpy).\n\nArgs\nx\tTensor numerator of numeric type.\ny\tTensor denominator of numeric type.\nname\tA name for the operation (optional).\n\nReturns\nx / y evaluated in floating point.\n\nRaises\nTypeError\tIf x and y have different dtypes.\n\n__rxor__\n\nView source\n\n__rxor__(\n    y, name='LogicalXor'\n)\n\n\nLogical XOR function.\n\nx ^ y = (x | y) & ~(x & y)\n\nRequires that x and y have the same shape or have broadcast-compatible shapes. For example, x and y can be:\n\nTwo single elements of type bool\nOne tf.Tensor of type bool and one single bool, where the result will be calculated by applying logical XOR with the single element to each element in the larger Tensor.\nTwo tf.Tensor objects of type bool of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.\nUsage:\na = tf.constant([True])\nb = tf.constant([False])\ntf.math.logical_xor(a, b)\n<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n\nc = tf.constant([True])\nx = tf.constant([False, True, True, False])\ntf.math.logical_xor(c, x)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_xor(y, z)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n\n\nArgs\nx\tA tf.Tensor type bool.\ny\tA tf.Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA tf.Tensor of type bool with the same size as that of x or y.\n\n__sub__\n\nView source\n\n__sub__(\n    y, name=None\n)\n\n\nReturns x - y element-wise.\n\nNote: tf.subtract supports broadcasting. More about broadcasting here\n\nBoth input and output have a range (-inf, inf).\n\nExample usages below.\n\nSubtract operation between an array and a scalar:\n\nx = [1, 2, 3, 4, 5]\ny = 1\ntf.subtract(x, y)\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>\ntf.subtract(y, x)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 0, -1, -2, -3, -4], dtype=int32)>\n\n\nNote that binary - operator can be used instead:\n\nx = tf.convert_to_tensor([1, 2, 3, 4, 5])\ny = tf.convert_to_tensor(1)\nx - y\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>\n\n\nSubtract operation between an array and a tensor of same shape:\n\nx = [1, 2, 3, 4, 5]\ny = tf.constant([5, 4, 3, 2, 1])\ntf.subtract(y, x)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 4,  2,  0, -2, -4], dtype=int32)>\n\nWarning: If one of the inputs (x or y) is a tensor and the other is a non-tensor, the non-tensor input will adopt (or get casted to) the data type of the tensor input. This can potentially cause unwanted overflow or underflow conversion.\n\nFor example,\n\nx = tf.constant([1, 2], dtype=tf.int8)\ny = [2**8 + 1, 2**8 + 2]\ntf.subtract(x, y)\n<tf.Tensor: shape=(2,), dtype=int8, numpy=array([0, 0], dtype=int8)>\n\n\nWhen subtracting two input values of different shapes, tf.subtract follows the general broadcasting rules . The two input array shapes are compared element-wise. Starting with the trailing dimensions, the two dimensions either have to be equal or one of them needs to be 1.\n\nFor example,\n\nx = np.ones(6).reshape(2, 3, 1)\ny = np.ones(6).reshape(2, 1, 3)\ntf.subtract(x, y)\n<tf.Tensor: shape=(2, 3, 3), dtype=float64, numpy=\narray([[[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]],\n       [[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]])>\n\n\nExample with inputs of different dimensions:\n\nx = np.ones(6).reshape(2, 3, 1)\ny = np.ones(6).reshape(1, 6)\ntf.subtract(x, y)\n<tf.Tensor: shape=(2, 3, 6), dtype=float64, numpy=\narray([[[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]],\n       [[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]]])>\n\n\nArgs\nx\tA Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128, uint32, uint64.\ny\tA Tensor. Must have the same type as x.\nname\tA name for the operation (optional).\n\nReturns\nA Tensor. Has the same type as x.\n\n__truediv__\n\nView source\n\n__truediv__(\n    y, name=None\n)\n\n\nDivides x / y elementwise (using Python 3 division operator semantics).\n\nNote: Prefer using the Tensor operator or tf.divide which obey Python division operator semantics.\n\nThis function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal x / y division in Python 3 and in Python 2.7 with from __future__ import division. If you want integer division that rounds down, use x // y or tf.math.floordiv.\n\nx and y must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to float32 for int8 and int16 and float64 for int32 and int64 (matching the behavior of Numpy).\n\nArgs\nx\tTensor numerator of numeric type.\ny\tTensor denominator of numeric type.\nname\tA name for the operation (optional).\n\nReturns\nx / y evaluated in floating point.\n\nRaises\nTypeError\tIf x and y have different dtypes.\n\n__xor__\n\nView source\n\n__xor__(\n    y, name='LogicalXor'\n)\n\n\nLogical XOR function.\n\nx ^ y = (x | y) & ~(x & y)\n\nRequires that x and y have the same shape or have broadcast-compatible shapes. For example, x and y can be:\n\nTwo single elements of type bool\nOne tf.Tensor of type bool and one single bool, where the result will be calculated by applying logical XOR with the single element to each element in the larger Tensor.\nTwo tf.Tensor objects of type bool of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.\nUsage:\na = tf.constant([True])\nb = tf.constant([False])\ntf.math.logical_xor(a, b)\n<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n\nc = tf.constant([True])\nx = tf.constant([False, True, True, False])\ntf.math.logical_xor(c, x)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_xor(y, z)\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n\n\nArgs\nx\tA tf.Tensor type bool.\ny\tA tf.Tensor of type bool.\nname\tA name for the operation (optional).\n\nReturns\nA tf.Tensor of type bool with the same size as that of x or y.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.OptionalSpec  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/OptionalSpec",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nAttributes\nMethods\nexperimental_as_proto\nexperimental_from_proto\nexperimental_type_proto\nfrom_value\nis_compatible_with\nis_subtype_of\nmost_specific_common_supertype\nmost_specific_compatible_type\n__eq__\n__ne__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\ntf.OptionalSpec \nbookmark_border\n\nView source on GitHub\n\nType specification for tf.experimental.Optional.\n\nInherits From: TypeSpec, TraceType\n\nView aliases\ntf.OptionalSpec(\n    element_spec\n)\n\n\nFor instance, tf.OptionalSpec can be used to define a tf.function that takes tf.experimental.Optional as an input argument:\n\n@tf.function(input_signature=[tf.OptionalSpec(\n  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])\ndef maybe_square(optional):\n  if optional.has_value():\n    x = optional.get_value()\n    return x * x\n  return -1\noptional = tf.experimental.Optional.from_value(5)\nprint(maybe_square(optional))\ntf.Tensor(25, shape=(), dtype=int32)\n\n\nAttributes\n\nelement_spec\tA (nested) structure of TypeSpec objects that represents the type specification of the optional element.\nvalue_type\tThe Python type for values that are compatible with this TypeSpec.\n\nIn particular, all values that are compatible with this TypeSpec must be an instance of this type.\n\nMethods\nexperimental_as_proto\n\nView source\n\nexperimental_as_proto() -> struct_pb2.TypeSpecProto\n\n\nReturns a proto representation of the TypeSpec instance.\n\nDo NOT override for custom non-TF types.\n\nexperimental_from_proto\n\nView source\n\n@classmethod\nexperimental_from_proto(\n    proto: struct_pb2.TypeSpecProto\n) -> 'TypeSpec'\n\n\nReturns a TypeSpec instance based on the serialized proto.\n\nDo NOT override for custom non-TF types.\n\nArgs\nproto\tProto generated using 'experimental_as_proto'.\n\nexperimental_type_proto\n\nView source\n\n@classmethod\nexperimental_type_proto() -> Type[struct_pb2.TypeSpecProto]\n\n\nReturns the type of proto associated with TypeSpec serialization.\n\nDo NOT override for custom non-TF types.\n\nfrom_value\n\nView source\n\n@staticmethod\nfrom_value(\n    value\n)\n\nis_compatible_with\n\nView source\n\nis_compatible_with(\n    spec_or_value\n)\n\n\nReturns true if spec_or_value is compatible with this TypeSpec.\n\nPrefer using \"is_subtype_of\" and \"most_specific_common_supertype\" wherever possible.\n\nArgs\nspec_or_value\tA TypeSpec or TypeSpec associated value to compare against.\n\nis_subtype_of\n\nView source\n\nis_subtype_of(\n    other: tf.types.experimental.TraceType\n) -> bool\n\n\nReturns True if self is a subtype of other.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nother\tA TraceType object.\n\nmost_specific_common_supertype\n\nView source\n\nmost_specific_common_supertype(\n    others: Sequence[tf.types.experimental.TraceType]\n) -> Optional['TypeSpec']\n\n\nReturns the most specific supertype TypeSpec of self and others.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nothers\tA sequence of TraceTypes.\n\nmost_specific_compatible_type\n\nView source\n\nmost_specific_compatible_type(\n    other: 'TypeSpec'\n) -> 'TypeSpec'\n\n\nReturns the most specific TypeSpec compatible with self and other. (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use most_specific_common_supertype instead.\n\nDeprecated. Please use most_specific_common_supertype instead. Do not override this function.\n\nArgs\nother\tA TypeSpec.\n\nRaises\nValueError\tIf there is no TypeSpec that is compatible with both self and other.\n\n__eq__\n\nView source\n\n__eq__(\n    other\n) -> bool\n\n\nReturn self==value.\n\n__ne__\n\nView source\n\n__ne__(\n    other\n) -> bool\n\n\nReturn self!=value.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.Operation  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/Operation",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nAttributes\nMethods\ncolocation_groups\nexperimental_set_type\nfrom_node_def\nget_attr\nrun\nvalues\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\ntf.Operation \nbookmark_border\n\nView source on GitHub\n\nRepresents a graph node that performs computation on tensors.\n\nView aliases\ntf.Operation(\n    *args, **kwargs\n)\n\n\nAn Operation is a node in a tf.Graph that takes zero or more Tensor objects as input, and produces zero or more Tensor objects as output. Objects of type Operation are created by calling a Python op constructor (such as tf.matmul) within a tf.function or under a tf.Graph.as_default context manager.\n\nFor example, within a tf.function, c = tf.matmul(a, b) creates an Operation of type \"MatMul\" that takes tensors a and b as input, and produces c as output.\n\nIf a tf.compat.v1.Session is used, an Operation of a tf.Graph can be executed by passing it to tf.Session.run. op.run() is a shortcut for calling tf.compat.v1.get_default_session().run(op).\n\nAttributes\n\ncontrol_inputs\tThe Operation objects on which this op has a control dependency.\n\nBefore this op is executed, TensorFlow will ensure that the operations in self.control_inputs have finished executing. This mechanism can be used to run ops sequentially for performance reasons, or to ensure that the side effects of an op are observed in the correct order.\n\n\ndevice\tThe name of the device to which this op has been assigned, if any.\ngraph\t\n\n\ninputs\tThe sequence of Tensor objects representing the data inputs of this op.\nname\t\n\n\nnode_def\t\n\n\nop_def\t\n\n\noutputs\t\n\n\ntraceback\tReturns the call stack from when this operation was constructed.\ntype\t\n\nMethods\ncolocation_groups\n\nView source\n\ncolocation_groups() -> list[bytes]\n\n\nReturns the list of colocation groups of the op.\n\nexperimental_set_type\n\nView source\n\nexperimental_set_type(\n    type_proto\n) -> None\n\n\nSets the corresponding node's experimental_type field.\n\nSee the description of NodeDef.experimental_type for more info.\n\nArgs\ntype_proto\tA FullTypeDef proto message. The root type_if of this object must be TFT_PRODUCT, even for ops which only have a singlre return value.\n\nfrom_node_def\n\nView source\n\n@classmethod\nfrom_node_def(\n    node_def,\n    g,\n    inputs=None,\n    output_types=None,\n    control_inputs=None,\n    input_types=None,\n    original_op=None,\n    op_def=None\n) -> OperationType\n\n\nCreates an Operation.\n\nNote: This constructor validates the name of the Operation (passed as node_def.name). Valid Operation names match the following regular expression:\n[A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*\n\n\nArgs\nnode_def\tnode_def_pb2.NodeDef. NodeDef for the Operation. Used for attributes of node_def_pb2.NodeDef, typically name, op, and device. The input attribute is irrelevant here as it will be computed when generating the model.\ng\tGraph. The parent graph.\ninputs\tlist of Tensor objects. The inputs to this Operation.\noutput_types\tlist of DType objects. List of the types of the Tensors computed by this operation. The length of this list indicates the number of output endpoints of the Operation.\ncontrol_inputs\tlist of operations or tensors from which to have a control dependency.\ninput_types\tList of DType objects representing the types of the tensors accepted by the Operation. By default uses [x.dtype.base_dtype for x in inputs]. Operations that expect reference-typed inputs must specify these explicitly.\noriginal_op\tOptional. Used to associate the new Operation with an existing Operation (for example, a replica with the op that was replicated).\nop_def\tOptional. The op_def_pb2.OpDef proto that describes the op type that this Operation represents.\n\nRaises\nTypeError\tif control inputs are not Operations or Tensors, or if node_def is not a NodeDef, or if g is not a Graph, or if inputs are not tensors, or if inputs and input_types are incompatible.\nValueError\tif the node_def name is not valid.\n\nReturns\nOperation object.\n\nget_attr\n\nView source\n\nget_attr(\n    name\n)\n\n\nReturns the value of the attr of this op with the given name.\n\nArgs\nname\tThe name of the attr to fetch.\n\nReturns\nThe value of the attr, as a Python object.\n\nRaises\nValueError\tIf this op does not have an attr with the given name.\n\nrun\n\nView source\n\nrun(\n    feed_dict=None, session=None\n) -> None\n\n\nRuns this operation in a Session.\n\nCalling this method will execute all preceding operations that produce the inputs needed for this operation.\n\nNote: Before invoking Operation.run(), its graph must have been launched in a session, and either a default session must be available, or session must be specified explicitly.\n\nArgs\nfeed_dict\tA dictionary that maps Tensor objects to feed values. See tf.Session.run for a description of the valid feed values.\nsession\t(Optional.) The Session to be used to run to this operation. If none, the default session will be used.\n\nvalues\n\nView source\n\nvalues() -> tuple[Any, ...]\n\nDeprecated: Use outputs.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.Module  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/Module",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nAttributes\nMethods\nwith_name_scope\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.Module \nbookmark_border\n\nView source on GitHub\n\nBase neural network module class.\n\nView aliases\ntf.Module(\n    name=None\n)\n\nUsed in the notebooks\nUsed in the guide\tUsed in the tutorials\n\nIntroduction to modules, layers, and models\nBetter performance with tf.function\nDistributed training with Core APIs and DTensor\nLogistic regression for binary classification with Core APIs\nMultilayer perceptrons for digit recognition with Core APIs\n\t\nDistributed training with DTensors\nDeepDream\nSimple audio recognition: Recognizing keywords\nNeural machine translation with attention\nNeural machine translation with a Transformer and Keras\n\nA module is a named container for tf.Variables, other tf.Modules and functions which apply to user input. For example a dense layer in a neural network might be implemented as a tf.Module:\n\nclass Dense(tf.Module):\n  def __init__(self, input_dim, output_size, name=None):\n    super().__init__(name=name)\n    self.w = tf.Variable(\n      tf.random.normal([input_dim, output_size]), name='w')\n    self.b = tf.Variable(tf.zeros([output_size]), name='b')\n  def __call__(self, x):\n    y = tf.matmul(x, self.w) + self.b\n    return tf.nn.relu(y)\n\n\nYou can use the Dense layer as you would expect:\n\nd = Dense(input_dim=3, output_size=2)\nd(tf.ones([1, 3]))\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=..., dtype=float32)>\n\n\nBy subclassing tf.Module instead of object any tf.Variable or tf.Module instances assigned to object properties can be collected using the variables, trainable_variables or submodules property:\n\nd.variables\n    (<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=...,\n    dtype=float32)>,\n    <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=..., dtype=float32)>)\n\n\nSubclasses of tf.Module can also take advantage of the _flatten method which can be used to implement tracking of any other types.\n\nAll tf.Module classes have an associated tf.name_scope which can be used to group operations in TensorBoard and create hierarchies for variable names which can help with debugging. We suggest using the name scope when creating nested submodules/parameters or for forward methods whose graph you might want to inspect in TensorBoard. You can enter the name scope explicitly using with self.name_scope: or you can annotate methods (apart from __init__) with @tf.Module.with_name_scope.\n\nclass MLP(tf.Module):\n  def __init__(self, input_size, sizes, name=None):\n    super().__init__(name=name)\n    self.layers = []\n    with self.name_scope:\n      for size in sizes:\n        self.layers.append(Dense(input_dim=input_size, output_size=size))\n        input_size = size\n  @tf.Module.with_name_scope\n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    return x\n\nmodule = MLP(input_size=5, sizes=[5, 5])\nmodule.variables\n(<tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)>,\n<tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n   dtype=float32)>,\n<tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)>,\n<tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n   dtype=float32)>)\n\n\nAttributes\n\nname\tReturns the name of this module as passed or determined in the ctor.\n\nNote: This is not the same as the self.name_scope.name which includes parent module names.\n\nname_scope\tReturns a tf.name_scope instance for this class.\nnon_trainable_variables\tSequence of non-trainable variables owned by this module and its submodules.\nNote: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change.\n\nsubmodules\tSequence of all sub-modules.\n\nSubmodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on).\n\na = tf.Module()\nb = tf.Module()\nc = tf.Module()\na.b = b\nb.c = c\nlist(a.submodules) == [b, c]\nTrue\nlist(b.submodules) == [c]\nTrue\nlist(c.submodules) == []\nTrue\n\n\n\ntrainable_variables\tSequence of trainable variables owned by this module and its submodules.\n\nNote: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change.\n\nvariables\tSequence of variables owned by this module and its submodules.\nNote: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change.\nMethods\nwith_name_scope\n\nView source\n\n@classmethod\nwith_name_scope(\n    method\n)\n\n\nDecorator to automatically enter the module name scope.\n\nclass MyModule(tf.Module):\n  @tf.Module.with_name_scope\n  def __call__(self, x):\n    if not hasattr(self, 'w'):\n      self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n    return tf.matmul(x, self.w)\n\n\nUsing the above module would produce tf.Variables and tf.Tensors whose names included the module name:\n\nmod = MyModule()\nmod(tf.ones([1, 2]))\n<tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\nmod.w\n<tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\nnumpy=..., dtype=float32)>\n\n\nArgs\nmethod\tThe method to wrap.\n\nReturns\nThe original method wrapped such that it enters the module's name scope.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.IndexedSlicesSpec  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/IndexedSlicesSpec",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nArgs\nAttributes\nMethods\nexperimental_as_proto\nexperimental_from_proto\nexperimental_type_proto\nis_compatible_with\nis_subtype_of\nmost_specific_common_supertype\nmost_specific_compatible_type\n__eq__\n__ne__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.IndexedSlicesSpec \nbookmark_border\n\nView source on GitHub\n\nType specification for a tf.IndexedSlices.\n\nInherits From: TypeSpec, TraceType\n\nView aliases\ntf.IndexedSlicesSpec(\n    shape=None,\n    dtype=tf.dtypes.float32,\n    indices_dtype=tf.dtypes.int64,\n    dense_shape_dtype=None,\n    indices_shape=None\n)\n\n\nArgs\n\nshape\tThe dense shape of the IndexedSlices, or None to allow any dense shape.\ndtype\ttf.DType of values in the IndexedSlices.\nindices_dtype\ttf.DType of the indices in the IndexedSlices. One of tf.int32 or tf.int64.\ndense_shape_dtype\ttf.DType of the dense_shape in the IndexedSlices. One of tf.int32, tf.int64, or None (if the IndexedSlices has no dense_shape tensor).\nindices_shape\tThe shape of the indices component, which indicates how many slices are in the IndexedSlices.\n\nAttributes\n\nvalue_type\t\n\nMethods\nexperimental_as_proto\n\nView source\n\nexperimental_as_proto() -> struct_pb2.TypeSpecProto\n\n\nReturns a proto representation of the TypeSpec instance.\n\nDo NOT override for custom non-TF types.\n\nexperimental_from_proto\n\nView source\n\n@classmethod\nexperimental_from_proto(\n    proto: struct_pb2.TypeSpecProto\n) -> 'TypeSpec'\n\n\nReturns a TypeSpec instance based on the serialized proto.\n\nDo NOT override for custom non-TF types.\n\nArgs\nproto\tProto generated using 'experimental_as_proto'.\n\nexperimental_type_proto\n\nView source\n\n@classmethod\nexperimental_type_proto() -> Type[struct_pb2.TypeSpecProto]\n\n\nReturns the type of proto associated with TypeSpec serialization.\n\nDo NOT override for custom non-TF types.\n\nis_compatible_with\n\nView source\n\nis_compatible_with(\n    spec_or_value\n)\n\n\nReturns true if spec_or_value is compatible with this TypeSpec.\n\nPrefer using \"is_subtype_of\" and \"most_specific_common_supertype\" wherever possible.\n\nArgs\nspec_or_value\tA TypeSpec or TypeSpec associated value to compare against.\n\nis_subtype_of\n\nView source\n\nis_subtype_of(\n    other: tf.types.experimental.TraceType\n) -> bool\n\n\nReturns True if self is a subtype of other.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nother\tA TraceType object.\n\nmost_specific_common_supertype\n\nView source\n\nmost_specific_common_supertype(\n    others: Sequence[tf.types.experimental.TraceType]\n) -> Optional['TypeSpec']\n\n\nReturns the most specific supertype TypeSpec of self and others.\n\nImplements the tf.types.experimental.func.TraceType interface.\n\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\n\nArgs\nothers\tA sequence of TraceTypes.\n\nmost_specific_compatible_type\n\nView source\n\nmost_specific_compatible_type(\n    other: 'TypeSpec'\n) -> 'TypeSpec'\n\n\nReturns the most specific TypeSpec compatible with self and other. (deprecated)\n\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use most_specific_common_supertype instead.\n\nDeprecated. Please use most_specific_common_supertype instead. Do not override this function.\n\nArgs\nother\tA TypeSpec.\n\nRaises\nValueError\tIf there is no TypeSpec that is compatible with both self and other.\n\n__eq__\n\nView source\n\n__eq__(\n    other\n) -> bool\n\n\nReturn self==value.\n\n__ne__\n\nView source\n\n__ne__(\n    other\n) -> bool\n\n\nReturn self!=value.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.Graph  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/Graph",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nUsing graphs directly (deprecated)\nAttributes\nMethods\nDismantle\nadd_to_collection\nadd_to_collections\nas_default\nas_graph_def\nas_graph_element\nclear_collection\ncolocate_with\ncontainer\ncontrol_dependencies\ncreate_op\ndevice\nfinalize\nget\nget_all_collection_keys\nget_collection\nget_collection_ref\nget_name_scope\nget_operation_by_name\nget_operations\nget_tensor_by_name\ngradient_override_map\nis_feedable\nis_fetchable\nname_scope\nnew_operations\nnum_operations\nop_def_for_type\nprevent_feeding\nprevent_fetching\nswitch_to_thread_local\nunique_name\n__enter__\n__exit__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.Graph \nbookmark_border\n\nView source on GitHub\n\nA TensorFlow computation, represented as a dataflow graph.\n\nView aliases\ntf.Graph() -> None\n\nUsed in the notebooks\nUsed in the guide\tUsed in the tutorials\n\nMigrating model checkpoints\nValidating correctness & numerical equivalence\nMigrate the SavedModel workflow\nDebug a TensorFlow 2 migrated training pipeline\nMigrating your TFLite code to TF2\n\t\nGenerating Images with Little Data Using S3GAN\nExploring the TF-Hub CORD-19 Swivel Embeddings\nWiki40B Language Models\nMigrating tf.summary usage to TF 2.x\n\nGraphs are used by tf.functions to represent the function's computations. Each graph contains a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations.\n\nUsing graphs directly (deprecated)\n\nA tf.Graph can be constructed and used directly without a tf.function, as was required in TensorFlow 1, but this is deprecated and it is recommended to use a tf.function instead. If a graph is directly used, other deprecated TensorFlow 1 classes are also required to execute the graph, such as a tf.compat.v1.Session.\n\nA default graph can be registered with the tf.Graph.as_default context manager. Then, operations will be added to the graph instead of being executed eagerly. For example:\n\ng = tf.Graph()\nwith g.as_default():\n  # Define operations and tensors in `g`.\n  c = tf.constant(30.0)\n  assert c.graph is g\n\n\ntf.compat.v1.get_default_graph() can be used to obtain the default graph.\n\nImportant note: This class is not thread-safe for graph construction. All operations should be created from a single thread, or external synchronization must be provided. Unless otherwise specified, all methods are not thread-safe.\n\nA Graph instance supports an arbitrary number of \"collections\" that are identified by name. For convenience when building a large graph, collections can store groups of related objects: for example, the tf.Variable uses a collection (named tf.GraphKeys.GLOBAL_VARIABLES) for all variables that are created during the construction of a graph. The caller may define additional collections by specifying a new name.\n\nAttributes\n\nbuilding_function\tReturns True iff this graph represents a function.\ncollections\tReturns the names of the collections known to this graph.\nfinalized\tTrue if this graph has been finalized.\ngraph_def_versions\tThe GraphDef version information of this graph.\n\nFor details on the meaning of each version, see GraphDef.\n\n\noperations\t\n\n\nseed\tThe graph-level random seed of this graph.\nversion\t\n\nMethods\nDismantle\nDismantle()\n\n\n(self: handle) -> None\n\nadd_to_collection\n\nView source\n\nadd_to_collection(\n    name, value\n) -> None\n\n\nStores value in the collection with the given name.\n\nNote that collections are not sets, so it is possible to add a value to a collection several times.\n\nArgs\nname\tThe key for the collection. The GraphKeys class contains many standard names for collections.\nvalue\tThe value to add to the collection.\n\nadd_to_collections\n\nView source\n\nadd_to_collections(\n    names, value\n) -> None\n\n\nStores value in the collections given by names.\n\nNote that collections are not sets, so it is possible to add a value to a collection several times. This function makes sure that duplicates in names are ignored, but it will not check for pre-existing membership of value in any of the collections in names.\n\nnames can be any iterable, but if names is a string, it is treated as a single collection name.\n\nArgs\nnames\tThe keys for the collections to add to. The GraphKeys class contains many standard names for collections.\nvalue\tThe value to add to the collections.\n\nas_default\n\nView source\n\nas_default() -> ContextManager['Graph']\n\n\nReturns a context manager that makes this Graph the default graph.\n\nThis method should be used if you want to create multiple graphs in the same process. For convenience, a global default graph is provided, and all ops will be added to this graph if you do not create a new graph explicitly.\n\nUse this method with the with keyword to specify that ops created within the scope of a block should be added to this graph. In this case, once the scope of the with is exited, the previous default graph is set again as default. There is a stack, so it's ok to have multiple nested levels of as_default calls.\n\nThe default graph is a property of the current thread. If you create a new thread, and wish to use the default graph in that thread, you must explicitly add a with g.as_default(): in that thread's function.\n\nThe following code examples are equivalent:\n\n# 1. Using Graph.as_default():\ng = tf.Graph()\nwith g.as_default():\n  c = tf.constant(5.0)\n  assert c.graph is g\n\n# 2. Constructing and making default:\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  assert c.graph is g\n\n\nIf eager execution is enabled ops created under this context manager will be added to the graph instead of executed eagerly.\n\nReturns\nA context manager for using this graph as the default graph.\n\nas_graph_def\n\nView source\n\nas_graph_def(\n    from_version=None, add_shapes=False, use_pybind11_proto=False\n) -> tf.compat.v1.GraphDef\n\n\nReturns a serialized GraphDef representation of this graph.\n\nThe serialized GraphDef can be imported into another Graph (using tf.import_graph_def) or used with the C++ Session API.\n\nThis method is thread-safe.\n\nArgs\nfrom_version\tOptional. If this is set, returns a GraphDef containing only the nodes that were added to this graph since its version property had the given value.\nadd_shapes\tIf true, adds an \"_output_shapes\" list attr to each node with the inferred shapes of each of its outputs.\nuse_pybind11_proto\tIf true, If true, uses the c++ pybind11_proto api to get the GraphDef proto directly from c++, instead of through a TF buffer. See https://github.com/pybind/pybind11_protobuf for reference.\n\nReturns\nA GraphDef protocol buffer.\n\nRaises\nValueError\tIf the graph_def would be too large.\n\nas_graph_element\n\nView source\n\nas_graph_element(\n    obj, allow_tensor=True, allow_operation=True\n) -> Union[tensor_lib.Tensor, 'Operation']\n\n\nReturns the object referred to by obj, as an Operation or Tensor.\n\nThis function validates that obj represents an element of this graph, and gives an informative error message if it is not.\n\nThis function is the canonical way to get/validate an object of one of the allowed types from an external argument reference in the Session API.\n\nThis method may be called concurrently from multiple threads.\n\nArgs\nobj\tA Tensor, an Operation, or the name of a tensor or operation. Can also be any object with an _as_graph_element() method that returns a value of one of these types. Note: _as_graph_element will be called inside the graph's lock and so may not modify the graph.\nallow_tensor\tIf true, obj may refer to a Tensor.\nallow_operation\tIf true, obj may refer to an Operation.\n\nReturns\nThe Tensor or Operation in the Graph corresponding to obj.\n\nRaises\nTypeError\tIf obj is not a type we support attempting to convert to types.\nValueError\tIf obj is of an appropriate type but invalid. For example, an invalid string.\nKeyError\tIf obj is not an object in the graph.\n\nclear_collection\n\nView source\n\nclear_collection(\n    name\n) -> None\n\n\nClears all values in a collection.\n\nArgs\nname\tThe key for the collection. The GraphKeys class contains many standard names for collections.\n\ncolocate_with\n\nView source\n\n@tf_contextlib.contextmanager\ncolocate_with(\n    op, ignore_existing=False\n) -> Iterator[None]\n\n\nReturns a context manager that specifies an op to colocate with.\n\nNote: this function is not for public use, only for internal libraries.\nFor example:\na = tf.Variable([1.0])\nwith g.colocate_with(a):\n  b = tf.constant(1.0)\n  c = tf.add(a, b)\n\n\nb and c will always be colocated with a, no matter where a is eventually placed.\n\nNote: Using a colocation scope resets any existing device constraints.\n\nIf op is None then ignore_existing must be True and the new scope resets all colocation and device constraints.\n\nArgs\nop\tThe op to colocate all created ops with, or None.\nignore_existing\tIf true, only applies colocation of this op within the context, rather than applying all colocation properties on the stack. If op is None, this value must be True.\n\nRaises\nValueError\tif op is None but ignore_existing is False.\n\nYields\nA context manager that specifies the op with which to colocate newly created ops.\n\ncontainer\n\nView source\n\n@tf_contextlib.contextmanager\ncontainer(\n    container_name\n) -> Iterator[str]\n\n\nReturns a context manager that specifies the resource container to use.\n\nStateful operations, such as variables and queues, can maintain their states on devices so that they can be shared by multiple processes. A resource container is a string name under which these stateful operations are tracked. These resources can be released or cleared with tf.Session.reset().\n\nFor example:\nwith g.container('experiment0'):\n  # All stateful Operations constructed in this context will be placed\n  # in resource container \"experiment0\".\n  v1 = tf.Variable([1.0])\n  v2 = tf.Variable([2.0])\n  with g.container(\"experiment1\"):\n    # All stateful Operations constructed in this context will be\n    # placed in resource container \"experiment1\".\n    v3 = tf.Variable([3.0])\n    q1 = tf.queue.FIFOQueue(10, tf.float32)\n  # All stateful Operations constructed in this context will be\n  # be created in the \"experiment0\".\n  v4 = tf.Variable([4.0])\n  q1 = tf.queue.FIFOQueue(20, tf.float32)\n  with g.container(\"\"):\n    # All stateful Operations constructed in this context will be\n    # be placed in the default resource container.\n    v5 = tf.Variable([5.0])\n    q3 = tf.queue.FIFOQueue(30, tf.float32)\n\n# Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n# will become undefined (such as uninitialized).\ntf.Session.reset(target, [\"experiment0\"])\n\n\nArgs\ncontainer_name\tcontainer name string.\n\nReturns\nA context manager for defining resource containers for stateful ops, yields the container name.\n\ncontrol_dependencies\n\nView source\n\ncontrol_dependencies(\n    control_inputs\n) -> _ControlDependenciesController\n\n\nReturns a context manager that specifies control dependencies.\n\nUse with the with keyword to specify that all operations constructed within the context should have control dependencies on control_inputs. For example:\n\nwith g.control_dependencies([a, b, c]):\n  # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n  d = ...\n  e = ...\n\n\nMultiple calls to control_dependencies() can be nested, and in that case a new Operation will have control dependencies on the union of control_inputs from all active contexts.\n\nwith g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies([c, d]):\n    # Ops constructed here run after `a`, `b`, `c`, and `d`.\n\n\nYou can pass None to clear the control dependencies:\n\nwith g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies(None):\n    # Ops constructed here run normally, not waiting for either `a` or `b`.\n    with g.control_dependencies([c, d]):\n      # Ops constructed here run after `c` and `d`, also not waiting\n      # for either `a` or `b`.\n\nNote: The control dependencies context applies only to ops that are constructed within the context. Merely using an op or tensor in the context does not add a control dependency. The following example illustrates this point:\n# WRONG\ndef my_func(pred, tensor):\n  t = tf.matmul(tensor, tensor)\n  with tf.control_dependencies([pred]):\n    # The matmul op is created outside the context, so no control\n    # dependency will be added.\n    return t\n\n# RIGHT\ndef my_func(pred, tensor):\n  with tf.control_dependencies([pred]):\n    # The matmul op is created in the context, so a control dependency\n    # will be added.\n    return tf.matmul(tensor, tensor)\n\n\nAlso note that though execution of ops created under this scope will trigger execution of the dependencies, the ops created under this scope might still be pruned from a normal tensorflow graph. For example, in the following snippet of code the dependencies are never executed:\n\n  loss = model.loss()\n  with tf.control_dependencies(dependencies):\n    loss = loss + tf.constant(1)  # note: dependencies ignored in the\n                                  # backward pass\n  return tf.gradients(loss, model.variables)\n\n\nThis is because evaluating the gradient graph does not require evaluating the constant(1) op created in the forward pass.\n\nArgs\ncontrol_inputs\tA list of Operation or Tensor objects which must be executed or computed before running the operations defined in the context. Can also be None to clear the control dependencies.\n\nReturns\nA context manager that specifies control dependencies for all operations constructed within the context.\n\nRaises\nTypeError\tIf control_inputs is not a list of Operation or Tensor objects.\n\ncreate_op\n\nView source\n\ncreate_op(\n    op_type,\n    inputs,\n    dtypes=None,\n    input_types=None,\n    name=None,\n    attrs=None,\n    op_def=None,\n    compute_shapes=True,\n    compute_device=True\n) -> 'Operation'\n\n\nCreates an Operation in this graph. (deprecated arguments)\n\nDeprecated: SOME ARGUMENTS ARE DEPRECATED: (compute_shapes). They will be removed in a future version. Instructions for updating: Shapes are always computed; don't use the compute_shapes as it has no effect.\n\nThis is a low-level interface for creating an Operation. Most programs will not call this method directly, and instead use the Python op constructors, such as tf.constant(), which add ops to the default graph.\n\nArgs\nop_type\tThe Operation type to create. This corresponds to the OpDef.name field for the proto that defines the operation.\ninputs\tA list of Tensor objects that will be inputs to the Operation.\ndtypes\t(Optional) A list of DType objects that will be the types of the tensors that the operation produces.\ninput_types\t(Optional.) A list of DTypes that will be the types of the tensors that the operation consumes. By default, uses the base DType of each input in inputs. Operations that expect reference-typed inputs must specify input_types explicitly.\nname\t(Optional.) A string name for the operation. If not specified, a name is generated based on op_type.\nattrs\t(Optional.) A dictionary where the key is the attribute name (a string) and the value is the respective attr attribute of the NodeDef proto that will represent the operation (an AttrValue proto).\nop_def\t(Optional.) The OpDef proto that describes the op_type that the operation will have.\ncompute_shapes\t(Optional.) Deprecated. Has no effect (shapes are always computed).\ncompute_device\t(Optional.) If True, device functions will be executed to compute the device property of the Operation.\n\nRaises\nTypeError\tif any of the inputs is not a Tensor.\nValueError\tif colocation conflicts with existing device assignment.\n\nReturns\nAn Operation object.\n\ndevice\n\nView source\n\n@tf_contextlib.contextmanager\ndevice(\n    device_name_or_function\n) -> Iterator[None]\n\n\nReturns a context manager that specifies the default device to use.\n\nThe device_name_or_function argument may either be a device name string, a device function, or None:\n\nIf it is a device name string, all operations constructed in this context will be assigned to the device with that name, unless overridden by a nested device() context.\nIf it is a function, it will be treated as a function from Operation objects to device name strings, and invoked each time a new Operation is created. The Operation will be assigned to the device with the returned name.\nIf it is None, all device() invocations from the enclosing context will be ignored.\n\nFor information about the valid syntax of device name strings, see the documentation in DeviceNameUtils.\n\nFor example:\nwith g.device('/device:GPU:0'):\n  # All operations constructed in this context will be placed\n  # on GPU 0.\n  with g.device(None):\n    # All operations constructed in this context will have no\n    # assigned device.\n\n# Defines a function from `Operation` to device string.\ndef matmul_on_gpu(n):\n  if n.type == \"MatMul\":\n    return \"/device:GPU:0\"\n  else:\n    return \"/cpu:0\"\n\nwith g.device(matmul_on_gpu):\n  # All operations of type \"MatMul\" constructed in this context\n  # will be placed on GPU 0; all other operations will be placed\n  # on CPU 0.\n\nNote: The device scope may be overridden by op wrappers or other library code. For example, a variable assignment op v.assign() must be colocated with the tf.Variable v, and incompatible device scopes will be ignored.\n\nArgs\ndevice_name_or_function\tThe device name or function to use in the context.\n\nYields\nA context manager that specifies the default device to use for newly created ops.\n\nRaises\nRuntimeError\tIf device scopes are not properly nested.\n\nfinalize\n\nView source\n\nfinalize() -> None\n\n\nFinalizes this graph, making it read-only.\n\nAfter calling g.finalize(), no new operations can be added to g. This method is used to ensure that no operations are added to a graph when it is shared between multiple threads, for example when using a tf.compat.v1.train.QueueRunner.\n\nget\n\nView source\n\nget() -> GraphType\n\nget_all_collection_keys\n\nView source\n\nget_all_collection_keys() -> list[str]\n\n\nReturns a list of collections used in this graph.\n\nget_collection\n\nView source\n\nget_collection(\n    name, scope=None\n) -> list[Any]\n\n\nReturns a list of values in the collection with the given name.\n\nThis is different from get_collection_ref() which always returns the actual collection list if it exists in that it returns a new list each time it is called.\n\nArgs\nname\tThe key for the collection. For example, the GraphKeys class contains many standard names for collections.\nscope\t(Optional.) A string. If supplied, the resulting list is filtered to include only items whose name attribute matches scope using re.match. Items without a name attribute are never returned if a scope is supplied. The choice of re.match means that a scope without special tokens filters by prefix.\n\nReturns\nThe list of values in the collection with the given name, or an empty list if no value has been added to that collection. The list contains the values in the order under which they were collected.\n\nget_collection_ref\n\nView source\n\nget_collection_ref(\n    name\n) -> list[Any]\n\n\nReturns a list of values in the collection with the given name.\n\nIf the collection exists, this returns the list itself, which can be modified in place to change the collection. If the collection does not exist, it is created as an empty list and the list is returned.\n\nThis is different from get_collection() which always returns a copy of the collection list if it exists and never creates an empty collection.\n\nArgs\nname\tThe key for the collection. For example, the GraphKeys class contains many standard names for collections.\n\nReturns\nThe list of values in the collection with the given name, or an empty list if no value has been added to that collection.\n\nget_name_scope\n\nView source\n\nget_name_scope() -> str\n\n\nReturns the current name scope.\n\nFor example:\nwith tf.name_scope('scope1'):\n  with tf.name_scope('scope2'):\n    print(tf.compat.v1.get_default_graph().get_name_scope())\n\n\nwould print the string scope1/scope2.\n\nReturns\nA string representing the current name scope.\n\nget_operation_by_name\n\nView source\n\nget_operation_by_name(\n    name\n) -> 'Operation'\n\n\nReturns the Operation with the given name.\n\nThis method may be called concurrently from multiple threads.\n\nArgs\nname\tThe name of the Operation to return.\n\nReturns\nThe Operation with the given name.\n\nRaises\nTypeError\tIf name is not a string.\nKeyError\tIf name does not correspond to an operation in this graph.\n\nget_operations\nget_operations()\n\n\n(self: handle) -> list\n\nget_tensor_by_name\n\nView source\n\nget_tensor_by_name(\n    name\n) -> tf.Tensor\n\n\nReturns the Tensor with the given name.\n\nThis method may be called concurrently from multiple threads.\n\nArgs\nname\tThe name of the Tensor to return.\n\nReturns\nThe Tensor with the given name.\n\nRaises\nTypeError\tIf name is not a string.\nKeyError\tIf name does not correspond to a tensor in this graph.\n\ngradient_override_map\n\nView source\n\n@tf_contextlib.contextmanager\ngradient_override_map(\n    op_type_map\n) -> Iterator[None]\n\n\nEXPERIMENTAL: A context manager for overriding gradient functions.\n\nThis context manager can be used to override the gradient function that will be used for ops within the scope of the context.\n\nFor example:\n@tf.RegisterGradient(\"CustomSquare\")\ndef _custom_square_grad(op, grad):\n  # ...\n\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n                          # gradient of s_2.\n\n\nArgs\nop_type_map\tA dictionary mapping op type strings to alternative op type strings.\n\nReturns\nA context manager that sets the alternative op type to be used for one or more ops created in that context.\n\nRaises\nTypeError\tIf op_type_map is not a dictionary mapping strings to strings.\n\nis_feedable\n\nView source\n\nis_feedable(\n    tensor\n) -> bool\n\n\nReturns True if and only if tensor is feedable.\n\nis_fetchable\n\nView source\n\nis_fetchable(\n    tensor_or_op\n) -> bool\n\n\nReturns True if and only if tensor_or_op is fetchable.\n\nname_scope\n\nView source\n\n@tf_contextlib.contextmanager\nname_scope(\n    name\n) -> Iterator[str]\n\n\nReturns a context manager that creates hierarchical names for operations.\n\nA graph maintains a stack of name scopes. A with name_scope(...): statement pushes a new name onto the stack for the lifetime of the context.\n\nThe name argument will be interpreted as follows:\n\nA string (not ending with '/') will create a new name scope, in which name is appended to the prefix of all operations created in the context. If name has been used before, it will be made unique by calling self.unique_name(name).\nA scope previously captured from a with g.name_scope(...) as scope: statement will be treated as an \"absolute\" name scope, which makes it possible to re-enter existing scopes.\nA value of None or the empty string will reset the current name scope to the top-level (empty) name scope.\nFor example:\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0, name=\"c\")\n  assert c.op.name == \"c\"\n  c_1 = tf.constant(6.0, name=\"c\")\n  assert c_1.op.name == \"c_1\"\n\n  # Creates a scope called \"nested\"\n  with g.name_scope(\"nested\") as scope:\n    nested_c = tf.constant(10.0, name=\"c\")\n    assert nested_c.op.name == \"nested/c\"\n\n    # Creates a nested scope called \"inner\".\n    with g.name_scope(\"inner\"):\n      nested_inner_c = tf.constant(20.0, name=\"c\")\n      assert nested_inner_c.op.name == \"nested/inner/c\"\n\n    # Create a nested scope called \"inner_1\".\n    with g.name_scope(\"inner\"):\n      nested_inner_1_c = tf.constant(30.0, name=\"c\")\n      assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n\n      # Treats `scope` as an absolute name scope, and\n      # switches to the \"nested/\" scope.\n      with g.name_scope(scope):\n        nested_d = tf.constant(40.0, name=\"d\")\n        assert nested_d.op.name == \"nested/d\"\n\n        with g.name_scope(\"\"):\n          e = tf.constant(50.0, name=\"e\")\n          assert e.op.name == \"e\"\n\n\nThe name of the scope itself can be captured by with g.name_scope(...) as scope:, which stores the name of the scope in the variable scope. This value can be used to name an operation that represents the overall result of executing the ops in a scope. For example:\n\ninputs = tf.constant(...)\nwith g.name_scope('my_layer') as scope:\n  weights = tf.Variable(..., name=\"weights\")\n  biases = tf.Variable(..., name=\"biases\")\n  affine = tf.matmul(inputs, weights) + biases\n  output = tf.nn.relu(affine, name=scope)\n\nNote: This constructor validates the given name. Valid scope names match one of the following regular expressions:\n[A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)\n[A-Za-z0-9_.\\-/]* (for other scopes)\n\n\nArgs\nname\tA name for the scope.\n\nReturns\nA context manager that installs name as a new name scope.\n\nRaises\nValueError\tIf name is not a valid scope name, according to the rules above.\n\nnew_operations\nnew_operations()\n\n\n(self: handle) -> List[TF_Operation]\n\nnum_operations\nnum_operations()\n\n\n(self: handle) -> int\n\nop_def_for_type\n\nView source\n\nop_def_for_type(\n    type\n) -> op_def_pb2.OpDef\n\n\nReturns the OpDef proto for type. type is a string.\n\nprevent_feeding\n\nView source\n\nprevent_feeding(\n    tensor\n) -> None\n\n\nMarks the given tensor as unfeedable in this graph.\n\nprevent_fetching\n\nView source\n\nprevent_fetching(\n    op\n) -> None\n\n\nMarks the given op as unfetchable in this graph.\n\nswitch_to_thread_local\n\nView source\n\nswitch_to_thread_local() -> None\n\n\nMake device, colocation and dependencies stacks thread-local.\n\nDevice, colocation and dependencies stacks are not thread-local be default. If multiple threads access them, then the state is shared. This means that one thread may affect the behavior of another thread.\n\nAfter this method is called, the stacks become thread-local. If multiple threads access them, then the state is not shared. Each thread uses its own value; a thread doesn't affect other threads by mutating such a stack.\n\nThe initial value for every thread's stack is set to the current value of the stack when switch_to_thread_local() was first called.\n\nunique_name\n\nView source\n\nunique_name(\n    name, mark_as_used=True\n) -> str\n\n\nReturn a unique operation name for name.\n\nNote: You rarely need to call unique_name() directly. Most of the time you just need to create with g.name_scope() blocks to generate structured names.\n\nunique_name is used to generate structured names, separated by \"/\", to help identify operations when debugging a graph. Operation names are displayed in error messages reported by the TensorFlow runtime, and in various visualization tools such as TensorBoard.\n\nIf mark_as_used is set to True, which is the default, a new unique name is created and marked as in use. If it's set to False, the unique name is returned without actually being marked as used. This is useful when the caller simply wants to know what the name to be created will be.\n\nArgs\nname\tThe name for an operation.\nmark_as_used\tWhether to mark this name as being used.\n\nReturns\nA string to be passed to create_op() that will be used to name the operation being created.\n\n__enter__\n\nView source\n\n__enter__() -> GraphType\n\n__exit__\n\nView source\n\n__exit__(\n    *args\n) -> None\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "All symbols in TensorFlow 2  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/all_symbols",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nPrimary symbols\nCompat v1 symbols\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\nAll symbols in TensorFlow 2 \nbookmark_border\n\nPrimary symbols\ntf\ntf.AggregationMethod\ntf.Assert\ntf.CriticalSection\ntf.DType\ntf.DeviceSpec\ntf.GradientTape\ntf.Graph\ntf.IndexedSlices\ntf.IndexedSlicesSpec\ntf.Module\ntf.Operation\ntf.OptionalSpec\ntf.RaggedTensor\ntf.RaggedTensorSpec\ntf.RegisterGradient\ntf.SparseTensor\ntf.SparseTensorSpec\ntf.Tensor\ntf.TensorArray\ntf.TensorArraySpec\ntf.TensorShape\ntf.TensorSpec\ntf.TypeSpec\ntf.UnconnectedGradients\ntf.Variable\ntf.Variable.SaveSliceInfo\ntf.VariableAggregation\ntf.VariableSynchronization\ntf.abs\ntf.acos\ntf.acosh\ntf.add\ntf.add_n\ntf.approx_top_k\ntf.argmax\ntf.argmin\ntf.argsort\ntf.as_dtype\ntf.as_string\ntf.asin\ntf.asinh\ntf.assert_equal\ntf.assert_greater\ntf.assert_less\ntf.assert_rank\ntf.atan\ntf.atan2\ntf.atanh\ntf.audio\ntf.audio.decode_wav\ntf.audio.encode_wav\ntf.autodiff\ntf.autodiff.ForwardAccumulator\ntf.autodiff.GradientTape\ntf.autograph\ntf.autograph.experimental\ntf.autograph.experimental.Feature\ntf.autograph.experimental.do_not_convert\ntf.autograph.experimental.set_loop_options\ntf.autograph.set_verbosity\ntf.autograph.to_code\ntf.autograph.to_graph\ntf.autograph.trace\ntf.batch_to_space\ntf.bitcast\ntf.bitwise\ntf.bitwise.bitwise_and\ntf.bitwise.bitwise_or\ntf.bitwise.bitwise_xor\ntf.bitwise.invert\ntf.bitwise.left_shift\ntf.bitwise.right_shift\ntf.boolean_mask\ntf.broadcast_dynamic_shape\ntf.broadcast_static_shape\ntf.broadcast_to\ntf.case\ntf.cast\ntf.clip_by_global_norm\ntf.clip_by_norm\ntf.clip_by_value\ntf.compat\ntf.compat.as_bytes\ntf.compat.as_str\ntf.compat.as_str_any\ntf.compat.as_text\ntf.compat.dimension_at_index\ntf.compat.dimension_value\ntf.compat.forward_compatibility_horizon\ntf.compat.forward_compatible\ntf.compat.path_to_str\ntf.complex\ntf.concat\ntf.cond\ntf.config\ntf.config.LogicalDevice\ntf.config.LogicalDeviceConfiguration\ntf.config.PhysicalDevice\ntf.config.experimental\ntf.config.experimental.ClusterDeviceFilters\ntf.config.experimental.VirtualDeviceConfiguration\ntf.config.experimental.disable_mlir_bridge\ntf.config.experimental.enable_mlir_bridge\ntf.config.experimental.enable_op_determinism\ntf.config.experimental.enable_tensor_float_32_execution\ntf.config.experimental.get_device_details\ntf.config.experimental.get_device_policy\ntf.config.experimental.get_memory_growth\ntf.config.experimental.get_memory_info\ntf.config.experimental.get_memory_usage\ntf.config.experimental.get_synchronous_execution\ntf.config.experimental.get_virtual_device_configuration\ntf.config.experimental.get_visible_devices\ntf.config.experimental.list_logical_devices\ntf.config.experimental.list_physical_devices\ntf.config.experimental.reset_memory_stats\ntf.config.experimental.set_device_policy\ntf.config.experimental.set_memory_growth\ntf.config.experimental.set_synchronous_execution\ntf.config.experimental.set_virtual_device_configuration\ntf.config.experimental.set_visible_devices\ntf.config.experimental.tensor_float_32_execution_enabled\ntf.config.experimental_connect_to_cluster\ntf.config.experimental_connect_to_host\ntf.config.experimental_functions_run_eagerly\ntf.config.experimental_run_functions_eagerly\ntf.config.functions_run_eagerly\ntf.config.get_logical_device_configuration\ntf.config.get_soft_device_placement\ntf.config.get_visible_devices\ntf.config.list_logical_devices\ntf.config.list_physical_devices\ntf.config.optimizer\ntf.config.optimizer.get_experimental_options\ntf.config.optimizer.get_jit\ntf.config.optimizer.set_experimental_options\ntf.config.optimizer.set_jit\ntf.config.run_functions_eagerly\ntf.config.set_logical_device_configuration\ntf.config.set_soft_device_placement\ntf.config.set_visible_devices\ntf.config.threading\ntf.config.threading.get_inter_op_parallelism_threads\ntf.config.threading.get_intra_op_parallelism_threads\ntf.config.threading.set_inter_op_parallelism_threads\ntf.config.threading.set_intra_op_parallelism_threads\ntf.constant\ntf.constant_initializer\ntf.control_dependencies\ntf.conv\ntf.conv2d_backprop_filter_v2\ntf.conv2d_backprop_input_v2\ntf.convert_to_tensor\ntf.cos\ntf.cosh\ntf.cumsum\ntf.custom_gradient\ntf.data\ntf.data.Dataset\ntf.data.DatasetSpec\ntf.data.FixedLengthRecordDataset\ntf.data.Iterator\ntf.data.IteratorSpec\ntf.data.NumpyIterator\ntf.data.Options\ntf.data.TFRecordDataset\ntf.data.TextLineDataset\ntf.data.ThreadingOptions\ntf.data.experimental\ntf.data.experimental.AutoShardPolicy\ntf.data.experimental.AutotuneAlgorithm\ntf.data.experimental.AutotuneOptions\ntf.data.experimental.Counter\ntf.data.experimental.CsvDataset\ntf.data.experimental.DatasetInitializer\ntf.data.experimental.DistributeOptions\ntf.data.experimental.ExternalStatePolicy\ntf.data.experimental.OptimizationOptions\ntf.data.experimental.Optional\ntf.data.experimental.RandomDataset\ntf.data.experimental.Reducer\ntf.data.experimental.SqlDataset\ntf.data.experimental.TFRecordWriter\ntf.data.experimental.ThreadingOptions\ntf.data.experimental.assert_cardinality\ntf.data.experimental.at\ntf.data.experimental.bucket_by_sequence_length\ntf.data.experimental.cardinality\ntf.data.experimental.choose_from_datasets\ntf.data.experimental.copy_to_device\ntf.data.experimental.dense_to_ragged_batch\ntf.data.experimental.dense_to_sparse_batch\ntf.data.experimental.enable_debug_mode\ntf.data.experimental.enumerate_dataset\ntf.data.experimental.from_list\ntf.data.experimental.from_variant\ntf.data.experimental.get_next_as_optional\ntf.data.experimental.get_single_element\ntf.data.experimental.get_structure\ntf.data.experimental.group_by_reducer\ntf.data.experimental.group_by_window\ntf.data.experimental.ignore_errors\ntf.data.experimental.index_table_from_dataset\ntf.data.experimental.load\ntf.data.experimental.make_batched_features_dataset\ntf.data.experimental.make_csv_dataset\ntf.data.experimental.make_saveable_from_iterator\ntf.data.experimental.map_and_batch\ntf.data.experimental.pad_to_cardinality\ntf.data.experimental.parallel_interleave\ntf.data.experimental.parse_example_dataset\ntf.data.experimental.prefetch_to_device\ntf.data.experimental.rejection_resample\ntf.data.experimental.sample_from_datasets\ntf.data.experimental.save\ntf.data.experimental.scan\ntf.data.experimental.service\ntf.data.experimental.service.CrossTrainerCache\ntf.data.experimental.service.DispatchServer\ntf.data.experimental.service.DispatcherConfig\ntf.data.experimental.service.ShardingPolicy\ntf.data.experimental.service.WorkerConfig\ntf.data.experimental.service.WorkerServer\ntf.data.experimental.service.distribute\ntf.data.experimental.service.from_dataset_id\ntf.data.experimental.service.register_dataset\ntf.data.experimental.shuffle_and_repeat\ntf.data.experimental.snapshot\ntf.data.experimental.table_from_dataset\ntf.data.experimental.take_while\ntf.data.experimental.to_variant\ntf.data.experimental.unbatch\ntf.data.experimental.unique\ntf.debugging\ntf.debugging.Assert\ntf.debugging.assert_all_finite\ntf.debugging.assert_equal\ntf.debugging.assert_greater\ntf.debugging.assert_greater_equal\ntf.debugging.assert_integer\ntf.debugging.assert_less\ntf.debugging.assert_less_equal\ntf.debugging.assert_near\ntf.debugging.assert_negative\ntf.debugging.assert_non_negative\ntf.debugging.assert_non_positive\ntf.debugging.assert_none_equal\ntf.debugging.assert_positive\ntf.debugging.assert_proper_iterable\ntf.debugging.assert_rank\ntf.debugging.assert_rank_at_least\ntf.debugging.assert_rank_in\ntf.debugging.assert_same_float_dtype\ntf.debugging.assert_scalar\ntf.debugging.assert_shapes\ntf.debugging.assert_type\ntf.debugging.check_numerics\ntf.debugging.disable_check_numerics\ntf.debugging.disable_traceback_filtering\ntf.debugging.enable_check_numerics\ntf.debugging.enable_traceback_filtering\ntf.debugging.experimental\ntf.debugging.experimental.disable_dump_debug_info\ntf.debugging.experimental.enable_dump_debug_info\ntf.debugging.get_log_device_placement\ntf.debugging.is_numeric_tensor\ntf.debugging.is_traceback_filtering_enabled\ntf.debugging.set_log_device_placement\ntf.device\ntf.distribute\ntf.distribute.CrossDeviceOps\ntf.distribute.DistributedDataset\ntf.distribute.DistributedIterator\ntf.distribute.DistributedValues\ntf.distribute.HierarchicalCopyAllReduce\ntf.distribute.InputContext\ntf.distribute.InputOptions\ntf.distribute.InputReplicationMode\ntf.distribute.MirroredStrategy\ntf.distribute.MultiWorkerMirroredStrategy\ntf.distribute.NcclAllReduce\ntf.distribute.OneDeviceStrategy\ntf.distribute.ParameterServerStrategy\ntf.distribute.ReduceOp\ntf.distribute.ReductionToOneDevice\ntf.distribute.ReplicaContext\ntf.distribute.RunOptions\ntf.distribute.Server\ntf.distribute.Strategy\ntf.distribute.StrategyExtended\ntf.distribute.TPUStrategy\ntf.distribute.cluster_resolver\ntf.distribute.cluster_resolver.ClusterResolver\ntf.distribute.cluster_resolver.GCEClusterResolver\ntf.distribute.cluster_resolver.KubernetesClusterResolver\ntf.distribute.cluster_resolver.SimpleClusterResolver\ntf.distribute.cluster_resolver.SlurmClusterResolver\ntf.distribute.cluster_resolver.TFConfigClusterResolver\ntf.distribute.cluster_resolver.TPUClusterResolver\ntf.distribute.cluster_resolver.UnionResolver\ntf.distribute.coordinator\ntf.distribute.coordinator.ClusterCoordinator\ntf.distribute.coordinator.PerWorkerValue\ntf.distribute.coordinator.RemoteValue\ntf.distribute.coordinator.experimental_get_current_worker_index\ntf.distribute.experimental\ntf.distribute.experimental.CentralStorageStrategy\ntf.distribute.experimental.CollectiveCommunication\ntf.distribute.experimental.CollectiveHints\ntf.distribute.experimental.CommunicationImplementation\ntf.distribute.experimental.CommunicationOptions\ntf.distribute.experimental.MultiWorkerMirroredStrategy\ntf.distribute.experimental.ParameterServerStrategy\ntf.distribute.experimental.PreemptionCheckpointHandler\ntf.distribute.experimental.PreemptionWatcher\ntf.distribute.experimental.TPUStrategy\ntf.distribute.experimental.TerminationConfig\ntf.distribute.experimental.ValueContext\ntf.distribute.experimental.coordinator\ntf.distribute.experimental.coordinator.ClusterCoordinator\ntf.distribute.experimental.coordinator.PerWorkerValues\ntf.distribute.experimental.coordinator.RemoteValue\ntf.distribute.experimental.partitioners\ntf.distribute.experimental.partitioners.FixedShardsPartitioner\ntf.distribute.experimental.partitioners.MaxSizePartitioner\ntf.distribute.experimental.partitioners.MinSizePartitioner\ntf.distribute.experimental.partitioners.Partitioner\ntf.distribute.experimental.rpc\ntf.distribute.experimental.rpc.Client\ntf.distribute.experimental.rpc.Server\ntf.distribute.experimental_set_strategy\ntf.distribute.get_replica_context\ntf.distribute.get_strategy\ntf.distribute.has_strategy\ntf.distribute.in_cross_replica_context\ntf.divide\ntf.dtypes\ntf.dtypes.DType\ntf.dtypes.as_dtype\ntf.dtypes.cast\ntf.dtypes.complex\ntf.dtypes.experimental\ntf.dtypes.saturate_cast\ntf.dynamic_partition\ntf.dynamic_stitch\ntf.edit_distance\ntf.eig\ntf.eigvals\ntf.einsum\ntf.ensure_shape\ntf.equal\ntf.errors\ntf.errors.AbortedError\ntf.errors.AlreadyExistsError\ntf.errors.CancelledError\ntf.errors.DataLossError\ntf.errors.DeadlineExceededError\ntf.errors.FailedPreconditionError\ntf.errors.InternalError\ntf.errors.InvalidArgumentError\ntf.errors.NotFoundError\ntf.errors.OpError\ntf.errors.OperatorNotAllowedInGraphError\ntf.errors.OutOfRangeError\ntf.errors.PermissionDeniedError\ntf.errors.ResourceExhaustedError\ntf.errors.UnauthenticatedError\ntf.errors.UnavailableError\ntf.errors.UnimplementedError\ntf.errors.UnknownError\ntf.executing_eagerly\ntf.exp\ntf.expand_dims\ntf.experimental\ntf.experimental.BatchableExtensionType\ntf.experimental.DynamicRaggedShape\ntf.experimental.DynamicRaggedShape.Spec\ntf.experimental.ExtensionType\ntf.experimental.ExtensionTypeBatchEncoder\ntf.experimental.ExtensionTypeSpec\ntf.experimental.Optional\ntf.experimental.RowPartition\ntf.experimental.StructuredTensor\ntf.experimental.StructuredTensor.FieldName\ntf.experimental.StructuredTensor.Spec\ntf.experimental.async_clear_error\ntf.experimental.async_scope\ntf.experimental.dispatch_for_api\ntf.experimental.dispatch_for_binary_elementwise_apis\ntf.experimental.dispatch_for_binary_elementwise_assert_apis\ntf.experimental.dispatch_for_unary_elementwise_apis\ntf.experimental.dlpack\ntf.experimental.dlpack.from_dlpack\ntf.experimental.dlpack.to_dlpack\ntf.experimental.dtensor\ntf.experimental.dtensor.DTensorCheckpoint\ntf.experimental.dtensor.DTensorDataset\ntf.experimental.dtensor.DVariable\ntf.experimental.dtensor.DVariable.SaveSliceInfo\ntf.experimental.dtensor.Layout\ntf.experimental.dtensor.Mesh\ntf.experimental.dtensor.barrier\ntf.experimental.dtensor.call_with_layout\ntf.experimental.dtensor.check_layout\ntf.experimental.dtensor.client_id\ntf.experimental.dtensor.copy_to_mesh\ntf.experimental.dtensor.create_distributed_mesh\ntf.experimental.dtensor.create_mesh\ntf.experimental.dtensor.create_tpu_mesh\ntf.experimental.dtensor.default_mesh\ntf.experimental.dtensor.device_name\ntf.experimental.dtensor.enable_save_as_bf16\ntf.experimental.dtensor.fetch_layout\ntf.experimental.dtensor.full_job_name\ntf.experimental.dtensor.get_default_mesh\ntf.experimental.dtensor.heartbeat_enabled\ntf.experimental.dtensor.initialize_accelerator_system\ntf.experimental.dtensor.initialize_multi_client\ntf.experimental.dtensor.initialize_tpu_system\ntf.experimental.dtensor.is_dtensor\ntf.experimental.dtensor.job_name\ntf.experimental.dtensor.jobs\ntf.experimental.dtensor.local_devices\ntf.experimental.dtensor.name_based_restore\ntf.experimental.dtensor.name_based_save\ntf.experimental.dtensor.num_clients\ntf.experimental.dtensor.num_global_devices\ntf.experimental.dtensor.num_local_devices\ntf.experimental.dtensor.pack\ntf.experimental.dtensor.preferred_device_type\ntf.experimental.dtensor.relayout\ntf.experimental.dtensor.relayout_like\ntf.experimental.dtensor.run_on\ntf.experimental.dtensor.sharded_save\ntf.experimental.dtensor.shutdown_accelerator_system\ntf.experimental.dtensor.shutdown_tpu_system\ntf.experimental.dtensor.unpack\ntf.experimental.enable_strict_mode\ntf.experimental.extension_type\ntf.experimental.extension_type.as_dict\ntf.experimental.function_executor_type\ntf.experimental.numpy\ntf.experimental.numpy.abs\ntf.experimental.numpy.absolute\ntf.experimental.numpy.add\ntf.experimental.numpy.all\ntf.experimental.numpy.allclose\ntf.experimental.numpy.amax\ntf.experimental.numpy.amin\ntf.experimental.numpy.angle\ntf.experimental.numpy.any\ntf.experimental.numpy.append\ntf.experimental.numpy.arange\ntf.experimental.numpy.arccos\ntf.experimental.numpy.arccosh\ntf.experimental.numpy.arcsin\ntf.experimental.numpy.arcsinh\ntf.experimental.numpy.arctan\ntf.experimental.numpy.arctan2\ntf.experimental.numpy.arctanh\ntf.experimental.numpy.argmax\ntf.experimental.numpy.argmin\ntf.experimental.numpy.argsort\ntf.experimental.numpy.around\ntf.experimental.numpy.array\ntf.experimental.numpy.array_equal\ntf.experimental.numpy.asanyarray\ntf.experimental.numpy.asarray\ntf.experimental.numpy.ascontiguousarray\ntf.experimental.numpy.atleast_1d\ntf.experimental.numpy.atleast_2d\ntf.experimental.numpy.atleast_3d\ntf.experimental.numpy.average\ntf.experimental.numpy.bitwise_and\ntf.experimental.numpy.bitwise_not\ntf.experimental.numpy.bitwise_or\ntf.experimental.numpy.bitwise_xor\ntf.experimental.numpy.bool_\ntf.experimental.numpy.broadcast_arrays\ntf.experimental.numpy.broadcast_to\ntf.experimental.numpy.cbrt\ntf.experimental.numpy.ceil\ntf.experimental.numpy.clip\ntf.experimental.numpy.complex128\ntf.experimental.numpy.complex64\ntf.experimental.numpy.complex_\ntf.experimental.numpy.compress\ntf.experimental.numpy.concatenate\ntf.experimental.numpy.conj\ntf.experimental.numpy.conjugate\ntf.experimental.numpy.copy\ntf.experimental.numpy.cos\ntf.experimental.numpy.cosh\ntf.experimental.numpy.count_nonzero\ntf.experimental.numpy.cross\ntf.experimental.numpy.cumprod\ntf.experimental.numpy.cumsum\ntf.experimental.numpy.deg2rad\ntf.experimental.numpy.diag\ntf.experimental.numpy.diag_indices\ntf.experimental.numpy.diagflat\ntf.experimental.numpy.diagonal\ntf.experimental.numpy.diff\ntf.experimental.numpy.divide\ntf.experimental.numpy.divmod\ntf.experimental.numpy.dot\ntf.experimental.numpy.dsplit\ntf.experimental.numpy.dstack\ntf.experimental.numpy.einsum\ntf.experimental.numpy.empty\ntf.experimental.numpy.empty_like\ntf.experimental.numpy.equal\ntf.experimental.numpy.exp\ntf.experimental.numpy.exp2\ntf.experimental.numpy.expand_dims\ntf.experimental.numpy.experimental_enable_numpy_behavior\ntf.experimental.numpy.expm1\ntf.experimental.numpy.eye\ntf.experimental.numpy.fabs\ntf.experimental.numpy.finfo\ntf.experimental.numpy.fix\ntf.experimental.numpy.flatten\ntf.experimental.numpy.flip\ntf.experimental.numpy.fliplr\ntf.experimental.numpy.flipud\ntf.experimental.numpy.float16\ntf.experimental.numpy.float32\ntf.experimental.numpy.float64\ntf.experimental.numpy.float_\ntf.experimental.numpy.float_power\ntf.experimental.numpy.floor\ntf.experimental.numpy.floor_divide\ntf.experimental.numpy.full\ntf.experimental.numpy.full_like\ntf.experimental.numpy.gcd\ntf.experimental.numpy.geomspace\ntf.experimental.numpy.greater\ntf.experimental.numpy.greater_equal\ntf.experimental.numpy.heaviside\ntf.experimental.numpy.hsplit\ntf.experimental.numpy.hstack\ntf.experimental.numpy.hypot\ntf.experimental.numpy.identity\ntf.experimental.numpy.iinfo\ntf.experimental.numpy.imag\ntf.experimental.numpy.inexact\ntf.experimental.numpy.inner\ntf.experimental.numpy.int16\ntf.experimental.numpy.int32\ntf.experimental.numpy.int64\ntf.experimental.numpy.int8\ntf.experimental.numpy.int_\ntf.experimental.numpy.isclose\ntf.experimental.numpy.iscomplex\ntf.experimental.numpy.iscomplexobj\ntf.experimental.numpy.isfinite\ntf.experimental.numpy.isinf\ntf.experimental.numpy.isnan\ntf.experimental.numpy.isneginf\ntf.experimental.numpy.isposinf\ntf.experimental.numpy.isreal\ntf.experimental.numpy.isrealobj\ntf.experimental.numpy.isscalar\ntf.experimental.numpy.issubdtype\ntf.experimental.numpy.ix_\ntf.experimental.numpy.kron\ntf.experimental.numpy.lcm\ntf.experimental.numpy.less\ntf.experimental.numpy.less_equal\ntf.experimental.numpy.linspace\ntf.experimental.numpy.log\ntf.experimental.numpy.log10\ntf.experimental.numpy.log1p\ntf.experimental.numpy.log2\ntf.experimental.numpy.logaddexp\ntf.experimental.numpy.logaddexp2\ntf.experimental.numpy.logical_and\ntf.experimental.numpy.logical_not\ntf.experimental.numpy.logical_or\ntf.experimental.numpy.logical_xor\ntf.experimental.numpy.logspace\ntf.experimental.numpy.matmul\ntf.experimental.numpy.max\ntf.experimental.numpy.maximum\ntf.experimental.numpy.mean\ntf.experimental.numpy.meshgrid\ntf.experimental.numpy.min\ntf.experimental.numpy.minimum\ntf.experimental.numpy.mod\ntf.experimental.numpy.moveaxis\ntf.experimental.numpy.multiply\ntf.experimental.numpy.nanmean\ntf.experimental.numpy.nanprod\ntf.experimental.numpy.nansum\ntf.experimental.numpy.ndarray\ntf.experimental.numpy.ndim\ntf.experimental.numpy.negative\ntf.experimental.numpy.nextafter\ntf.experimental.numpy.nonzero\ntf.experimental.numpy.not_equal\ntf.experimental.numpy.object_\ntf.experimental.numpy.ones\ntf.experimental.numpy.ones_like\ntf.experimental.numpy.outer\ntf.experimental.numpy.pad\ntf.experimental.numpy.polyval\ntf.experimental.numpy.positive\ntf.experimental.numpy.power\ntf.experimental.numpy.prod\ntf.experimental.numpy.promote_types\ntf.experimental.numpy.ptp\ntf.experimental.numpy.rad2deg\ntf.experimental.numpy.random\ntf.experimental.numpy.random.poisson\ntf.experimental.numpy.random.rand\ntf.experimental.numpy.random.randint\ntf.experimental.numpy.random.randn\ntf.experimental.numpy.random.random\ntf.experimental.numpy.random.seed\ntf.experimental.numpy.random.standard_normal\ntf.experimental.numpy.random.uniform\ntf.experimental.numpy.ravel\ntf.experimental.numpy.real\ntf.experimental.numpy.reciprocal\ntf.experimental.numpy.remainder\ntf.experimental.numpy.repeat\ntf.experimental.numpy.reshape\ntf.experimental.numpy.result_type\ntf.experimental.numpy.roll\ntf.experimental.numpy.rot90\ntf.experimental.numpy.round\ntf.experimental.numpy.select\ntf.experimental.numpy.shape\ntf.experimental.numpy.sign\ntf.experimental.numpy.signbit\ntf.experimental.numpy.sin\ntf.experimental.numpy.sinc\ntf.experimental.numpy.sinh\ntf.experimental.numpy.size\ntf.experimental.numpy.sort\ntf.experimental.numpy.split\ntf.experimental.numpy.sqrt\ntf.experimental.numpy.square\ntf.experimental.numpy.squeeze\ntf.experimental.numpy.stack\ntf.experimental.numpy.std\ntf.experimental.numpy.string_\ntf.experimental.numpy.subtract\ntf.experimental.numpy.sum\ntf.experimental.numpy.swapaxes\ntf.experimental.numpy.take\ntf.experimental.numpy.take_along_axis\ntf.experimental.numpy.tan\ntf.experimental.numpy.tanh\ntf.experimental.numpy.tensordot\ntf.experimental.numpy.tile\ntf.experimental.numpy.trace\ntf.experimental.numpy.transpose\ntf.experimental.numpy.tri\ntf.experimental.numpy.tril\ntf.experimental.numpy.triu\ntf.experimental.numpy.true_divide\ntf.experimental.numpy.uint16\ntf.experimental.numpy.uint32\ntf.experimental.numpy.uint64\ntf.experimental.numpy.uint8\ntf.experimental.numpy.unicode_\ntf.experimental.numpy.vander\ntf.experimental.numpy.var\ntf.experimental.numpy.vdot\ntf.experimental.numpy.vsplit\ntf.experimental.numpy.vstack\ntf.experimental.numpy.where\ntf.experimental.numpy.zeros\ntf.experimental.numpy.zeros_like\ntf.experimental.register_filesystem_plugin\ntf.experimental.tensorrt\ntf.experimental.tensorrt.ConversionParams\ntf.experimental.tensorrt.Converter\ntf.experimental.unregister_dispatch_for\ntf.extract_volume_patches\ntf.eye\ntf.feature_column\ntf.feature_column.bucketized_column\ntf.feature_column.categorical_column_with_hash_bucket\ntf.feature_column.categorical_column_with_identity\ntf.feature_column.categorical_column_with_vocabulary_file\ntf.feature_column.categorical_column_with_vocabulary_list\ntf.feature_column.crossed_column\ntf.feature_column.embedding_column\ntf.feature_column.indicator_column\ntf.feature_column.make_parse_example_spec\ntf.feature_column.numeric_column\ntf.feature_column.sequence_categorical_column_with_hash_bucket\ntf.feature_column.sequence_categorical_column_with_identity\ntf.feature_column.sequence_categorical_column_with_vocabulary_file\ntf.feature_column.sequence_categorical_column_with_vocabulary_list\ntf.feature_column.sequence_numeric_column\ntf.feature_column.shared_embeddings\ntf.feature_column.weighted_categorical_column\ntf.fftnd\ntf.fill\ntf.fingerprint\ntf.floor\ntf.foldl\ntf.foldr\ntf.function\ntf.gather\ntf.gather_nd\ntf.get_current_name_scope\ntf.get_logger\ntf.get_static_value\ntf.grad_pass_through\ntf.gradients\ntf.graph_util\ntf.graph_util.import_graph_def\ntf.greater\ntf.greater_equal\ntf.group\ntf.guarantee_const\ntf.hessians\ntf.histogram_fixed_width\ntf.histogram_fixed_width_bins\ntf.identity\ntf.identity_n\ntf.ifftnd\ntf.image\ntf.image.ResizeMethod\ntf.image.adjust_brightness\ntf.image.adjust_contrast\ntf.image.adjust_gamma\ntf.image.adjust_hue\ntf.image.adjust_jpeg_quality\ntf.image.adjust_saturation\ntf.image.central_crop\ntf.image.combined_non_max_suppression\ntf.image.convert_image_dtype\ntf.image.crop_and_resize\ntf.image.crop_to_bounding_box\ntf.image.decode_and_crop_jpeg\ntf.image.decode_bmp\ntf.image.decode_gif\ntf.image.decode_image\ntf.image.decode_jpeg\ntf.image.decode_png\ntf.image.draw_bounding_boxes\ntf.image.encode_jpeg\ntf.image.encode_png\ntf.image.extract_glimpse\ntf.image.extract_jpeg_shape\ntf.image.extract_patches\ntf.image.flip_left_right\ntf.image.flip_up_down\ntf.image.generate_bounding_box_proposals\ntf.image.grayscale_to_rgb\ntf.image.hsv_to_rgb\ntf.image.image_gradients\ntf.image.is_jpeg\ntf.image.non_max_suppression\ntf.image.non_max_suppression_overlaps\ntf.image.non_max_suppression_padded\ntf.image.non_max_suppression_with_scores\ntf.image.pad_to_bounding_box\ntf.image.per_image_standardization\ntf.image.psnr\ntf.image.random_brightness\ntf.image.random_contrast\ntf.image.random_crop\ntf.image.random_flip_left_right\ntf.image.random_flip_up_down\ntf.image.random_hue\ntf.image.random_jpeg_quality\ntf.image.random_saturation\ntf.image.resize\ntf.image.resize_with_crop_or_pad\ntf.image.resize_with_pad\ntf.image.rgb_to_grayscale\ntf.image.rgb_to_hsv\ntf.image.rgb_to_yiq\ntf.image.rgb_to_yuv\ntf.image.rot90\ntf.image.sample_distorted_bounding_box\ntf.image.sobel_edges\ntf.image.ssim\ntf.image.ssim_multiscale\ntf.image.stateless_random_brightness\ntf.image.stateless_random_contrast\ntf.image.stateless_random_crop\ntf.image.stateless_random_flip_left_right\ntf.image.stateless_random_flip_up_down\ntf.image.stateless_random_hue\ntf.image.stateless_random_jpeg_quality\ntf.image.stateless_random_saturation\ntf.image.stateless_sample_distorted_bounding_box\ntf.image.total_variation\ntf.image.transpose\ntf.image.yiq_to_rgb\ntf.image.yuv_to_rgb\ntf.import_graph_def\ntf.init_scope\ntf.inside_function\ntf.io\ntf.io.FixedLenFeature\ntf.io.FixedLenSequenceFeature\ntf.io.RaggedFeature\ntf.io.RaggedFeature.RowLengths\ntf.io.RaggedFeature.RowLimits\ntf.io.RaggedFeature.RowSplits\ntf.io.RaggedFeature.RowStarts\ntf.io.RaggedFeature.UniformRowLength\ntf.io.RaggedFeature.ValueRowIds\ntf.io.SparseFeature\ntf.io.TFRecordOptions\ntf.io.TFRecordWriter\ntf.io.VarLenFeature\ntf.io.decode_and_crop_jpeg\ntf.io.decode_base64\ntf.io.decode_bmp\ntf.io.decode_compressed\ntf.io.decode_csv\ntf.io.decode_gif\ntf.io.decode_image\ntf.io.decode_jpeg\ntf.io.decode_json_example\ntf.io.decode_png\ntf.io.decode_proto\ntf.io.decode_raw\ntf.io.deserialize_many_sparse\ntf.io.encode_base64\ntf.io.encode_jpeg\ntf.io.encode_png\ntf.io.encode_proto\ntf.io.extract_jpeg_shape\ntf.io.gfile\ntf.io.gfile.GFile\ntf.io.gfile.copy\ntf.io.gfile.exists\ntf.io.gfile.get_registered_schemes\ntf.io.gfile.glob\ntf.io.gfile.isdir\ntf.io.gfile.join\ntf.io.gfile.listdir\ntf.io.gfile.makedirs\ntf.io.gfile.mkdir\ntf.io.gfile.remove\ntf.io.gfile.rename\ntf.io.gfile.rmtree\ntf.io.gfile.stat\ntf.io.gfile.walk\ntf.io.is_jpeg\ntf.io.match_filenames_once\ntf.io.matching_files\ntf.io.parse_example\ntf.io.parse_sequence_example\ntf.io.parse_single_example\ntf.io.parse_single_sequence_example\ntf.io.parse_tensor\ntf.io.read_file\ntf.io.serialize_many_sparse\ntf.io.serialize_sparse\ntf.io.serialize_tensor\ntf.io.write_file\ntf.io.write_graph\ntf.irfftnd\ntf.is_symbolic_tensor\ntf.is_tensor\ntf.keras\ntf.keras.AbsMaxQuantizer\ntf.keras.DTypePolicy\ntf.keras.FloatDTypePolicy\ntf.keras.Function\ntf.keras.Initializer\ntf.keras.Input\ntf.keras.InputSpec\ntf.keras.KerasTensor\ntf.keras.Layer\ntf.keras.Loss\ntf.keras.Metric\ntf.keras.Model\ntf.keras.Operation\ntf.keras.Optimizer\ntf.keras.QuantizedDTypePolicy\ntf.keras.Quantizer\ntf.keras.Regularizer\ntf.keras.Sequential\ntf.keras.StatelessScope\ntf.keras.Variable\ntf.keras.activations\ntf.keras.activations.deserialize\ntf.keras.activations.elu\ntf.keras.activations.exponential\ntf.keras.activations.gelu\ntf.keras.activations.get\ntf.keras.activations.hard_sigmoid\ntf.keras.activations.hard_silu\ntf.keras.activations.hard_swish\ntf.keras.activations.leaky_relu\ntf.keras.activations.linear\ntf.keras.activations.log_softmax\ntf.keras.activations.mish\ntf.keras.activations.relu\ntf.keras.activations.relu6\ntf.keras.activations.selu\ntf.keras.activations.serialize\ntf.keras.activations.sigmoid\ntf.keras.activations.silu\ntf.keras.activations.softmax\ntf.keras.activations.softplus\ntf.keras.activations.softsign\ntf.keras.activations.swish\ntf.keras.activations.tanh\ntf.keras.applications\ntf.keras.applications.ConvNeXtBase\ntf.keras.applications.ConvNeXtLarge\ntf.keras.applications.ConvNeXtSmall\ntf.keras.applications.ConvNeXtTiny\ntf.keras.applications.ConvNeXtXLarge\ntf.keras.applications.DenseNet121\ntf.keras.applications.DenseNet169\ntf.keras.applications.DenseNet201\ntf.keras.applications.EfficientNetB0\ntf.keras.applications.EfficientNetB1\ntf.keras.applications.EfficientNetB2\ntf.keras.applications.EfficientNetB3\ntf.keras.applications.EfficientNetB4\ntf.keras.applications.EfficientNetB5\ntf.keras.applications.EfficientNetB6\ntf.keras.applications.EfficientNetB7\ntf.keras.applications.EfficientNetV2B0\ntf.keras.applications.EfficientNetV2B1\ntf.keras.applications.EfficientNetV2B2\ntf.keras.applications.EfficientNetV2B3\ntf.keras.applications.EfficientNetV2L\ntf.keras.applications.EfficientNetV2M\ntf.keras.applications.EfficientNetV2S\ntf.keras.applications.InceptionResNetV2\ntf.keras.applications.InceptionV3\ntf.keras.applications.MobileNet\ntf.keras.applications.MobileNetV2\ntf.keras.applications.MobileNetV3Large\ntf.keras.applications.MobileNetV3Small\ntf.keras.applications.NASNetLarge\ntf.keras.applications.NASNetMobile\ntf.keras.applications.ResNet101\ntf.keras.applications.ResNet101V2\ntf.keras.applications.ResNet152\ntf.keras.applications.ResNet152V2\ntf.keras.applications.ResNet50\ntf.keras.applications.ResNet50V2\ntf.keras.applications.VGG16\ntf.keras.applications.VGG19\ntf.keras.applications.Xception\ntf.keras.applications.convnext\ntf.keras.applications.convnext.ConvNeXtBase\ntf.keras.applications.convnext.ConvNeXtLarge\ntf.keras.applications.convnext.ConvNeXtSmall\ntf.keras.applications.convnext.ConvNeXtTiny\ntf.keras.applications.convnext.ConvNeXtXLarge\ntf.keras.applications.convnext.decode_predictions\ntf.keras.applications.convnext.preprocess_input\ntf.keras.applications.densenet\ntf.keras.applications.densenet.DenseNet121\ntf.keras.applications.densenet.DenseNet169\ntf.keras.applications.densenet.DenseNet201\ntf.keras.applications.densenet.decode_predictions\ntf.keras.applications.densenet.preprocess_input\ntf.keras.applications.efficientnet\ntf.keras.applications.efficientnet.EfficientNetB0\ntf.keras.applications.efficientnet.EfficientNetB1\ntf.keras.applications.efficientnet.EfficientNetB2\ntf.keras.applications.efficientnet.EfficientNetB3\ntf.keras.applications.efficientnet.EfficientNetB4\ntf.keras.applications.efficientnet.EfficientNetB5\ntf.keras.applications.efficientnet.EfficientNetB6\ntf.keras.applications.efficientnet.EfficientNetB7\ntf.keras.applications.efficientnet.decode_predictions\ntf.keras.applications.efficientnet.preprocess_input\ntf.keras.applications.efficientnet_v2\ntf.keras.applications.efficientnet_v2.EfficientNetV2B0\ntf.keras.applications.efficientnet_v2.EfficientNetV2B1\ntf.keras.applications.efficientnet_v2.EfficientNetV2B2\ntf.keras.applications.efficientnet_v2.EfficientNetV2B3\ntf.keras.applications.efficientnet_v2.EfficientNetV2L\ntf.keras.applications.efficientnet_v2.EfficientNetV2M\ntf.keras.applications.efficientnet_v2.EfficientNetV2S\ntf.keras.applications.efficientnet_v2.decode_predictions\ntf.keras.applications.efficientnet_v2.preprocess_input\ntf.keras.applications.imagenet_utils\ntf.keras.applications.imagenet_utils.decode_predictions\ntf.keras.applications.imagenet_utils.preprocess_input\ntf.keras.applications.inception_resnet_v2\ntf.keras.applications.inception_resnet_v2.InceptionResNetV2\ntf.keras.applications.inception_resnet_v2.decode_predictions\ntf.keras.applications.inception_resnet_v2.preprocess_input\ntf.keras.applications.inception_v3\ntf.keras.applications.inception_v3.InceptionV3\ntf.keras.applications.inception_v3.decode_predictions\ntf.keras.applications.inception_v3.preprocess_input\ntf.keras.applications.mobilenet\ntf.keras.applications.mobilenet.MobileNet\ntf.keras.applications.mobilenet.decode_predictions\ntf.keras.applications.mobilenet.preprocess_input\ntf.keras.applications.mobilenet_v2\ntf.keras.applications.mobilenet_v2.MobileNetV2\ntf.keras.applications.mobilenet_v2.decode_predictions\ntf.keras.applications.mobilenet_v2.preprocess_input\ntf.keras.applications.mobilenet_v3\ntf.keras.applications.mobilenet_v3.decode_predictions\ntf.keras.applications.mobilenet_v3.preprocess_input\ntf.keras.applications.nasnet\ntf.keras.applications.nasnet.NASNetLarge\ntf.keras.applications.nasnet.NASNetMobile\ntf.keras.applications.nasnet.decode_predictions\ntf.keras.applications.nasnet.preprocess_input\ntf.keras.applications.resnet\ntf.keras.applications.resnet.ResNet101\ntf.keras.applications.resnet.ResNet152\ntf.keras.applications.resnet.ResNet50\ntf.keras.applications.resnet.decode_predictions\ntf.keras.applications.resnet.preprocess_input\ntf.keras.applications.resnet50\ntf.keras.applications.resnet50.ResNet50\ntf.keras.applications.resnet50.decode_predictions\ntf.keras.applications.resnet50.preprocess_input\ntf.keras.applications.resnet_v2\ntf.keras.applications.resnet_v2.ResNet101V2\ntf.keras.applications.resnet_v2.ResNet152V2\ntf.keras.applications.resnet_v2.ResNet50V2\ntf.keras.applications.resnet_v2.decode_predictions\ntf.keras.applications.resnet_v2.preprocess_input\ntf.keras.applications.vgg16\ntf.keras.applications.vgg16.VGG16\ntf.keras.applications.vgg16.decode_predictions\ntf.keras.applications.vgg16.preprocess_input\ntf.keras.applications.vgg19\ntf.keras.applications.vgg19.VGG19\ntf.keras.applications.vgg19.decode_predictions\ntf.keras.applications.vgg19.preprocess_input\ntf.keras.applications.xception\ntf.keras.applications.xception.Xception\ntf.keras.applications.xception.decode_predictions\ntf.keras.applications.xception.preprocess_input\ntf.keras.backend\ntf.keras.backend.abs\ntf.keras.backend.all\ntf.keras.backend.any\ntf.keras.backend.arange\ntf.keras.backend.argmax\ntf.keras.backend.argmin\ntf.keras.backend.backend\ntf.keras.backend.batch_dot\ntf.keras.backend.batch_flatten\ntf.keras.backend.batch_get_value\ntf.keras.backend.batch_normalization\ntf.keras.backend.batch_set_value\ntf.keras.backend.bias_add\ntf.keras.backend.binary_crossentropy\ntf.keras.backend.binary_focal_crossentropy\ntf.keras.backend.cast\ntf.keras.backend.cast_to_floatx\ntf.keras.backend.categorical_crossentropy\ntf.keras.backend.categorical_focal_crossentropy\ntf.keras.backend.clear_session\ntf.keras.backend.clip\ntf.keras.backend.concatenate\ntf.keras.backend.constant\ntf.keras.backend.conv1d\ntf.keras.backend.conv2d\ntf.keras.backend.conv2d_transpose\ntf.keras.backend.conv3d\ntf.keras.backend.cos\ntf.keras.backend.count_params\ntf.keras.backend.ctc_batch_cost\ntf.keras.backend.ctc_decode\ntf.keras.backend.ctc_label_dense_to_sparse\ntf.keras.backend.cumprod\ntf.keras.backend.cumsum\ntf.keras.backend.depthwise_conv2d\ntf.keras.backend.dot\ntf.keras.backend.dropout\ntf.keras.backend.dtype\ntf.keras.backend.elu\ntf.keras.backend.epsilon\ntf.keras.backend.equal\ntf.keras.backend.eval\ntf.keras.backend.exp\ntf.keras.backend.expand_dims\ntf.keras.backend.eye\ntf.keras.backend.flatten\ntf.keras.backend.floatx\ntf.keras.backend.foldl\ntf.keras.backend.foldr\ntf.keras.backend.gather\ntf.keras.backend.get_uid\ntf.keras.backend.get_value\ntf.keras.backend.gradients\ntf.keras.backend.greater\ntf.keras.backend.greater_equal\ntf.keras.backend.hard_sigmoid\ntf.keras.backend.image_data_format\ntf.keras.backend.in_top_k\ntf.keras.backend.int_shape\ntf.keras.backend.is_float_dtype\ntf.keras.backend.is_int_dtype\ntf.keras.backend.is_keras_tensor\ntf.keras.backend.is_sparse\ntf.keras.backend.l2_normalize\ntf.keras.backend.less\ntf.keras.backend.less_equal\ntf.keras.backend.log\ntf.keras.backend.map_fn\ntf.keras.backend.max\ntf.keras.backend.maximum\ntf.keras.backend.mean\ntf.keras.backend.min\ntf.keras.backend.minimum\ntf.keras.backend.moving_average_update\ntf.keras.backend.name_scope\ntf.keras.backend.ndim\ntf.keras.backend.not_equal\ntf.keras.backend.one_hot\ntf.keras.backend.ones\ntf.keras.backend.ones_like\ntf.keras.backend.permute_dimensions\ntf.keras.backend.pool2d\ntf.keras.backend.pool3d\ntf.keras.backend.pow\ntf.keras.backend.prod\ntf.keras.backend.random_bernoulli\ntf.keras.backend.random_normal\ntf.keras.backend.random_normal_variable\ntf.keras.backend.random_uniform\ntf.keras.backend.random_uniform_variable\ntf.keras.backend.relu\ntf.keras.backend.repeat\ntf.keras.backend.repeat_elements\ntf.keras.backend.reshape\ntf.keras.backend.resize_images\ntf.keras.backend.resize_volumes\ntf.keras.backend.result_type\ntf.keras.backend.reverse\ntf.keras.backend.rnn\ntf.keras.backend.round\ntf.keras.backend.separable_conv2d\ntf.keras.backend.set_epsilon\ntf.keras.backend.set_floatx\ntf.keras.backend.set_image_data_format\ntf.keras.backend.set_value\ntf.keras.backend.shape\ntf.keras.backend.sigmoid\ntf.keras.backend.sign\ntf.keras.backend.sin\ntf.keras.backend.softmax\ntf.keras.backend.softplus\ntf.keras.backend.softsign\ntf.keras.backend.sparse_categorical_crossentropy\ntf.keras.backend.spatial_2d_padding\ntf.keras.backend.spatial_3d_padding\ntf.keras.backend.sqrt\ntf.keras.backend.square\ntf.keras.backend.squeeze\ntf.keras.backend.stack\ntf.keras.backend.standardize_dtype\ntf.keras.backend.std\ntf.keras.backend.stop_gradient\ntf.keras.backend.sum\ntf.keras.backend.switch\ntf.keras.backend.tanh\ntf.keras.backend.temporal_padding\ntf.keras.backend.tile\ntf.keras.backend.to_dense\ntf.keras.backend.transpose\ntf.keras.backend.truncated_normal\ntf.keras.backend.update\ntf.keras.backend.update_add\ntf.keras.backend.update_sub\ntf.keras.backend.var\ntf.keras.backend.variable\ntf.keras.backend.zeros\ntf.keras.backend.zeros_like\ntf.keras.callbacks\ntf.keras.callbacks.BackupAndRestore\ntf.keras.callbacks.CSVLogger\ntf.keras.callbacks.Callback\ntf.keras.callbacks.CallbackList\ntf.keras.callbacks.EarlyStopping\ntf.keras.callbacks.History\ntf.keras.callbacks.LambdaCallback\ntf.keras.callbacks.LearningRateScheduler\ntf.keras.callbacks.ModelCheckpoint\ntf.keras.callbacks.ProgbarLogger\ntf.keras.callbacks.ReduceLROnPlateau\ntf.keras.callbacks.RemoteMonitor\ntf.keras.callbacks.SwapEMAWeights\ntf.keras.callbacks.TensorBoard\ntf.keras.callbacks.TerminateOnNaN\ntf.keras.config\ntf.keras.config.backend\ntf.keras.config.disable_interactive_logging\ntf.keras.config.disable_traceback_filtering\ntf.keras.config.dtype_policy\ntf.keras.config.enable_interactive_logging\ntf.keras.config.enable_traceback_filtering\ntf.keras.config.enable_unsafe_deserialization\ntf.keras.config.epsilon\ntf.keras.config.floatx\ntf.keras.config.image_data_format\ntf.keras.config.is_interactive_logging_enabled\ntf.keras.config.is_traceback_filtering_enabled\ntf.keras.config.set_backend\ntf.keras.config.set_dtype_policy\ntf.keras.config.set_epsilon\ntf.keras.config.set_floatx\ntf.keras.config.set_image_data_format\ntf.keras.constraints\ntf.keras.constraints.Constraint\ntf.keras.constraints.MaxNorm\ntf.keras.constraints.MinMaxNorm\ntf.keras.constraints.NonNeg\ntf.keras.constraints.UnitNorm\ntf.keras.constraints.deserialize\ntf.keras.constraints.get\ntf.keras.constraints.max_norm\ntf.keras.constraints.min_max_norm\ntf.keras.constraints.non_neg\ntf.keras.constraints.serialize\ntf.keras.constraints.unit_norm\ntf.keras.datasets\ntf.keras.datasets.boston_housing\ntf.keras.datasets.boston_housing.load_data\ntf.keras.datasets.california_housing\ntf.keras.datasets.california_housing.load_data\ntf.keras.datasets.cifar10\ntf.keras.datasets.cifar10.load_data\ntf.keras.datasets.cifar100\ntf.keras.datasets.cifar100.load_data\ntf.keras.datasets.fashion_mnist\ntf.keras.datasets.fashion_mnist.load_data\ntf.keras.datasets.imdb\ntf.keras.datasets.imdb.get_word_index\ntf.keras.datasets.imdb.load_data\ntf.keras.datasets.mnist\ntf.keras.datasets.mnist.load_data\ntf.keras.datasets.reuters\ntf.keras.datasets.reuters.get_label_names\ntf.keras.datasets.reuters.get_word_index\ntf.keras.datasets.reuters.load_data\ntf.keras.device\ntf.keras.distribution\ntf.keras.distribution.DataParallel\ntf.keras.distribution.DeviceMesh\ntf.keras.distribution.LayoutMap\ntf.keras.distribution.ModelParallel\ntf.keras.distribution.TensorLayout\ntf.keras.distribution.distribute_tensor\ntf.keras.distribution.distribution\ntf.keras.distribution.initialize\ntf.keras.distribution.list_devices\ntf.keras.distribution.set_distribution\ntf.keras.dtype_policies\ntf.keras.dtype_policies.DTypePolicy\ntf.keras.dtype_policies.FloatDTypePolicy\ntf.keras.dtype_policies.QuantizedDTypePolicy\ntf.keras.dtype_policies.QuantizedFloat8DTypePolicy\ntf.keras.dtype_policies.deserialize\ntf.keras.dtype_policies.get\ntf.keras.dtype_policies.serialize\ntf.keras.export\ntf.keras.export.ExportArchive\ntf.keras.initializers\ntf.keras.initializers.Constant\ntf.keras.initializers.GlorotNormal\ntf.keras.initializers.GlorotUniform\ntf.keras.initializers.HeNormal\ntf.keras.initializers.HeUniform\ntf.keras.initializers.Identity\ntf.keras.initializers.IdentityInitializer\ntf.keras.initializers.Initializer\ntf.keras.initializers.LecunNormal\ntf.keras.initializers.LecunUniform\ntf.keras.initializers.Ones\ntf.keras.initializers.Orthogonal\ntf.keras.initializers.OrthogonalInitializer\ntf.keras.initializers.RandomNormal\ntf.keras.initializers.RandomUniform\ntf.keras.initializers.TruncatedNormal\ntf.keras.initializers.VarianceScaling\ntf.keras.initializers.Zeros\ntf.keras.initializers.constant\ntf.keras.initializers.deserialize\ntf.keras.initializers.get\ntf.keras.initializers.glorot_normal\ntf.keras.initializers.glorot_uniform\ntf.keras.initializers.he_normal\ntf.keras.initializers.he_uniform\ntf.keras.initializers.identity\ntf.keras.initializers.lecun_normal\ntf.keras.initializers.lecun_uniform\ntf.keras.initializers.ones\ntf.keras.initializers.orthogonal\ntf.keras.initializers.random_normal\ntf.keras.initializers.random_uniform\ntf.keras.initializers.serialize\ntf.keras.initializers.truncated_normal\ntf.keras.initializers.variance_scaling\ntf.keras.initializers.zeros\ntf.keras.layers\ntf.keras.layers.Activation\ntf.keras.layers.ActivityRegularization\ntf.keras.layers.Add\ntf.keras.layers.AdditiveAttention\ntf.keras.layers.AlphaDropout\ntf.keras.layers.Attention\ntf.keras.layers.Average\ntf.keras.layers.AveragePooling1D\ntf.keras.layers.AveragePooling2D\ntf.keras.layers.AveragePooling3D\ntf.keras.layers.AvgPool1D\ntf.keras.layers.AvgPool2D\ntf.keras.layers.AvgPool3D\ntf.keras.layers.BatchNormalization\ntf.keras.layers.Bidirectional\ntf.keras.layers.CategoryEncoding\ntf.keras.layers.CenterCrop\ntf.keras.layers.Concatenate\ntf.keras.layers.Conv1D\ntf.keras.layers.Conv1DTranspose\ntf.keras.layers.Conv2D\ntf.keras.layers.Conv2DTranspose\ntf.keras.layers.Conv3D\ntf.keras.layers.Conv3DTranspose\ntf.keras.layers.ConvLSTM1D\ntf.keras.layers.ConvLSTM2D\ntf.keras.layers.ConvLSTM3D\ntf.keras.layers.Convolution1D\ntf.keras.layers.Convolution1DTranspose\ntf.keras.layers.Convolution2D\ntf.keras.layers.Convolution2DTranspose\ntf.keras.layers.Convolution3D\ntf.keras.layers.Convolution3DTranspose\ntf.keras.layers.Cropping1D\ntf.keras.layers.Cropping2D\ntf.keras.layers.Cropping3D\ntf.keras.layers.Dense\ntf.keras.layers.DepthwiseConv1D\ntf.keras.layers.DepthwiseConv2D\ntf.keras.layers.Discretization\ntf.keras.layers.Dot\ntf.keras.layers.Dropout\ntf.keras.layers.ELU\ntf.keras.layers.EinsumDense\ntf.keras.layers.Embedding\ntf.keras.layers.Flatten\ntf.keras.layers.FlaxLayer\ntf.keras.layers.GRU\ntf.keras.layers.GRUCell\ntf.keras.layers.GaussianDropout\ntf.keras.layers.GaussianNoise\ntf.keras.layers.GlobalAveragePooling1D\ntf.keras.layers.GlobalAveragePooling2D\ntf.keras.layers.GlobalAveragePooling3D\ntf.keras.layers.GlobalAvgPool1D\ntf.keras.layers.GlobalAvgPool2D\ntf.keras.layers.GlobalAvgPool3D\ntf.keras.layers.GlobalMaxPool1D\ntf.keras.layers.GlobalMaxPool2D\ntf.keras.layers.GlobalMaxPool3D\ntf.keras.layers.GlobalMaxPooling1D\ntf.keras.layers.GlobalMaxPooling2D\ntf.keras.layers.GlobalMaxPooling3D\ntf.keras.layers.GroupNormalization\ntf.keras.layers.GroupQueryAttention\ntf.keras.layers.HashedCrossing\ntf.keras.layers.Hashing\ntf.keras.layers.Identity\ntf.keras.layers.Input\ntf.keras.layers.InputLayer\ntf.keras.layers.InputSpec\ntf.keras.layers.IntegerLookup\ntf.keras.layers.JaxLayer\ntf.keras.layers.LSTM\ntf.keras.layers.LSTMCell\ntf.keras.layers.Lambda\ntf.keras.layers.Layer\ntf.keras.layers.LayerNormalization\ntf.keras.layers.LeakyReLU\ntf.keras.layers.Masking\ntf.keras.layers.MaxPool1D\ntf.keras.layers.MaxPool2D\ntf.keras.layers.MaxPool3D\ntf.keras.layers.MaxPooling1D\ntf.keras.layers.MaxPooling2D\ntf.keras.layers.MaxPooling3D\ntf.keras.layers.Maximum\ntf.keras.layers.MelSpectrogram\ntf.keras.layers.Minimum\ntf.keras.layers.MultiHeadAttention\ntf.keras.layers.Multiply\ntf.keras.layers.Normalization\ntf.keras.layers.PReLU\ntf.keras.layers.Permute\ntf.keras.layers.RNN\ntf.keras.layers.RandomBrightness\ntf.keras.layers.RandomContrast\ntf.keras.layers.RandomCrop\ntf.keras.layers.RandomFlip\ntf.keras.layers.RandomHeight\ntf.keras.layers.RandomRotation\ntf.keras.layers.RandomTranslation\ntf.keras.layers.RandomWidth\ntf.keras.layers.RandomZoom\ntf.keras.layers.ReLU\ntf.keras.layers.RepeatVector\ntf.keras.layers.Rescaling\ntf.keras.layers.Reshape\ntf.keras.layers.Resizing\ntf.keras.layers.SeparableConv1D\ntf.keras.layers.SeparableConv2D\ntf.keras.layers.SeparableConvolution1D\ntf.keras.layers.SeparableConvolution2D\ntf.keras.layers.SimpleRNN\ntf.keras.layers.SimpleRNNCell\ntf.keras.layers.Softmax\ntf.keras.layers.SpatialDropout1D\ntf.keras.layers.SpatialDropout2D\ntf.keras.layers.SpatialDropout3D\ntf.keras.layers.SpectralNormalization\ntf.keras.layers.StackedRNNCells\ntf.keras.layers.StringLookup\ntf.keras.layers.Subtract\ntf.keras.layers.TFSMLayer\ntf.keras.layers.TextVectorization\ntf.keras.layers.ThresholdedReLU\ntf.keras.layers.TimeDistributed\ntf.keras.layers.TorchModuleWrapper\ntf.keras.layers.UnitNormalization\ntf.keras.layers.UpSampling1D\ntf.keras.layers.UpSampling2D\ntf.keras.layers.UpSampling3D\ntf.keras.layers.Wrapper\ntf.keras.layers.ZeroPadding1D\ntf.keras.layers.ZeroPadding2D\ntf.keras.layers.ZeroPadding3D\ntf.keras.layers.add\ntf.keras.layers.average\ntf.keras.layers.concatenate\ntf.keras.layers.deserialize\ntf.keras.layers.dot\ntf.keras.layers.maximum\ntf.keras.layers.minimum\ntf.keras.layers.multiply\ntf.keras.layers.serialize\ntf.keras.layers.subtract\ntf.keras.legacy\ntf.keras.legacy.saving\ntf.keras.legacy.saving.deserialize_keras_object\ntf.keras.legacy.saving.serialize_keras_object\ntf.keras.losses\ntf.keras.losses.BinaryCrossentropy\ntf.keras.losses.BinaryFocalCrossentropy\ntf.keras.losses.CTC\ntf.keras.losses.CategoricalCrossentropy\ntf.keras.losses.CategoricalFocalCrossentropy\ntf.keras.losses.CategoricalHinge\ntf.keras.losses.CosineSimilarity\ntf.keras.losses.Dice\ntf.keras.losses.Hinge\ntf.keras.losses.Huber\ntf.keras.losses.KLD\ntf.keras.losses.KLDivergence\ntf.keras.losses.LogCosh\ntf.keras.losses.Loss\ntf.keras.losses.MAE\ntf.keras.losses.MAPE\ntf.keras.losses.MSE\ntf.keras.losses.MSLE\ntf.keras.losses.MeanAbsoluteError\ntf.keras.losses.MeanAbsolutePercentageError\ntf.keras.losses.MeanSquaredError\ntf.keras.losses.MeanSquaredLogarithmicError\ntf.keras.losses.Poisson\ntf.keras.losses.Reduction\ntf.keras.losses.SparseCategoricalCrossentropy\ntf.keras.losses.SquaredHinge\ntf.keras.losses.Tversky\ntf.keras.losses.binary_crossentropy\ntf.keras.losses.binary_focal_crossentropy\ntf.keras.losses.categorical_crossentropy\ntf.keras.losses.categorical_focal_crossentropy\ntf.keras.losses.categorical_hinge\ntf.keras.losses.cosine_similarity\ntf.keras.losses.ctc\ntf.keras.losses.deserialize\ntf.keras.losses.dice\ntf.keras.losses.get\ntf.keras.losses.hinge\ntf.keras.losses.huber\ntf.keras.losses.kl_divergence\ntf.keras.losses.kld\ntf.keras.losses.kullback_leibler_divergence\ntf.keras.losses.log_cosh\ntf.keras.losses.logcosh\ntf.keras.losses.mae\ntf.keras.losses.mape\ntf.keras.losses.mean_absolute_error\ntf.keras.losses.mean_absolute_percentage_error\ntf.keras.losses.mean_squared_error\ntf.keras.losses.mean_squared_logarithmic_error\ntf.keras.losses.mse\ntf.keras.losses.msle\ntf.keras.losses.poisson\ntf.keras.losses.serialize\ntf.keras.losses.sparse_categorical_crossentropy\ntf.keras.losses.squared_hinge\ntf.keras.losses.tversky\ntf.keras.metrics\ntf.keras.metrics.AUC\ntf.keras.metrics.Accuracy\ntf.keras.metrics.BinaryAccuracy\ntf.keras.metrics.BinaryCrossentropy\ntf.keras.metrics.BinaryIoU\ntf.keras.metrics.CategoricalAccuracy\ntf.keras.metrics.CategoricalCrossentropy\ntf.keras.metrics.CategoricalHinge\ntf.keras.metrics.CosineSimilarity\ntf.keras.metrics.F1Score\ntf.keras.metrics.FBetaScore\ntf.keras.metrics.FalseNegatives\ntf.keras.metrics.FalsePositives\ntf.keras.metrics.Hinge\ntf.keras.metrics.IoU\ntf.keras.metrics.KLD\ntf.keras.metrics.KLDivergence\ntf.keras.metrics.LogCoshError\ntf.keras.metrics.MAE\ntf.keras.metrics.MAPE\ntf.keras.metrics.MSE\ntf.keras.metrics.MSLE\ntf.keras.metrics.Mean\ntf.keras.metrics.MeanAbsoluteError\ntf.keras.metrics.MeanAbsolutePercentageError\ntf.keras.metrics.MeanIoU\ntf.keras.metrics.MeanMetricWrapper\ntf.keras.metrics.MeanSquaredError\ntf.keras.metrics.MeanSquaredLogarithmicError\ntf.keras.metrics.Metric\ntf.keras.metrics.OneHotIoU\ntf.keras.metrics.OneHotMeanIoU\ntf.keras.metrics.Poisson\ntf.keras.metrics.Precision\ntf.keras.metrics.PrecisionAtRecall\ntf.keras.metrics.R2Score\ntf.keras.metrics.Recall\ntf.keras.metrics.RecallAtPrecision\ntf.keras.metrics.RootMeanSquaredError\ntf.keras.metrics.SensitivityAtSpecificity\ntf.keras.metrics.SparseCategoricalAccuracy\ntf.keras.metrics.SparseCategoricalCrossentropy\ntf.keras.metrics.SparseTopKCategoricalAccuracy\ntf.keras.metrics.SpecificityAtSensitivity\ntf.keras.metrics.SquaredHinge\ntf.keras.metrics.Sum\ntf.keras.metrics.TopKCategoricalAccuracy\ntf.keras.metrics.TrueNegatives\ntf.keras.metrics.TruePositives\ntf.keras.metrics.binary_accuracy\ntf.keras.metrics.binary_crossentropy\ntf.keras.metrics.binary_focal_crossentropy\ntf.keras.metrics.categorical_accuracy\ntf.keras.metrics.categorical_crossentropy\ntf.keras.metrics.categorical_focal_crossentropy\ntf.keras.metrics.categorical_hinge\ntf.keras.metrics.deserialize\ntf.keras.metrics.get\ntf.keras.metrics.hinge\ntf.keras.metrics.huber\ntf.keras.metrics.kl_divergence\ntf.keras.metrics.kld\ntf.keras.metrics.kullback_leibler_divergence\ntf.keras.metrics.log_cosh\ntf.keras.metrics.logcosh\ntf.keras.metrics.mae\ntf.keras.metrics.mape\ntf.keras.metrics.mean_absolute_error\ntf.keras.metrics.mean_absolute_percentage_error\ntf.keras.metrics.mean_squared_error\ntf.keras.metrics.mean_squared_logarithmic_error\ntf.keras.metrics.mse\ntf.keras.metrics.msle\ntf.keras.metrics.poisson\ntf.keras.metrics.serialize\ntf.keras.metrics.sparse_categorical_accuracy\ntf.keras.metrics.sparse_categorical_crossentropy\ntf.keras.metrics.sparse_top_k_categorical_accuracy\ntf.keras.metrics.squared_hinge\ntf.keras.metrics.top_k_categorical_accuracy\ntf.keras.mixed_precision\ntf.keras.mixed_precision.DTypePolicy\ntf.keras.mixed_precision.LossScaleOptimizer\ntf.keras.mixed_precision.Policy\ntf.keras.mixed_precision.dtype_policy\ntf.keras.mixed_precision.global_policy\ntf.keras.mixed_precision.set_dtype_policy\ntf.keras.mixed_precision.set_global_policy\ntf.keras.models\ntf.keras.models.Model\ntf.keras.models.Sequential\ntf.keras.models.clone_model\ntf.keras.models.load_model\ntf.keras.models.model_from_json\ntf.keras.models.save_model\ntf.keras.name_scope\ntf.keras.ops\ntf.keras.ops.abs\ntf.keras.ops.absolute\ntf.keras.ops.add\ntf.keras.ops.all\ntf.keras.ops.amax\ntf.keras.ops.amin\ntf.keras.ops.any\ntf.keras.ops.append\ntf.keras.ops.arange\ntf.keras.ops.arccos\ntf.keras.ops.arccosh\ntf.keras.ops.arcsin\ntf.keras.ops.arcsinh\ntf.keras.ops.arctan\ntf.keras.ops.arctan2\ntf.keras.ops.arctanh\ntf.keras.ops.argmax\ntf.keras.ops.argmin\ntf.keras.ops.argsort\ntf.keras.ops.array\ntf.keras.ops.average\ntf.keras.ops.average_pool\ntf.keras.ops.batch_normalization\ntf.keras.ops.binary_crossentropy\ntf.keras.ops.bincount\ntf.keras.ops.broadcast_to\ntf.keras.ops.cast\ntf.keras.ops.categorical_crossentropy\ntf.keras.ops.ceil\ntf.keras.ops.cholesky\ntf.keras.ops.clip\ntf.keras.ops.concatenate\ntf.keras.ops.cond\ntf.keras.ops.conj\ntf.keras.ops.conjugate\ntf.keras.ops.conv\ntf.keras.ops.conv_transpose\ntf.keras.ops.convert_to_numpy\ntf.keras.ops.convert_to_tensor\ntf.keras.ops.copy\ntf.keras.ops.correlate\ntf.keras.ops.cos\ntf.keras.ops.cosh\ntf.keras.ops.count_nonzero\ntf.keras.ops.cross\ntf.keras.ops.ctc_decode\ntf.keras.ops.ctc_loss\ntf.keras.ops.cumprod\ntf.keras.ops.cumsum\ntf.keras.ops.custom_gradient\ntf.keras.ops.depthwise_conv\ntf.keras.ops.det\ntf.keras.ops.diag\ntf.keras.ops.diagonal\ntf.keras.ops.diff\ntf.keras.ops.digitize\ntf.keras.ops.divide\ntf.keras.ops.divide_no_nan\ntf.keras.ops.dot\ntf.keras.ops.eig\ntf.keras.ops.eigh\ntf.keras.ops.einsum\ntf.keras.ops.elu\ntf.keras.ops.empty\ntf.keras.ops.equal\ntf.keras.ops.erf\ntf.keras.ops.erfinv\ntf.keras.ops.exp\ntf.keras.ops.expand_dims\ntf.keras.ops.expm1\ntf.keras.ops.extract_sequences\ntf.keras.ops.eye\ntf.keras.ops.fft\ntf.keras.ops.fft2\ntf.keras.ops.flip\ntf.keras.ops.floor\ntf.keras.ops.floor_divide\ntf.keras.ops.fori_loop\ntf.keras.ops.full\ntf.keras.ops.full_like\ntf.keras.ops.gelu\ntf.keras.ops.get_item\ntf.keras.ops.greater\ntf.keras.ops.greater_equal\ntf.keras.ops.hard_sigmoid\ntf.keras.ops.hard_silu\ntf.keras.ops.hard_swish\ntf.keras.ops.hstack\ntf.keras.ops.identity\ntf.keras.ops.imag\ntf.keras.ops.image\ntf.keras.ops.image.affine_transform\ntf.keras.ops.image.crop_images\ntf.keras.ops.image.extract_patches\ntf.keras.ops.image.map_coordinates\ntf.keras.ops.image.pad_images\ntf.keras.ops.image.resize\ntf.keras.ops.image.rgb_to_grayscale\ntf.keras.ops.in_top_k\ntf.keras.ops.inv\ntf.keras.ops.irfft\ntf.keras.ops.is_tensor\ntf.keras.ops.isclose\ntf.keras.ops.isfinite\ntf.keras.ops.isinf\ntf.keras.ops.isnan\ntf.keras.ops.istft\ntf.keras.ops.leaky_relu\ntf.keras.ops.less\ntf.keras.ops.less_equal\ntf.keras.ops.linalg\ntf.keras.ops.linalg.cholesky\ntf.keras.ops.linalg.det\ntf.keras.ops.linalg.eig\ntf.keras.ops.linalg.eigh\ntf.keras.ops.linalg.inv\ntf.keras.ops.linalg.lu_factor\ntf.keras.ops.linalg.norm\ntf.keras.ops.linalg.qr\ntf.keras.ops.linalg.solve\ntf.keras.ops.linalg.solve_triangular\ntf.keras.ops.linalg.svd\ntf.keras.ops.linspace\ntf.keras.ops.log\ntf.keras.ops.log10\ntf.keras.ops.log1p\ntf.keras.ops.log2\ntf.keras.ops.log_sigmoid\ntf.keras.ops.log_softmax\ntf.keras.ops.logaddexp\ntf.keras.ops.logical_and\ntf.keras.ops.logical_not\ntf.keras.ops.logical_or\ntf.keras.ops.logical_xor\ntf.keras.ops.logspace\ntf.keras.ops.logsumexp\ntf.keras.ops.lu_factor\ntf.keras.ops.matmul\ntf.keras.ops.max\ntf.keras.ops.max_pool\ntf.keras.ops.maximum\ntf.keras.ops.mean\ntf.keras.ops.median\ntf.keras.ops.meshgrid\ntf.keras.ops.min\ntf.keras.ops.minimum\ntf.keras.ops.mod\ntf.keras.ops.moments\ntf.keras.ops.moveaxis\ntf.keras.ops.multi_hot\ntf.keras.ops.multiply\ntf.keras.ops.nan_to_num\ntf.keras.ops.ndim\ntf.keras.ops.negative\ntf.keras.ops.nn\ntf.keras.ops.nn.average_pool\ntf.keras.ops.nn.batch_normalization\ntf.keras.ops.nn.binary_crossentropy\ntf.keras.ops.nn.categorical_crossentropy\ntf.keras.ops.nn.conv\ntf.keras.ops.nn.conv_transpose\ntf.keras.ops.nn.ctc_decode\ntf.keras.ops.nn.ctc_loss\ntf.keras.ops.nn.depthwise_conv\ntf.keras.ops.nn.elu\ntf.keras.ops.nn.gelu\ntf.keras.ops.nn.hard_sigmoid\ntf.keras.ops.nn.hard_silu\ntf.keras.ops.nn.hard_swish\ntf.keras.ops.nn.leaky_relu\ntf.keras.ops.nn.log_sigmoid\ntf.keras.ops.nn.log_softmax\ntf.keras.ops.nn.max_pool\ntf.keras.ops.nn.moments\ntf.keras.ops.nn.multi_hot\ntf.keras.ops.nn.normalize\ntf.keras.ops.nn.one_hot\ntf.keras.ops.nn.relu\ntf.keras.ops.nn.relu6\ntf.keras.ops.nn.selu\ntf.keras.ops.nn.separable_conv\ntf.keras.ops.nn.sigmoid\ntf.keras.ops.nn.silu\ntf.keras.ops.nn.softmax\ntf.keras.ops.nn.softplus\ntf.keras.ops.nn.softsign\ntf.keras.ops.nn.sparse_categorical_crossentropy\ntf.keras.ops.nn.swish\ntf.keras.ops.nonzero\ntf.keras.ops.norm\ntf.keras.ops.normalize\ntf.keras.ops.not_equal\ntf.keras.ops.numpy\ntf.keras.ops.numpy.abs\ntf.keras.ops.numpy.absolute\ntf.keras.ops.numpy.add\ntf.keras.ops.numpy.all\ntf.keras.ops.numpy.amax\ntf.keras.ops.numpy.amin\ntf.keras.ops.numpy.any\ntf.keras.ops.numpy.append\ntf.keras.ops.numpy.arange\ntf.keras.ops.numpy.arccos\ntf.keras.ops.numpy.arccosh\ntf.keras.ops.numpy.arcsin\ntf.keras.ops.numpy.arcsinh\ntf.keras.ops.numpy.arctan\ntf.keras.ops.numpy.arctan2\ntf.keras.ops.numpy.arctanh\ntf.keras.ops.numpy.argmax\ntf.keras.ops.numpy.argmin\ntf.keras.ops.numpy.argsort\ntf.keras.ops.numpy.array\ntf.keras.ops.numpy.average\ntf.keras.ops.numpy.bincount\ntf.keras.ops.numpy.broadcast_to\ntf.keras.ops.numpy.ceil\ntf.keras.ops.numpy.clip\ntf.keras.ops.numpy.concatenate\ntf.keras.ops.numpy.conj\ntf.keras.ops.numpy.conjugate\ntf.keras.ops.numpy.copy\ntf.keras.ops.numpy.correlate\ntf.keras.ops.numpy.cos\ntf.keras.ops.numpy.cosh\ntf.keras.ops.numpy.count_nonzero\ntf.keras.ops.numpy.cross\ntf.keras.ops.numpy.cumprod\ntf.keras.ops.numpy.cumsum\ntf.keras.ops.numpy.diag\ntf.keras.ops.numpy.diagonal\ntf.keras.ops.numpy.diff\ntf.keras.ops.numpy.digitize\ntf.keras.ops.numpy.divide\ntf.keras.ops.numpy.divide_no_nan\ntf.keras.ops.numpy.dot\ntf.keras.ops.numpy.einsum\ntf.keras.ops.numpy.empty\ntf.keras.ops.numpy.equal\ntf.keras.ops.numpy.exp\ntf.keras.ops.numpy.expand_dims\ntf.keras.ops.numpy.expm1\ntf.keras.ops.numpy.eye\ntf.keras.ops.numpy.flip\ntf.keras.ops.numpy.floor\ntf.keras.ops.numpy.floor_divide\ntf.keras.ops.numpy.full\ntf.keras.ops.numpy.full_like\ntf.keras.ops.numpy.get_item\ntf.keras.ops.numpy.greater\ntf.keras.ops.numpy.greater_equal\ntf.keras.ops.numpy.hstack\ntf.keras.ops.numpy.identity\ntf.keras.ops.numpy.imag\ntf.keras.ops.numpy.isclose\ntf.keras.ops.numpy.isfinite\ntf.keras.ops.numpy.isinf\ntf.keras.ops.numpy.isnan\ntf.keras.ops.numpy.less\ntf.keras.ops.numpy.less_equal\ntf.keras.ops.numpy.linspace\ntf.keras.ops.numpy.log\ntf.keras.ops.numpy.log10\ntf.keras.ops.numpy.log1p\ntf.keras.ops.numpy.log2\ntf.keras.ops.numpy.logaddexp\ntf.keras.ops.numpy.logical_and\ntf.keras.ops.numpy.logical_not\ntf.keras.ops.numpy.logical_or\ntf.keras.ops.numpy.logical_xor\ntf.keras.ops.numpy.logspace\ntf.keras.ops.numpy.matmul\ntf.keras.ops.numpy.max\ntf.keras.ops.numpy.maximum\ntf.keras.ops.numpy.mean\ntf.keras.ops.numpy.median\ntf.keras.ops.numpy.meshgrid\ntf.keras.ops.numpy.min\ntf.keras.ops.numpy.minimum\ntf.keras.ops.numpy.mod\ntf.keras.ops.numpy.moveaxis\ntf.keras.ops.numpy.multiply\ntf.keras.ops.numpy.nan_to_num\ntf.keras.ops.numpy.ndim\ntf.keras.ops.numpy.negative\ntf.keras.ops.numpy.nonzero\ntf.keras.ops.numpy.not_equal\ntf.keras.ops.numpy.ones\ntf.keras.ops.numpy.ones_like\ntf.keras.ops.numpy.outer\ntf.keras.ops.numpy.pad\ntf.keras.ops.numpy.power\ntf.keras.ops.numpy.prod\ntf.keras.ops.numpy.quantile\ntf.keras.ops.numpy.ravel\ntf.keras.ops.numpy.real\ntf.keras.ops.numpy.reciprocal\ntf.keras.ops.numpy.repeat\ntf.keras.ops.numpy.reshape\ntf.keras.ops.numpy.roll\ntf.keras.ops.numpy.round\ntf.keras.ops.numpy.select\ntf.keras.ops.numpy.sign\ntf.keras.ops.numpy.sin\ntf.keras.ops.numpy.sinh\ntf.keras.ops.numpy.size\ntf.keras.ops.numpy.sort\ntf.keras.ops.numpy.split\ntf.keras.ops.numpy.sqrt\ntf.keras.ops.numpy.square\ntf.keras.ops.numpy.squeeze\ntf.keras.ops.numpy.stack\ntf.keras.ops.numpy.std\ntf.keras.ops.numpy.subtract\ntf.keras.ops.numpy.sum\ntf.keras.ops.numpy.swapaxes\ntf.keras.ops.numpy.take\ntf.keras.ops.numpy.take_along_axis\ntf.keras.ops.numpy.tan\ntf.keras.ops.numpy.tanh\ntf.keras.ops.numpy.tensordot\ntf.keras.ops.numpy.tile\ntf.keras.ops.numpy.trace\ntf.keras.ops.numpy.transpose\ntf.keras.ops.numpy.tri\ntf.keras.ops.numpy.tril\ntf.keras.ops.numpy.triu\ntf.keras.ops.numpy.true_divide\ntf.keras.ops.numpy.var\ntf.keras.ops.numpy.vdot\ntf.keras.ops.numpy.vectorize\ntf.keras.ops.numpy.vstack\ntf.keras.ops.numpy.where\ntf.keras.ops.numpy.zeros\ntf.keras.ops.numpy.zeros_like\ntf.keras.ops.one_hot\ntf.keras.ops.ones\ntf.keras.ops.ones_like\ntf.keras.ops.outer\ntf.keras.ops.pad\ntf.keras.ops.power\ntf.keras.ops.prod\ntf.keras.ops.qr\ntf.keras.ops.quantile\ntf.keras.ops.ravel\ntf.keras.ops.real\ntf.keras.ops.reciprocal\ntf.keras.ops.relu\ntf.keras.ops.relu6\ntf.keras.ops.repeat\ntf.keras.ops.reshape\ntf.keras.ops.rfft\ntf.keras.ops.roll\ntf.keras.ops.round\ntf.keras.ops.rsqrt\ntf.keras.ops.scatter\ntf.keras.ops.scatter_update\ntf.keras.ops.segment_max\ntf.keras.ops.segment_sum\ntf.keras.ops.select\ntf.keras.ops.selu\ntf.keras.ops.separable_conv\ntf.keras.ops.shape\ntf.keras.ops.sigmoid\ntf.keras.ops.sign\ntf.keras.ops.silu\ntf.keras.ops.sin\ntf.keras.ops.sinh\ntf.keras.ops.size\ntf.keras.ops.slice\ntf.keras.ops.slice_update\ntf.keras.ops.softmax\ntf.keras.ops.softplus\ntf.keras.ops.softsign\ntf.keras.ops.solve\ntf.keras.ops.solve_triangular\ntf.keras.ops.sort\ntf.keras.ops.sparse_categorical_crossentropy\ntf.keras.ops.split\ntf.keras.ops.sqrt\ntf.keras.ops.square\ntf.keras.ops.squeeze\ntf.keras.ops.stack\ntf.keras.ops.std\ntf.keras.ops.stft\ntf.keras.ops.stop_gradient\ntf.keras.ops.subtract\ntf.keras.ops.sum\ntf.keras.ops.svd\ntf.keras.ops.swapaxes\ntf.keras.ops.swish\ntf.keras.ops.take\ntf.keras.ops.take_along_axis\ntf.keras.ops.tan\ntf.keras.ops.tanh\ntf.keras.ops.tensordot\ntf.keras.ops.tile\ntf.keras.ops.top_k\ntf.keras.ops.trace\ntf.keras.ops.transpose\ntf.keras.ops.tri\ntf.keras.ops.tril\ntf.keras.ops.triu\ntf.keras.ops.true_divide\ntf.keras.ops.unstack\ntf.keras.ops.var\ntf.keras.ops.vdot\ntf.keras.ops.vectorize\ntf.keras.ops.vectorized_map\ntf.keras.ops.vstack\ntf.keras.ops.where\ntf.keras.ops.while_loop\ntf.keras.ops.zeros\ntf.keras.ops.zeros_like\ntf.keras.optimizers\ntf.keras.optimizers.Adadelta\ntf.keras.optimizers.Adafactor\ntf.keras.optimizers.Adagrad\ntf.keras.optimizers.Adam\ntf.keras.optimizers.AdamW\ntf.keras.optimizers.Adamax\ntf.keras.optimizers.Ftrl\ntf.keras.optimizers.Lion\ntf.keras.optimizers.LossScaleOptimizer\ntf.keras.optimizers.Nadam\ntf.keras.optimizers.Optimizer\ntf.keras.optimizers.RMSprop\ntf.keras.optimizers.SGD\ntf.keras.optimizers.deserialize\ntf.keras.optimizers.get\ntf.keras.optimizers.legacy\ntf.keras.optimizers.legacy.Adagrad\ntf.keras.optimizers.legacy.Adam\ntf.keras.optimizers.legacy.Ftrl\ntf.keras.optimizers.legacy.Optimizer\ntf.keras.optimizers.legacy.RMSprop\ntf.keras.optimizers.legacy.SGD\ntf.keras.optimizers.schedules\ntf.keras.optimizers.schedules.CosineDecay\ntf.keras.optimizers.schedules.CosineDecayRestarts\ntf.keras.optimizers.schedules.ExponentialDecay\ntf.keras.optimizers.schedules.InverseTimeDecay\ntf.keras.optimizers.schedules.LearningRateSchedule\ntf.keras.optimizers.schedules.PiecewiseConstantDecay\ntf.keras.optimizers.schedules.PolynomialDecay\ntf.keras.optimizers.schedules.deserialize\ntf.keras.optimizers.schedules.serialize\ntf.keras.optimizers.serialize\ntf.keras.preprocessing\ntf.keras.preprocessing.image\ntf.keras.preprocessing.image.DirectoryIterator\ntf.keras.preprocessing.image.ImageDataGenerator\ntf.keras.preprocessing.image.Iterator\ntf.keras.preprocessing.image.NumpyArrayIterator\ntf.keras.preprocessing.image.apply_affine_transform\ntf.keras.preprocessing.image.apply_brightness_shift\ntf.keras.preprocessing.image.apply_channel_shift\ntf.keras.preprocessing.image.array_to_img\ntf.keras.preprocessing.image.img_to_array\ntf.keras.preprocessing.image.load_img\ntf.keras.preprocessing.image.random_brightness\ntf.keras.preprocessing.image.random_channel_shift\ntf.keras.preprocessing.image.random_rotation\ntf.keras.preprocessing.image.random_shear\ntf.keras.preprocessing.image.random_shift\ntf.keras.preprocessing.image.random_zoom\ntf.keras.preprocessing.image.save_img\ntf.keras.preprocessing.image.smart_resize\ntf.keras.preprocessing.image_dataset_from_directory\ntf.keras.preprocessing.sequence\ntf.keras.preprocessing.sequence.TimeseriesGenerator\ntf.keras.preprocessing.sequence.make_sampling_table\ntf.keras.preprocessing.sequence.pad_sequences\ntf.keras.preprocessing.sequence.skipgrams\ntf.keras.preprocessing.text\ntf.keras.preprocessing.text.Tokenizer\ntf.keras.preprocessing.text.hashing_trick\ntf.keras.preprocessing.text.one_hot\ntf.keras.preprocessing.text.text_to_word_sequence\ntf.keras.preprocessing.text.tokenizer_from_json\ntf.keras.preprocessing.text_dataset_from_directory\ntf.keras.preprocessing.timeseries_dataset_from_array\ntf.keras.quantizers\ntf.keras.quantizers.AbsMaxQuantizer\ntf.keras.quantizers.Quantizer\ntf.keras.quantizers.abs_max_quantize\ntf.keras.quantizers.compute_float8_amax_history\ntf.keras.quantizers.compute_float8_scale\ntf.keras.quantizers.deserialize\ntf.keras.quantizers.get\ntf.keras.quantizers.quantize_and_dequantize\ntf.keras.quantizers.serialize\ntf.keras.random\ntf.keras.random.SeedGenerator\ntf.keras.random.beta\ntf.keras.random.binomial\ntf.keras.random.categorical\ntf.keras.random.dropout\ntf.keras.random.gamma\ntf.keras.random.normal\ntf.keras.random.randint\ntf.keras.random.shuffle\ntf.keras.random.truncated_normal\ntf.keras.random.uniform\ntf.keras.regularizers\ntf.keras.regularizers.L1\ntf.keras.regularizers.L1L2\ntf.keras.regularizers.L2\ntf.keras.regularizers.OrthogonalRegularizer\ntf.keras.regularizers.Regularizer\ntf.keras.regularizers.deserialize\ntf.keras.regularizers.get\ntf.keras.regularizers.l1\ntf.keras.regularizers.l1_l2\ntf.keras.regularizers.l2\ntf.keras.regularizers.orthogonal_regularizer\ntf.keras.regularizers.serialize\ntf.keras.tree\ntf.keras.tree.assert_same_structure\ntf.keras.tree.flatten\ntf.keras.tree.is_nested\ntf.keras.tree.lists_to_tuples\ntf.keras.tree.map_shape_structure\ntf.keras.tree.map_structure\ntf.keras.tree.map_structure_up_to\ntf.keras.tree.pack_sequence_as\ntf.keras.tree.traverse\ntf.keras.utils\ntf.keras.utils.CustomObjectScope\ntf.keras.utils.FeatureSpace\ntf.keras.utils.Progbar\ntf.keras.utils.PyDataset\ntf.keras.utils.Sequence\ntf.keras.utils.array_to_img\ntf.keras.utils.audio_dataset_from_directory\ntf.keras.utils.clear_session\ntf.keras.utils.custom_object_scope\ntf.keras.utils.deserialize_keras_object\ntf.keras.utils.disable_interactive_logging\ntf.keras.utils.enable_interactive_logging\ntf.keras.utils.get_custom_objects\ntf.keras.utils.get_file\ntf.keras.utils.get_registered_name\ntf.keras.utils.get_registered_object\ntf.keras.utils.get_source_inputs\ntf.keras.utils.image_dataset_from_directory\ntf.keras.utils.img_to_array\ntf.keras.utils.is_interactive_logging_enabled\ntf.keras.utils.is_keras_tensor\ntf.keras.utils.legacy\ntf.keras.utils.legacy.deserialize_keras_object\ntf.keras.utils.legacy.serialize_keras_object\ntf.keras.utils.load_img\ntf.keras.utils.model_to_dot\ntf.keras.utils.normalize\ntf.keras.utils.pack_x_y_sample_weight\ntf.keras.utils.pad_sequences\ntf.keras.utils.plot_model\ntf.keras.utils.register_keras_serializable\ntf.keras.utils.save_img\ntf.keras.utils.serialize_keras_object\ntf.keras.utils.set_random_seed\ntf.keras.utils.split_dataset\ntf.keras.utils.standardize_dtype\ntf.keras.utils.text_dataset_from_directory\ntf.keras.utils.timeseries_dataset_from_array\ntf.keras.utils.to_categorical\ntf.keras.utils.unpack_x_y_sample_weight\ntf.keras.version\ntf.less\ntf.less_equal\ntf.linalg\ntf.linalg.LinearOperator\ntf.linalg.LinearOperatorAdjoint\ntf.linalg.LinearOperatorBlockDiag\ntf.linalg.LinearOperatorBlockLowerTriangular\ntf.linalg.LinearOperatorCirculant\ntf.linalg.LinearOperatorCirculant2D\ntf.linalg.LinearOperatorCirculant3D\ntf.linalg.LinearOperatorComposition\ntf.linalg.LinearOperatorDiag\ntf.linalg.LinearOperatorFullMatrix\ntf.linalg.LinearOperatorHouseholder\ntf.linalg.LinearOperatorIdentity\ntf.linalg.LinearOperatorInversion\ntf.linalg.LinearOperatorKronecker\ntf.linalg.LinearOperatorLowRankUpdate\ntf.linalg.LinearOperatorLowerTriangular\ntf.linalg.LinearOperatorPermutation\ntf.linalg.LinearOperatorScaledIdentity\ntf.linalg.LinearOperatorToeplitz\ntf.linalg.LinearOperatorTridiag\ntf.linalg.LinearOperatorZeros\ntf.linalg.adjoint\ntf.linalg.band_part\ntf.linalg.banded_triangular_solve\ntf.linalg.cholesky\ntf.linalg.cholesky_solve\ntf.linalg.cross\ntf.linalg.det\ntf.linalg.diag\ntf.linalg.diag_part\ntf.linalg.eig\ntf.linalg.eigh\ntf.linalg.eigh_tridiagonal\ntf.linalg.eigvals\ntf.linalg.eigvalsh\ntf.linalg.einsum\ntf.linalg.experimental\ntf.linalg.experimental.conjugate_gradient\ntf.linalg.expm\ntf.linalg.eye\ntf.linalg.global_norm\ntf.linalg.inv\ntf.linalg.l2_normalize\ntf.linalg.logdet\ntf.linalg.logm\ntf.linalg.lstsq\ntf.linalg.lu\ntf.linalg.lu_matrix_inverse\ntf.linalg.lu_reconstruct\ntf.linalg.lu_solve\ntf.linalg.matmul\ntf.linalg.matrix_rank\ntf.linalg.matrix_transpose\ntf.linalg.matvec\ntf.linalg.norm\ntf.linalg.normalize\ntf.linalg.pinv\ntf.linalg.qr\ntf.linalg.set_diag\ntf.linalg.slogdet\ntf.linalg.solve\ntf.linalg.sqrtm\ntf.linalg.svd\ntf.linalg.tensor_diag\ntf.linalg.tensor_diag_part\ntf.linalg.tensordot\ntf.linalg.trace\ntf.linalg.triangular_solve\ntf.linalg.tridiagonal_matmul\ntf.linalg.tridiagonal_solve\ntf.linspace\ntf.lite\ntf.lite.Interpreter\ntf.lite.OpsSet\ntf.lite.Optimize\ntf.lite.RepresentativeDataset\ntf.lite.TFLiteConverter\ntf.lite.TargetSpec\ntf.lite.experimental\ntf.lite.experimental.Analyzer\ntf.lite.experimental.OpResolverType\ntf.lite.experimental.QuantizationDebugOptions\ntf.lite.experimental.QuantizationDebugger\ntf.lite.experimental.authoring\ntf.lite.experimental.authoring.compatible\ntf.lite.experimental.load_delegate\ntf.load_library\ntf.load_op_library\ntf.logical_and\ntf.logical_not\ntf.logical_or\ntf.lookup\ntf.lookup.KeyValueTensorInitializer\ntf.lookup.StaticHashTable\ntf.lookup.StaticVocabularyTable\ntf.lookup.TextFileIndex\ntf.lookup.TextFileInitializer\ntf.lookup.experimental\ntf.lookup.experimental.DenseHashTable\ntf.lookup.experimental.MutableHashTable\ntf.make_ndarray\ntf.make_tensor_proto\ntf.map_fn\ntf.math\ntf.math.abs\ntf.math.accumulate_n\ntf.math.acos\ntf.math.acosh\ntf.math.add\ntf.math.add_n\ntf.math.angle\ntf.math.approx_max_k\ntf.math.approx_min_k\ntf.math.argmax\ntf.math.argmin\ntf.math.asin\ntf.math.asinh\ntf.math.atan\ntf.math.atan2\ntf.math.atanh\ntf.math.bessel_i0\ntf.math.bessel_i0e\ntf.math.bessel_i1\ntf.math.bessel_i1e\ntf.math.betainc\ntf.math.bincount\ntf.math.ceil\ntf.math.confusion_matrix\ntf.math.conj\ntf.math.cos\ntf.math.cosh\ntf.math.count_nonzero\ntf.math.cumprod\ntf.math.cumsum\ntf.math.cumulative_logsumexp\ntf.math.digamma\ntf.math.divide\ntf.math.divide_no_nan\ntf.math.equal\ntf.math.erf\ntf.math.erfc\ntf.math.erfcinv\ntf.math.erfinv\ntf.math.exp\ntf.math.expm1\ntf.math.floor\ntf.math.floordiv\ntf.math.floormod\ntf.math.greater\ntf.math.greater_equal\ntf.math.igamma\ntf.math.igammac\ntf.math.imag\ntf.math.in_top_k\ntf.math.invert_permutation\ntf.math.is_finite\ntf.math.is_inf\ntf.math.is_nan\ntf.math.is_non_decreasing\ntf.math.is_strictly_increasing\ntf.math.l2_normalize\ntf.math.lbeta\ntf.math.less\ntf.math.less_equal\ntf.math.lgamma\ntf.math.log\ntf.math.log1p\ntf.math.log_sigmoid\ntf.math.log_softmax\ntf.math.logical_and\ntf.math.logical_not\ntf.math.logical_or\ntf.math.logical_xor\ntf.math.maximum\ntf.math.minimum\ntf.math.mod\ntf.math.multiply\ntf.math.multiply_no_nan\ntf.math.ndtri\ntf.math.negative\ntf.math.nextafter\ntf.math.not_equal\ntf.math.polygamma\ntf.math.polyval\ntf.math.pow\ntf.math.real\ntf.math.reciprocal\ntf.math.reciprocal_no_nan\ntf.math.reduce_all\ntf.math.reduce_any\ntf.math.reduce_euclidean_norm\ntf.math.reduce_logsumexp\ntf.math.reduce_max\ntf.math.reduce_mean\ntf.math.reduce_min\ntf.math.reduce_prod\ntf.math.reduce_std\ntf.math.reduce_sum\ntf.math.reduce_variance\ntf.math.rint\ntf.math.round\ntf.math.rsqrt\ntf.math.scalar_mul\ntf.math.segment_max\ntf.math.segment_mean\ntf.math.segment_min\ntf.math.segment_prod\ntf.math.segment_sum\ntf.math.sigmoid\ntf.math.sign\ntf.math.sin\ntf.math.sinh\ntf.math.sobol_sample\ntf.math.softmax\ntf.math.softplus\ntf.math.softsign\ntf.math.special\ntf.math.special.bessel_i0\ntf.math.special.bessel_i0e\ntf.math.special.bessel_i1\ntf.math.special.bessel_i1e\ntf.math.special.bessel_j0\ntf.math.special.bessel_j1\ntf.math.special.bessel_k0\ntf.math.special.bessel_k0e\ntf.math.special.bessel_k1\ntf.math.special.bessel_k1e\ntf.math.special.bessel_y0\ntf.math.special.bessel_y1\ntf.math.special.dawsn\ntf.math.special.expint\ntf.math.special.fresnel_cos\ntf.math.special.fresnel_sin\ntf.math.special.spence\ntf.math.sqrt\ntf.math.square\ntf.math.squared_difference\ntf.math.subtract\ntf.math.tan\ntf.math.tanh\ntf.math.top_k\ntf.math.truediv\ntf.math.unsorted_segment_max\ntf.math.unsorted_segment_mean\ntf.math.unsorted_segment_min\ntf.math.unsorted_segment_prod\ntf.math.unsorted_segment_sqrt_n\ntf.math.unsorted_segment_sum\ntf.math.xdivy\ntf.math.xlog1py\ntf.math.xlogy\ntf.math.zero_fraction\ntf.math.zeta\ntf.matmul\ntf.matrix_square_root\ntf.maximum\ntf.meshgrid\ntf.minimum\ntf.mlir\ntf.mlir.experimental\ntf.mlir.experimental.convert_function\ntf.mlir.experimental.convert_graph_def\ntf.mlir.experimental.convert_saved_model\ntf.mlir.experimental.convert_saved_model_v1\ntf.mlir.experimental.run_pass_pipeline\ntf.mlir.experimental.tflite_to_tosa_bytecode\ntf.mlir.experimental.write_bytecode\ntf.multiply\ntf.name_scope\ntf.negative\ntf.nest\ntf.nest.assert_same_structure\ntf.nest.flatten\ntf.nest.is_nested\ntf.nest.map_structure\ntf.nest.pack_sequence_as\ntf.nn\ntf.nn.RNNCellDeviceWrapper\ntf.nn.RNNCellDropoutWrapper\ntf.nn.RNNCellResidualWrapper\ntf.nn.all_candidate_sampler\ntf.nn.approx_max_k\ntf.nn.approx_min_k\ntf.nn.atrous_conv2d\ntf.nn.atrous_conv2d_transpose\ntf.nn.avg_pool\ntf.nn.avg_pool1d\ntf.nn.avg_pool2d\ntf.nn.avg_pool3d\ntf.nn.batch_norm_with_global_normalization\ntf.nn.batch_normalization\ntf.nn.bias_add\ntf.nn.collapse_repeated\ntf.nn.compute_accidental_hits\ntf.nn.compute_average_loss\ntf.nn.conv1d\ntf.nn.conv1d_transpose\ntf.nn.conv2d\ntf.nn.conv2d_transpose\ntf.nn.conv3d\ntf.nn.conv3d_transpose\ntf.nn.conv_transpose\ntf.nn.convolution\ntf.nn.crelu\ntf.nn.ctc_beam_search_decoder\ntf.nn.ctc_greedy_decoder\ntf.nn.ctc_loss\ntf.nn.ctc_unique_labels\ntf.nn.depth_to_space\ntf.nn.depthwise_conv2d\ntf.nn.depthwise_conv2d_backprop_filter\ntf.nn.depthwise_conv2d_backprop_input\ntf.nn.dilation2d\ntf.nn.dropout\ntf.nn.elu\ntf.nn.embedding_lookup\ntf.nn.embedding_lookup_sparse\ntf.nn.erosion2d\ntf.nn.experimental\ntf.nn.experimental.general_dropout\ntf.nn.experimental.stateless_dropout\ntf.nn.fixed_unigram_candidate_sampler\ntf.nn.fractional_avg_pool\ntf.nn.fractional_max_pool\ntf.nn.gelu\ntf.nn.in_top_k\ntf.nn.isotonic_regression\ntf.nn.l2_loss\ntf.nn.l2_normalize\ntf.nn.leaky_relu\ntf.nn.learned_unigram_candidate_sampler\ntf.nn.local_response_normalization\ntf.nn.log_poisson_loss\ntf.nn.log_softmax\ntf.nn.lrn\ntf.nn.max_pool\ntf.nn.max_pool1d\ntf.nn.max_pool2d\ntf.nn.max_pool3d\ntf.nn.max_pool_with_argmax\ntf.nn.moments\ntf.nn.nce_loss\ntf.nn.normalize_moments\ntf.nn.pool\ntf.nn.relu\ntf.nn.relu6\ntf.nn.safe_embedding_lookup_sparse\ntf.nn.sampled_softmax_loss\ntf.nn.scale_regularization_loss\ntf.nn.selu\ntf.nn.separable_conv2d\ntf.nn.sigmoid\ntf.nn.sigmoid_cross_entropy_with_logits\ntf.nn.silu\ntf.nn.softmax\ntf.nn.softmax_cross_entropy_with_logits\ntf.nn.softplus\ntf.nn.softsign\ntf.nn.space_to_batch\ntf.nn.space_to_depth\ntf.nn.sparse_softmax_cross_entropy_with_logits\ntf.nn.sufficient_statistics\ntf.nn.swish\ntf.nn.tanh\ntf.nn.top_k\ntf.nn.weighted_cross_entropy_with_logits\ntf.nn.weighted_moments\ntf.nn.with_space_to_batch\ntf.nn.zero_fraction\ntf.no_gradient\ntf.no_op\ntf.nondifferentiable_batch_function\ntf.norm\ntf.not_equal\ntf.numpy_function\ntf.one_hot\ntf.ones\ntf.ones_initializer\ntf.ones_like\ntf.pad\ntf.parallel_stack\ntf.pow\ntf.print\ntf.profiler\ntf.profiler.experimental\ntf.profiler.experimental.Profile\ntf.profiler.experimental.ProfilerOptions\ntf.profiler.experimental.Trace\ntf.profiler.experimental.client\ntf.profiler.experimental.client.monitor\ntf.profiler.experimental.client.trace\ntf.profiler.experimental.server\ntf.profiler.experimental.server.start\ntf.profiler.experimental.start\ntf.profiler.experimental.stop\ntf.py_function\ntf.quantization\ntf.quantization.dequantize\ntf.quantization.experimental\ntf.quantization.experimental.QuantizationComponentSpec\ntf.quantization.experimental.QuantizationMethod\ntf.quantization.experimental.QuantizationOptions\ntf.quantization.experimental.QuantizationOptions.RepresentativeDatasetsEntry\ntf.quantization.experimental.TfRecordRepresentativeDatasetSaver\ntf.quantization.experimental.UnitWiseQuantizationSpec\ntf.quantization.experimental.UnitWiseQuantizationSpec.QuantizationUnit\ntf.quantization.experimental.quantize_saved_model\ntf.quantization.fake_quant_with_min_max_args\ntf.quantization.fake_quant_with_min_max_args_gradient\ntf.quantization.fake_quant_with_min_max_vars\ntf.quantization.fake_quant_with_min_max_vars_gradient\ntf.quantization.fake_quant_with_min_max_vars_per_channel\ntf.quantization.fake_quant_with_min_max_vars_per_channel_gradient\ntf.quantization.quantize\ntf.quantization.quantize_and_dequantize\ntf.quantization.quantize_and_dequantize_v2\ntf.quantization.quantized_concat\ntf.queue\ntf.queue.FIFOQueue\ntf.queue.PaddingFIFOQueue\ntf.queue.PriorityQueue\ntf.queue.QueueBase\ntf.queue.RandomShuffleQueue\ntf.ragged\ntf.ragged.boolean_mask\ntf.ragged.constant\ntf.ragged.cross\ntf.ragged.cross_hashed\ntf.ragged.map_flat_values\ntf.ragged.range\ntf.ragged.row_splits_to_segment_ids\ntf.ragged.segment_ids_to_row_splits\ntf.ragged.stack\ntf.ragged.stack_dynamic_partitions\ntf.ragged_fill_empty_rows\ntf.ragged_fill_empty_rows_grad\ntf.random\ntf.random.Algorithm\ntf.random.Generator\ntf.random.all_candidate_sampler\ntf.random.categorical\ntf.random.create_rng_state\ntf.random.experimental\ntf.random.experimental.Algorithm\ntf.random.experimental.Generator\ntf.random.experimental.create_rng_state\ntf.random.experimental.get_global_generator\ntf.random.experimental.index_shuffle\ntf.random.experimental.set_global_generator\ntf.random.experimental.stateless_fold_in\ntf.random.experimental.stateless_shuffle\ntf.random.experimental.stateless_split\ntf.random.fixed_unigram_candidate_sampler\ntf.random.fold_in\ntf.random.gamma\ntf.random.get_global_generator\ntf.random.learned_unigram_candidate_sampler\ntf.random.log_uniform_candidate_sampler\ntf.random.normal\ntf.random.poisson\ntf.random.set_global_generator\ntf.random.set_seed\ntf.random.shuffle\ntf.random.split\ntf.random.stateless_binomial\ntf.random.stateless_categorical\ntf.random.stateless_gamma\ntf.random.stateless_normal\ntf.random.stateless_parameterized_truncated_normal\ntf.random.stateless_poisson\ntf.random.stateless_truncated_normal\ntf.random.stateless_uniform\ntf.random.truncated_normal\ntf.random.uniform\ntf.random.uniform_candidate_sampler\ntf.random_index_shuffle\ntf.random_normal_initializer\ntf.random_uniform_initializer\ntf.range\ntf.rank\ntf.raw_ops\ntf.raw_ops.Abort\ntf.raw_ops.Abs\ntf.raw_ops.AccumulateNV2\ntf.raw_ops.AccumulatorApplyGradient\ntf.raw_ops.AccumulatorNumAccumulated\ntf.raw_ops.AccumulatorSetGlobalStep\ntf.raw_ops.AccumulatorTakeGradient\ntf.raw_ops.Acos\ntf.raw_ops.Acosh\ntf.raw_ops.Add\ntf.raw_ops.AddManySparseToTensorsMap\ntf.raw_ops.AddN\ntf.raw_ops.AddSparseToTensorsMap\ntf.raw_ops.AddV2\ntf.raw_ops.AdjustContrast\ntf.raw_ops.AdjustContrastv2\ntf.raw_ops.AdjustHue\ntf.raw_ops.AdjustSaturation\ntf.raw_ops.All\ntf.raw_ops.AllCandidateSampler\ntf.raw_ops.AllToAll\ntf.raw_ops.Angle\ntf.raw_ops.AnonymousHashTable\ntf.raw_ops.AnonymousIterator\ntf.raw_ops.AnonymousIteratorV2\ntf.raw_ops.AnonymousIteratorV3\ntf.raw_ops.AnonymousMemoryCache\ntf.raw_ops.AnonymousMultiDeviceIterator\ntf.raw_ops.AnonymousMultiDeviceIteratorV3\ntf.raw_ops.AnonymousMutableDenseHashTable\ntf.raw_ops.AnonymousMutableHashTable\ntf.raw_ops.AnonymousMutableHashTableOfTensors\ntf.raw_ops.AnonymousRandomSeedGenerator\ntf.raw_ops.AnonymousSeedGenerator\ntf.raw_ops.Any\ntf.raw_ops.ApplyAdaMax\ntf.raw_ops.ApplyAdadelta\ntf.raw_ops.ApplyAdagrad\ntf.raw_ops.ApplyAdagradDA\ntf.raw_ops.ApplyAdagradV2\ntf.raw_ops.ApplyAdam\ntf.raw_ops.ApplyAddSign\ntf.raw_ops.ApplyCenteredRMSProp\ntf.raw_ops.ApplyFtrl\ntf.raw_ops.ApplyFtrlV2\ntf.raw_ops.ApplyGradientDescent\ntf.raw_ops.ApplyMomentum\ntf.raw_ops.ApplyPowerSign\ntf.raw_ops.ApplyProximalAdagrad\ntf.raw_ops.ApplyProximalGradientDescent\ntf.raw_ops.ApplyRMSProp\ntf.raw_ops.ApproxTopK\ntf.raw_ops.ApproximateEqual\ntf.raw_ops.ArgMax\ntf.raw_ops.ArgMin\ntf.raw_ops.AsString\ntf.raw_ops.Asin\ntf.raw_ops.Asinh\ntf.raw_ops.Assert\ntf.raw_ops.AssertCardinalityDataset\ntf.raw_ops.AssertNextDataset\ntf.raw_ops.AssertPrevDataset\ntf.raw_ops.Assign\ntf.raw_ops.AssignAdd\ntf.raw_ops.AssignAddVariableOp\ntf.raw_ops.AssignSub\ntf.raw_ops.AssignSubVariableOp\ntf.raw_ops.AssignVariableOp\ntf.raw_ops.AssignVariableXlaConcatND\ntf.raw_ops.Atan\ntf.raw_ops.Atan2\ntf.raw_ops.Atanh\ntf.raw_ops.AudioSpectrogram\ntf.raw_ops.AudioSummary\ntf.raw_ops.AudioSummaryV2\ntf.raw_ops.AutoShardDataset\ntf.raw_ops.AvgPool\ntf.raw_ops.AvgPool3D\ntf.raw_ops.AvgPool3DGrad\ntf.raw_ops.AvgPoolGrad\ntf.raw_ops.BandedTriangularSolve\ntf.raw_ops.Barrier\ntf.raw_ops.BarrierClose\ntf.raw_ops.BarrierIncompleteSize\ntf.raw_ops.BarrierInsertMany\ntf.raw_ops.BarrierReadySize\ntf.raw_ops.BarrierTakeMany\ntf.raw_ops.Batch\ntf.raw_ops.BatchCholesky\ntf.raw_ops.BatchCholeskyGrad\ntf.raw_ops.BatchDataset\ntf.raw_ops.BatchDatasetV2\ntf.raw_ops.BatchFFT\ntf.raw_ops.BatchFFT2D\ntf.raw_ops.BatchFFT3D\ntf.raw_ops.BatchFunction\ntf.raw_ops.BatchIFFT\ntf.raw_ops.BatchIFFT2D\ntf.raw_ops.BatchIFFT3D\ntf.raw_ops.BatchMatMul\ntf.raw_ops.BatchMatMulV2\ntf.raw_ops.BatchMatMulV3\ntf.raw_ops.BatchMatrixBandPart\ntf.raw_ops.BatchMatrixDeterminant\ntf.raw_ops.BatchMatrixDiag\ntf.raw_ops.BatchMatrixDiagPart\ntf.raw_ops.BatchMatrixInverse\ntf.raw_ops.BatchMatrixSetDiag\ntf.raw_ops.BatchMatrixSolve\ntf.raw_ops.BatchMatrixSolveLs\ntf.raw_ops.BatchMatrixTriangularSolve\ntf.raw_ops.BatchNormWithGlobalNormalization\ntf.raw_ops.BatchNormWithGlobalNormalizationGrad\ntf.raw_ops.BatchSelfAdjointEig\ntf.raw_ops.BatchSelfAdjointEigV2\ntf.raw_ops.BatchSvd\ntf.raw_ops.BatchToSpace\ntf.raw_ops.BatchToSpaceND\ntf.raw_ops.BesselI0\ntf.raw_ops.BesselI0e\ntf.raw_ops.BesselI1\ntf.raw_ops.BesselI1e\ntf.raw_ops.BesselJ0\ntf.raw_ops.BesselJ1\ntf.raw_ops.BesselK0\ntf.raw_ops.BesselK0e\ntf.raw_ops.BesselK1\ntf.raw_ops.BesselK1e\ntf.raw_ops.BesselY0\ntf.raw_ops.BesselY1\ntf.raw_ops.Betainc\ntf.raw_ops.BiasAdd\ntf.raw_ops.BiasAddGrad\ntf.raw_ops.BiasAddV1\ntf.raw_ops.Bincount\ntf.raw_ops.Bitcast\ntf.raw_ops.BitwiseAnd\ntf.raw_ops.BitwiseOr\ntf.raw_ops.BitwiseXor\ntf.raw_ops.BlockLSTM\ntf.raw_ops.BlockLSTMGrad\ntf.raw_ops.BlockLSTMGradV2\ntf.raw_ops.BlockLSTMV2\ntf.raw_ops.BoostedTreesAggregateStats\ntf.raw_ops.BoostedTreesBucketize\ntf.raw_ops.BoostedTreesCalculateBestFeatureSplit\ntf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2\ntf.raw_ops.BoostedTreesCalculateBestGainsPerFeature\ntf.raw_ops.BoostedTreesCenterBias\ntf.raw_ops.BoostedTreesCreateEnsemble\ntf.raw_ops.BoostedTreesCreateQuantileStreamResource\ntf.raw_ops.BoostedTreesDeserializeEnsemble\ntf.raw_ops.BoostedTreesEnsembleResourceHandleOp\ntf.raw_ops.BoostedTreesExampleDebugOutputs\ntf.raw_ops.BoostedTreesFlushQuantileSummaries\ntf.raw_ops.BoostedTreesGetEnsembleStates\ntf.raw_ops.BoostedTreesMakeQuantileSummaries\ntf.raw_ops.BoostedTreesMakeStatsSummary\ntf.raw_ops.BoostedTreesPredict\ntf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries\ntf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize\ntf.raw_ops.BoostedTreesQuantileStreamResourceFlush\ntf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries\ntf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp\ntf.raw_ops.BoostedTreesSerializeEnsemble\ntf.raw_ops.BoostedTreesSparseAggregateStats\ntf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit\ntf.raw_ops.BoostedTreesTrainingPredict\ntf.raw_ops.BoostedTreesUpdateEnsemble\ntf.raw_ops.BoostedTreesUpdateEnsembleV2\ntf.raw_ops.BroadcastArgs\ntf.raw_ops.BroadcastGradientArgs\ntf.raw_ops.BroadcastTo\ntf.raw_ops.Bucketize\ntf.raw_ops.BytesProducedStatsDataset\ntf.raw_ops.CSRSparseMatrixComponents\ntf.raw_ops.CSRSparseMatrixToDense\ntf.raw_ops.CSRSparseMatrixToSparseTensor\ntf.raw_ops.CSVDataset\ntf.raw_ops.CSVDatasetV2\ntf.raw_ops.CTCBeamSearchDecoder\ntf.raw_ops.CTCGreedyDecoder\ntf.raw_ops.CTCLoss\ntf.raw_ops.CTCLossV2\ntf.raw_ops.CacheDataset\ntf.raw_ops.CacheDatasetV2\ntf.raw_ops.Case\ntf.raw_ops.Cast\ntf.raw_ops.Ceil\ntf.raw_ops.CheckNumerics\ntf.raw_ops.CheckNumericsV2\ntf.raw_ops.Cholesky\ntf.raw_ops.CholeskyGrad\ntf.raw_ops.ChooseFastestBranchDataset\ntf.raw_ops.ChooseFastestDataset\ntf.raw_ops.ClipByValue\ntf.raw_ops.CloseSummaryWriter\ntf.raw_ops.CollectiveAllToAllV2\ntf.raw_ops.CollectiveAllToAllV3\ntf.raw_ops.CollectiveAssignGroupV2\ntf.raw_ops.CollectiveBcastRecv\ntf.raw_ops.CollectiveBcastRecvV2\ntf.raw_ops.CollectiveBcastSend\ntf.raw_ops.CollectiveBcastSendV2\ntf.raw_ops.CollectiveGather\ntf.raw_ops.CollectiveGatherV2\ntf.raw_ops.CollectiveInitializeCommunicator\ntf.raw_ops.CollectivePermute\ntf.raw_ops.CollectiveReduce\ntf.raw_ops.CollectiveReduceScatterV2\ntf.raw_ops.CollectiveReduceV2\ntf.raw_ops.CollectiveReduceV3\ntf.raw_ops.CombinedNonMaxSuppression\ntf.raw_ops.Complex\ntf.raw_ops.ComplexAbs\ntf.raw_ops.CompositeTensorVariantFromComponents\ntf.raw_ops.CompositeTensorVariantToComponents\ntf.raw_ops.CompressElement\ntf.raw_ops.ComputeAccidentalHits\ntf.raw_ops.ComputeBatchSize\ntf.raw_ops.Concat\ntf.raw_ops.ConcatOffset\ntf.raw_ops.ConcatV2\ntf.raw_ops.ConcatenateDataset\ntf.raw_ops.ConditionalAccumulator\ntf.raw_ops.ConfigureDistributedTPU\ntf.raw_ops.ConfigureTPUEmbedding\ntf.raw_ops.Conj\ntf.raw_ops.ConjugateTranspose\ntf.raw_ops.Const\ntf.raw_ops.ConsumeMutexLock\ntf.raw_ops.ControlTrigger\ntf.raw_ops.Conv\ntf.raw_ops.Conv2D\ntf.raw_ops.Conv2DBackpropFilter\ntf.raw_ops.Conv2DBackpropFilterV2\ntf.raw_ops.Conv2DBackpropInput\ntf.raw_ops.Conv2DBackpropInputV2\ntf.raw_ops.Conv3D\ntf.raw_ops.Conv3DBackpropFilter\ntf.raw_ops.Conv3DBackpropFilterV2\ntf.raw_ops.Conv3DBackpropInput\ntf.raw_ops.Conv3DBackpropInputV2\ntf.raw_ops.ConvertToCooTensor\ntf.raw_ops.Copy\ntf.raw_ops.CopyHost\ntf.raw_ops.Cos\ntf.raw_ops.Cosh\ntf.raw_ops.CountUpTo\ntf.raw_ops.CreateSummaryDbWriter\ntf.raw_ops.CreateSummaryFileWriter\ntf.raw_ops.CropAndResize\ntf.raw_ops.CropAndResizeGradBoxes\ntf.raw_ops.CropAndResizeGradImage\ntf.raw_ops.Cross\ntf.raw_ops.CrossReplicaSum\ntf.raw_ops.CudnnRNN\ntf.raw_ops.CudnnRNNBackprop\ntf.raw_ops.CudnnRNNBackpropV2\ntf.raw_ops.CudnnRNNBackpropV3\ntf.raw_ops.CudnnRNNCanonicalToParams\ntf.raw_ops.CudnnRNNCanonicalToParamsV2\ntf.raw_ops.CudnnRNNParamsSize\ntf.raw_ops.CudnnRNNParamsToCanonical\ntf.raw_ops.CudnnRNNParamsToCanonicalV2\ntf.raw_ops.CudnnRNNV2\ntf.raw_ops.CudnnRNNV3\ntf.raw_ops.Cumprod\ntf.raw_ops.Cumsum\ntf.raw_ops.CumulativeLogsumexp\ntf.raw_ops.DataFormatDimMap\ntf.raw_ops.DataFormatVecPermute\ntf.raw_ops.DataServiceDataset\ntf.raw_ops.DataServiceDatasetV2\ntf.raw_ops.DataServiceDatasetV3\ntf.raw_ops.DataServiceDatasetV4\ntf.raw_ops.DatasetCardinality\ntf.raw_ops.DatasetFingerprint\ntf.raw_ops.DatasetFromGraph\ntf.raw_ops.DatasetToGraph\ntf.raw_ops.DatasetToGraphV2\ntf.raw_ops.DatasetToSingleElement\ntf.raw_ops.DatasetToTFRecord\ntf.raw_ops.Dawsn\ntf.raw_ops.DebugGradientIdentity\ntf.raw_ops.DebugGradientRefIdentity\ntf.raw_ops.DebugIdentity\ntf.raw_ops.DebugIdentityV2\ntf.raw_ops.DebugIdentityV3\ntf.raw_ops.DebugNanCount\ntf.raw_ops.DebugNumericSummary\ntf.raw_ops.DebugNumericSummaryV2\ntf.raw_ops.DecodeAndCropJpeg\ntf.raw_ops.DecodeBase64\ntf.raw_ops.DecodeBmp\ntf.raw_ops.DecodeCSV\ntf.raw_ops.DecodeCompressed\ntf.raw_ops.DecodeGif\ntf.raw_ops.DecodeImage\ntf.raw_ops.DecodeJSONExample\ntf.raw_ops.DecodeJpeg\ntf.raw_ops.DecodePaddedRaw\ntf.raw_ops.DecodePng\ntf.raw_ops.DecodeProtoV2\ntf.raw_ops.DecodeRaw\ntf.raw_ops.DecodeWav\ntf.raw_ops.DeepCopy\ntf.raw_ops.DeleteIterator\ntf.raw_ops.DeleteMemoryCache\ntf.raw_ops.DeleteMultiDeviceIterator\ntf.raw_ops.DeleteRandomSeedGenerator\ntf.raw_ops.DeleteSeedGenerator\ntf.raw_ops.DeleteSessionTensor\ntf.raw_ops.DenseBincount\ntf.raw_ops.DenseCountSparseOutput\ntf.raw_ops.DenseToCSRSparseMatrix\ntf.raw_ops.DenseToDenseSetOperation\ntf.raw_ops.DenseToSparseBatchDataset\ntf.raw_ops.DenseToSparseSetOperation\ntf.raw_ops.DepthToSpace\ntf.raw_ops.DepthwiseConv2dNative\ntf.raw_ops.DepthwiseConv2dNativeBackpropFilter\ntf.raw_ops.DepthwiseConv2dNativeBackpropInput\ntf.raw_ops.Dequantize\ntf.raw_ops.DeserializeIterator\ntf.raw_ops.DeserializeManySparse\ntf.raw_ops.DeserializeSparse\ntf.raw_ops.DestroyResourceOp\ntf.raw_ops.DestroyTemporaryVariable\ntf.raw_ops.DeviceIndex\ntf.raw_ops.Diag\ntf.raw_ops.DiagPart\ntf.raw_ops.Digamma\ntf.raw_ops.Dilation2D\ntf.raw_ops.Dilation2DBackpropFilter\ntf.raw_ops.Dilation2DBackpropInput\ntf.raw_ops.DirectedInterleaveDataset\ntf.raw_ops.DisableCopyOnRead\ntf.raw_ops.DistributedSave\ntf.raw_ops.Div\ntf.raw_ops.DivNoNan\ntf.raw_ops.DrawBoundingBoxes\ntf.raw_ops.DrawBoundingBoxesV2\ntf.raw_ops.DummyIterationCounter\ntf.raw_ops.DummyMemoryCache\ntf.raw_ops.DummySeedGenerator\ntf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch\ntf.raw_ops.DynamicEnqueueTPUEmbeddingRaggedTensorBatch\ntf.raw_ops.DynamicPartition\ntf.raw_ops.DynamicStitch\ntf.raw_ops.EagerPyFunc\ntf.raw_ops.EditDistance\ntf.raw_ops.Eig\ntf.raw_ops.Einsum\ntf.raw_ops.Elu\ntf.raw_ops.EluGrad\ntf.raw_ops.Empty\ntf.raw_ops.EmptyTensorList\ntf.raw_ops.EmptyTensorMap\ntf.raw_ops.EncodeBase64\ntf.raw_ops.EncodeJpeg\ntf.raw_ops.EncodeJpegVariableQuality\ntf.raw_ops.EncodePng\ntf.raw_ops.EncodeProto\ntf.raw_ops.EncodeWav\ntf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch\ntf.raw_ops.EnqueueTPUEmbeddingIntegerBatch\ntf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch\ntf.raw_ops.EnqueueTPUEmbeddingSparseBatch\ntf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch\ntf.raw_ops.EnsureShape\ntf.raw_ops.Enter\ntf.raw_ops.Equal\ntf.raw_ops.Erf\ntf.raw_ops.Erfc\ntf.raw_ops.Erfinv\ntf.raw_ops.EuclideanNorm\ntf.raw_ops.Exit\ntf.raw_ops.Exp\ntf.raw_ops.ExpandDims\ntf.raw_ops.ExperimentalAssertNextDataset\ntf.raw_ops.ExperimentalAutoShardDataset\ntf.raw_ops.ExperimentalBytesProducedStatsDataset\ntf.raw_ops.ExperimentalCSVDataset\ntf.raw_ops.ExperimentalChooseFastestDataset\ntf.raw_ops.ExperimentalDatasetCardinality\ntf.raw_ops.ExperimentalDatasetToTFRecord\ntf.raw_ops.ExperimentalDenseToSparseBatchDataset\ntf.raw_ops.ExperimentalDirectedInterleaveDataset\ntf.raw_ops.ExperimentalGroupByReducerDataset\ntf.raw_ops.ExperimentalGroupByWindowDataset\ntf.raw_ops.ExperimentalIgnoreErrorsDataset\ntf.raw_ops.ExperimentalIteratorGetDevice\ntf.raw_ops.ExperimentalLMDBDataset\ntf.raw_ops.ExperimentalLatencyStatsDataset\ntf.raw_ops.ExperimentalMapAndBatchDataset\ntf.raw_ops.ExperimentalMapDataset\ntf.raw_ops.ExperimentalMatchingFilesDataset\ntf.raw_ops.ExperimentalMaxIntraOpParallelismDataset\ntf.raw_ops.ExperimentalNonSerializableDataset\ntf.raw_ops.ExperimentalParallelInterleaveDataset\ntf.raw_ops.ExperimentalParseExampleDataset\ntf.raw_ops.ExperimentalPrivateThreadPoolDataset\ntf.raw_ops.ExperimentalRandomDataset\ntf.raw_ops.ExperimentalRebatchDataset\ntf.raw_ops.ExperimentalScanDataset\ntf.raw_ops.ExperimentalSetStatsAggregatorDataset\ntf.raw_ops.ExperimentalSleepDataset\ntf.raw_ops.ExperimentalSlidingWindowDataset\ntf.raw_ops.ExperimentalSqlDataset\ntf.raw_ops.ExperimentalStatsAggregatorHandle\ntf.raw_ops.ExperimentalStatsAggregatorSummary\ntf.raw_ops.ExperimentalTakeWhileDataset\ntf.raw_ops.ExperimentalThreadPoolDataset\ntf.raw_ops.ExperimentalThreadPoolHandle\ntf.raw_ops.ExperimentalUnbatchDataset\ntf.raw_ops.ExperimentalUniqueDataset\ntf.raw_ops.Expint\ntf.raw_ops.Expm1\ntf.raw_ops.ExtractGlimpse\ntf.raw_ops.ExtractGlimpseV2\ntf.raw_ops.ExtractImagePatches\ntf.raw_ops.ExtractJpegShape\ntf.raw_ops.ExtractVolumePatches\ntf.raw_ops.FFT\ntf.raw_ops.FFT2D\ntf.raw_ops.FFT3D\ntf.raw_ops.FFTND\ntf.raw_ops.FIFOQueue\ntf.raw_ops.FIFOQueueV2\ntf.raw_ops.Fact\ntf.raw_ops.FakeParam\ntf.raw_ops.FakeQuantWithMinMaxArgs\ntf.raw_ops.FakeQuantWithMinMaxArgsGradient\ntf.raw_ops.FakeQuantWithMinMaxVars\ntf.raw_ops.FakeQuantWithMinMaxVarsGradient\ntf.raw_ops.FakeQuantWithMinMaxVarsPerChannel\ntf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient\ntf.raw_ops.FakeQueue\ntf.raw_ops.FileSystemSetConfiguration\ntf.raw_ops.Fill\ntf.raw_ops.FilterByLastComponentDataset\ntf.raw_ops.FilterDataset\ntf.raw_ops.FinalizeDataset\ntf.raw_ops.Fingerprint\ntf.raw_ops.FixedLengthRecordDataset\ntf.raw_ops.FixedLengthRecordDatasetV2\ntf.raw_ops.FixedLengthRecordReader\ntf.raw_ops.FixedLengthRecordReaderV2\ntf.raw_ops.FixedUnigramCandidateSampler\ntf.raw_ops.FlatMapDataset\ntf.raw_ops.Floor\ntf.raw_ops.FloorDiv\ntf.raw_ops.FloorMod\ntf.raw_ops.FlushSummaryWriter\ntf.raw_ops.For\ntf.raw_ops.FractionalAvgPool\ntf.raw_ops.FractionalAvgPoolGrad\ntf.raw_ops.FractionalMaxPool\ntf.raw_ops.FractionalMaxPoolGrad\ntf.raw_ops.FresnelCos\ntf.raw_ops.FresnelSin\ntf.raw_ops.FusedBatchNorm\ntf.raw_ops.FusedBatchNormGrad\ntf.raw_ops.FusedBatchNormGradV2\ntf.raw_ops.FusedBatchNormGradV3\ntf.raw_ops.FusedBatchNormV2\ntf.raw_ops.FusedBatchNormV3\ntf.raw_ops.FusedPadConv2D\ntf.raw_ops.FusedResizeAndPadConv2D\ntf.raw_ops.GRUBlockCell\ntf.raw_ops.GRUBlockCellGrad\ntf.raw_ops.Gather\ntf.raw_ops.GatherNd\ntf.raw_ops.GatherV2\ntf.raw_ops.GenerateBoundingBoxProposals\ntf.raw_ops.GenerateVocabRemapping\ntf.raw_ops.GeneratorDataset\ntf.raw_ops.GetElementAtIndex\ntf.raw_ops.GetMinibatchSplitsWithPhysicalReplica\ntf.raw_ops.GetMinibatchesInCsrWithPhysicalReplica\ntf.raw_ops.GetOptions\ntf.raw_ops.GetSessionHandle\ntf.raw_ops.GetSessionHandleV2\ntf.raw_ops.GetSessionTensor\ntf.raw_ops.GlobalIterId\ntf.raw_ops.Greater\ntf.raw_ops.GreaterEqual\ntf.raw_ops.GroupByReducerDataset\ntf.raw_ops.GroupByWindowDataset\ntf.raw_ops.GuaranteeConst\ntf.raw_ops.HSVToRGB\ntf.raw_ops.HashTable\ntf.raw_ops.HashTableV2\ntf.raw_ops.HistogramFixedWidth\ntf.raw_ops.HistogramSummary\ntf.raw_ops.IFFT\ntf.raw_ops.IFFT2D\ntf.raw_ops.IFFT3D\ntf.raw_ops.IFFTND\ntf.raw_ops.IRFFT\ntf.raw_ops.IRFFT2D\ntf.raw_ops.IRFFT3D\ntf.raw_ops.IRFFTND\ntf.raw_ops.Identity\ntf.raw_ops.IdentityN\ntf.raw_ops.IdentityReader\ntf.raw_ops.IdentityReaderV2\ntf.raw_ops.If\ntf.raw_ops.Igamma\ntf.raw_ops.IgammaGradA\ntf.raw_ops.Igammac\ntf.raw_ops.IgnoreErrorsDataset\ntf.raw_ops.Imag\ntf.raw_ops.ImageProjectiveTransformV2\ntf.raw_ops.ImageProjectiveTransformV3\ntf.raw_ops.ImageSummary\ntf.raw_ops.ImmutableConst\ntf.raw_ops.ImportEvent\ntf.raw_ops.InTopK\ntf.raw_ops.InTopKV2\ntf.raw_ops.InfeedDequeue\ntf.raw_ops.InfeedDequeueTuple\ntf.raw_ops.InfeedEnqueue\ntf.raw_ops.InfeedEnqueuePrelinearizedBuffer\ntf.raw_ops.InfeedEnqueueTuple\ntf.raw_ops.InitializeTable\ntf.raw_ops.InitializeTableFromDataset\ntf.raw_ops.InitializeTableFromTextFile\ntf.raw_ops.InitializeTableFromTextFileV2\ntf.raw_ops.InitializeTableV2\ntf.raw_ops.InplaceAdd\ntf.raw_ops.InplaceSub\ntf.raw_ops.InplaceUpdate\ntf.raw_ops.InterleaveDataset\ntf.raw_ops.Inv\ntf.raw_ops.InvGrad\ntf.raw_ops.Invert\ntf.raw_ops.InvertPermutation\ntf.raw_ops.IsBoostedTreesEnsembleInitialized\ntf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized\ntf.raw_ops.IsFinite\ntf.raw_ops.IsInf\ntf.raw_ops.IsNan\ntf.raw_ops.IsTPUEmbeddingInitialized\ntf.raw_ops.IsVariableInitialized\ntf.raw_ops.IsotonicRegression\ntf.raw_ops.Iterator\ntf.raw_ops.IteratorFromStringHandle\ntf.raw_ops.IteratorFromStringHandleV2\ntf.raw_ops.IteratorGetDevice\ntf.raw_ops.IteratorGetNext\ntf.raw_ops.IteratorGetNextAsOptional\ntf.raw_ops.IteratorGetNextSync\ntf.raw_ops.IteratorToStringHandle\ntf.raw_ops.IteratorV2\ntf.raw_ops.KMC2ChainInitialization\ntf.raw_ops.KmeansPlusPlusInitialization\ntf.raw_ops.L2Loss\ntf.raw_ops.LMDBDataset\ntf.raw_ops.LMDBReader\ntf.raw_ops.LRN\ntf.raw_ops.LRNGrad\ntf.raw_ops.LSTMBlockCell\ntf.raw_ops.LSTMBlockCellGrad\ntf.raw_ops.LatencyStatsDataset\ntf.raw_ops.LeakyRelu\ntf.raw_ops.LeakyReluGrad\ntf.raw_ops.LearnedUnigramCandidateSampler\ntf.raw_ops.LeftShift\ntf.raw_ops.LegacyParallelInterleaveDatasetV2\ntf.raw_ops.Less\ntf.raw_ops.LessEqual\ntf.raw_ops.Lgamma\ntf.raw_ops.LinSpace\ntf.raw_ops.ListDataset\ntf.raw_ops.ListDiff\ntf.raw_ops.ListSnapshotChunksDataset\ntf.raw_ops.LoadAndRemapMatrix\ntf.raw_ops.LoadDataset\ntf.raw_ops.LoadTPUEmbeddingADAMParameters\ntf.raw_ops.LoadTPUEmbeddingAdadeltaParameters\ntf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters\ntf.raw_ops.LoadTPUEmbeddingAdagradParameters\ntf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters\ntf.raw_ops.LoadTPUEmbeddingFTRLParameters\ntf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters\ntf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters\ntf.raw_ops.LoadTPUEmbeddingMomentumParameters\ntf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters\ntf.raw_ops.LoadTPUEmbeddingProximalYogiParameters\ntf.raw_ops.LoadTPUEmbeddingRMSPropParameters\ntf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters\ntf.raw_ops.Log\ntf.raw_ops.Log1p\ntf.raw_ops.LogMatrixDeterminant\ntf.raw_ops.LogSoftmax\ntf.raw_ops.LogUniformCandidateSampler\ntf.raw_ops.LogicalAnd\ntf.raw_ops.LogicalNot\ntf.raw_ops.LogicalOr\ntf.raw_ops.LookupTableExport\ntf.raw_ops.LookupTableExportV2\ntf.raw_ops.LookupTableFind\ntf.raw_ops.LookupTableFindV2\ntf.raw_ops.LookupTableImport\ntf.raw_ops.LookupTableImportV2\ntf.raw_ops.LookupTableInsert\ntf.raw_ops.LookupTableInsertV2\ntf.raw_ops.LookupTableRemoveV2\ntf.raw_ops.LookupTableSize\ntf.raw_ops.LookupTableSizeV2\ntf.raw_ops.LoopCond\ntf.raw_ops.LowerBound\ntf.raw_ops.Lu\ntf.raw_ops.MakeIterator\ntf.raw_ops.MapAndBatchDataset\ntf.raw_ops.MapClear\ntf.raw_ops.MapDataset\ntf.raw_ops.MapDefun\ntf.raw_ops.MapIncompleteSize\ntf.raw_ops.MapPeek\ntf.raw_ops.MapSize\ntf.raw_ops.MapStage\ntf.raw_ops.MapUnstage\ntf.raw_ops.MapUnstageNoKey\ntf.raw_ops.MatMul\ntf.raw_ops.MatchingFiles\ntf.raw_ops.MatchingFilesDataset\ntf.raw_ops.MatrixBandPart\ntf.raw_ops.MatrixDeterminant\ntf.raw_ops.MatrixDiag\ntf.raw_ops.MatrixDiagPart\ntf.raw_ops.MatrixDiagPartV2\ntf.raw_ops.MatrixDiagPartV3\ntf.raw_ops.MatrixDiagV2\ntf.raw_ops.MatrixDiagV3\ntf.raw_ops.MatrixExponential\ntf.raw_ops.MatrixInverse\ntf.raw_ops.MatrixLogarithm\ntf.raw_ops.MatrixSetDiag\ntf.raw_ops.MatrixSetDiagV2\ntf.raw_ops.MatrixSetDiagV3\ntf.raw_ops.MatrixSolve\ntf.raw_ops.MatrixSolveLs\ntf.raw_ops.MatrixSquareRoot\ntf.raw_ops.MatrixTriangularSolve\ntf.raw_ops.Max\ntf.raw_ops.MaxIntraOpParallelismDataset\ntf.raw_ops.MaxPool\ntf.raw_ops.MaxPool3D\ntf.raw_ops.MaxPool3DGrad\ntf.raw_ops.MaxPool3DGradGrad\ntf.raw_ops.MaxPoolGrad\ntf.raw_ops.MaxPoolGradGrad\ntf.raw_ops.MaxPoolGradGradV2\ntf.raw_ops.MaxPoolGradGradWithArgmax\ntf.raw_ops.MaxPoolGradV2\ntf.raw_ops.MaxPoolGradWithArgmax\ntf.raw_ops.MaxPoolV2\ntf.raw_ops.MaxPoolWithArgmax\ntf.raw_ops.Maximum\ntf.raw_ops.Mean\ntf.raw_ops.Merge\ntf.raw_ops.MergeSummary\ntf.raw_ops.MergeV2Checkpoints\ntf.raw_ops.Mfcc\ntf.raw_ops.Min\ntf.raw_ops.Minimum\ntf.raw_ops.MirrorPad\ntf.raw_ops.MirrorPadGrad\ntf.raw_ops.Mod\ntf.raw_ops.ModelDataset\ntf.raw_ops.Mul\ntf.raw_ops.MulNoNan\ntf.raw_ops.MultiDeviceIterator\ntf.raw_ops.MultiDeviceIteratorFromStringHandle\ntf.raw_ops.MultiDeviceIteratorGetNextFromShard\ntf.raw_ops.MultiDeviceIteratorInit\ntf.raw_ops.MultiDeviceIteratorToStringHandle\ntf.raw_ops.Multinomial\ntf.raw_ops.MutableDenseHashTable\ntf.raw_ops.MutableDenseHashTableV2\ntf.raw_ops.MutableHashTable\ntf.raw_ops.MutableHashTableOfTensors\ntf.raw_ops.MutableHashTableOfTensorsV2\ntf.raw_ops.MutableHashTableV2\ntf.raw_ops.MutexLock\ntf.raw_ops.MutexV2\ntf.raw_ops.NcclAllReduce\ntf.raw_ops.NcclBroadcast\ntf.raw_ops.NcclReduce\ntf.raw_ops.Ndtri\ntf.raw_ops.NearestNeighbors\ntf.raw_ops.Neg\ntf.raw_ops.NextAfter\ntf.raw_ops.NextIteration\ntf.raw_ops.NoOp\ntf.raw_ops.NonDeterministicInts\ntf.raw_ops.NonMaxSuppression\ntf.raw_ops.NonMaxSuppressionV2\ntf.raw_ops.NonMaxSuppressionV3\ntf.raw_ops.NonMaxSuppressionV4\ntf.raw_ops.NonMaxSuppressionV5\ntf.raw_ops.NonMaxSuppressionWithOverlaps\ntf.raw_ops.NonSerializableDataset\ntf.raw_ops.NotEqual\ntf.raw_ops.NthElement\ntf.raw_ops.OneHot\ntf.raw_ops.OneShotIterator\ntf.raw_ops.OnesLike\ntf.raw_ops.OptimizeDataset\ntf.raw_ops.OptimizeDatasetV2\ntf.raw_ops.OptionalFromValue\ntf.raw_ops.OptionalGetValue\ntf.raw_ops.OptionalHasValue\ntf.raw_ops.OptionalNone\ntf.raw_ops.OptionsDataset\ntf.raw_ops.OrderedMapClear\ntf.raw_ops.OrderedMapIncompleteSize\ntf.raw_ops.OrderedMapPeek\ntf.raw_ops.OrderedMapSize\ntf.raw_ops.OrderedMapStage\ntf.raw_ops.OrderedMapUnstage\ntf.raw_ops.OrderedMapUnstageNoKey\ntf.raw_ops.OutfeedDequeue\ntf.raw_ops.OutfeedDequeueTuple\ntf.raw_ops.OutfeedDequeueTupleV2\ntf.raw_ops.OutfeedDequeueV2\ntf.raw_ops.OutfeedEnqueue\ntf.raw_ops.OutfeedEnqueueTuple\ntf.raw_ops.Pack\ntf.raw_ops.Pad\ntf.raw_ops.PadV2\ntf.raw_ops.PaddedBatchDataset\ntf.raw_ops.PaddedBatchDatasetV2\ntf.raw_ops.PaddingFIFOQueue\ntf.raw_ops.PaddingFIFOQueueV2\ntf.raw_ops.ParallelBatchDataset\ntf.raw_ops.ParallelConcat\ntf.raw_ops.ParallelDynamicStitch\ntf.raw_ops.ParallelFilterDataset\ntf.raw_ops.ParallelInterleaveDataset\ntf.raw_ops.ParallelInterleaveDatasetV2\ntf.raw_ops.ParallelInterleaveDatasetV3\ntf.raw_ops.ParallelInterleaveDatasetV4\ntf.raw_ops.ParallelMapDataset\ntf.raw_ops.ParallelMapDatasetV2\ntf.raw_ops.ParameterizedTruncatedNormal\ntf.raw_ops.ParseExample\ntf.raw_ops.ParseExampleDataset\ntf.raw_ops.ParseExampleDatasetV2\ntf.raw_ops.ParseExampleV2\ntf.raw_ops.ParseSequenceExample\ntf.raw_ops.ParseSequenceExampleV2\ntf.raw_ops.ParseSingleExample\ntf.raw_ops.ParseSingleSequenceExample\ntf.raw_ops.ParseTensor\ntf.raw_ops.PartitionedCall\ntf.raw_ops.Placeholder\ntf.raw_ops.PlaceholderV2\ntf.raw_ops.PlaceholderWithDefault\ntf.raw_ops.Polygamma\ntf.raw_ops.PopulationCount\ntf.raw_ops.Pow\ntf.raw_ops.PrefetchDataset\ntf.raw_ops.Prelinearize\ntf.raw_ops.PrelinearizeTuple\ntf.raw_ops.PreventGradient\ntf.raw_ops.Print\ntf.raw_ops.PrintV2\ntf.raw_ops.PriorityQueue\ntf.raw_ops.PriorityQueueV2\ntf.raw_ops.PrivateThreadPoolDataset\ntf.raw_ops.Prod\ntf.raw_ops.PyFunc\ntf.raw_ops.PyFuncStateless\ntf.raw_ops.Qr\ntf.raw_ops.QuantizeAndDequantize\ntf.raw_ops.QuantizeAndDequantizeV2\ntf.raw_ops.QuantizeAndDequantizeV3\ntf.raw_ops.QuantizeAndDequantizeV4\ntf.raw_ops.QuantizeAndDequantizeV4Grad\ntf.raw_ops.QuantizeDownAndShrinkRange\ntf.raw_ops.QuantizeV2\ntf.raw_ops.QuantizedAdd\ntf.raw_ops.QuantizedAvgPool\ntf.raw_ops.QuantizedBatchNormWithGlobalNormalization\ntf.raw_ops.QuantizedBiasAdd\ntf.raw_ops.QuantizedConcat\ntf.raw_ops.QuantizedConv2D\ntf.raw_ops.QuantizedConv2DAndRelu\ntf.raw_ops.QuantizedConv2DAndReluAndRequantize\ntf.raw_ops.QuantizedConv2DAndRequantize\ntf.raw_ops.QuantizedConv2DPerChannel\ntf.raw_ops.QuantizedConv2DWithBias\ntf.raw_ops.QuantizedConv2DWithBiasAndRelu\ntf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize\ntf.raw_ops.QuantizedConv2DWithBiasAndRequantize\ntf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\ntf.raw_ops.QuantizedConv2DWithBiasSumAndRelu\ntf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize\ntf.raw_ops.QuantizedDepthwiseConv2D\ntf.raw_ops.QuantizedDepthwiseConv2DWithBias\ntf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu\ntf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\ntf.raw_ops.QuantizedInstanceNorm\ntf.raw_ops.QuantizedMatMul\ntf.raw_ops.QuantizedMatMulWithBias\ntf.raw_ops.QuantizedMatMulWithBiasAndDequantize\ntf.raw_ops.QuantizedMatMulWithBiasAndRelu\ntf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize\ntf.raw_ops.QuantizedMatMulWithBiasAndRequantize\ntf.raw_ops.QuantizedMaxPool\ntf.raw_ops.QuantizedMul\ntf.raw_ops.QuantizedRelu\ntf.raw_ops.QuantizedRelu6\ntf.raw_ops.QuantizedReluX\ntf.raw_ops.QuantizedReshape\ntf.raw_ops.QuantizedResizeBilinear\ntf.raw_ops.QueueClose\ntf.raw_ops.QueueCloseV2\ntf.raw_ops.QueueDequeue\ntf.raw_ops.QueueDequeueMany\ntf.raw_ops.QueueDequeueManyV2\ntf.raw_ops.QueueDequeueUpTo\ntf.raw_ops.QueueDequeueUpToV2\ntf.raw_ops.QueueDequeueV2\ntf.raw_ops.QueueEnqueue\ntf.raw_ops.QueueEnqueueMany\ntf.raw_ops.QueueEnqueueManyV2\ntf.raw_ops.QueueEnqueueV2\ntf.raw_ops.QueueIsClosed\ntf.raw_ops.QueueIsClosedV2\ntf.raw_ops.QueueSize\ntf.raw_ops.QueueSizeV2\ntf.raw_ops.RFFT\ntf.raw_ops.RFFT2D\ntf.raw_ops.RFFT3D\ntf.raw_ops.RFFTND\ntf.raw_ops.RGBToHSV\ntf.raw_ops.RaggedBincount\ntf.raw_ops.RaggedCountSparseOutput\ntf.raw_ops.RaggedCross\ntf.raw_ops.RaggedFillEmptyRows\ntf.raw_ops.RaggedFillEmptyRowsGrad\ntf.raw_ops.RaggedGather\ntf.raw_ops.RaggedRange\ntf.raw_ops.RaggedTensorFromVariant\ntf.raw_ops.RaggedTensorToSparse\ntf.raw_ops.RaggedTensorToTensor\ntf.raw_ops.RaggedTensorToVariant\ntf.raw_ops.RaggedTensorToVariantGradient\ntf.raw_ops.RandomCrop\ntf.raw_ops.RandomDataset\ntf.raw_ops.RandomDatasetV2\ntf.raw_ops.RandomGamma\ntf.raw_ops.RandomGammaGrad\ntf.raw_ops.RandomIndexShuffle\ntf.raw_ops.RandomPoisson\ntf.raw_ops.RandomPoissonV2\ntf.raw_ops.RandomShuffle\ntf.raw_ops.RandomShuffleQueue\ntf.raw_ops.RandomShuffleQueueV2\ntf.raw_ops.RandomStandardNormal\ntf.raw_ops.RandomUniform\ntf.raw_ops.RandomUniformInt\ntf.raw_ops.Range\ntf.raw_ops.RangeDataset\ntf.raw_ops.Rank\ntf.raw_ops.ReadFile\ntf.raw_ops.ReadVariableOp\ntf.raw_ops.ReadVariableXlaSplitND\ntf.raw_ops.ReaderNumRecordsProduced\ntf.raw_ops.ReaderNumRecordsProducedV2\ntf.raw_ops.ReaderNumWorkUnitsCompleted\ntf.raw_ops.ReaderNumWorkUnitsCompletedV2\ntf.raw_ops.ReaderRead\ntf.raw_ops.ReaderReadUpTo\ntf.raw_ops.ReaderReadUpToV2\ntf.raw_ops.ReaderReadV2\ntf.raw_ops.ReaderReset\ntf.raw_ops.ReaderResetV2\ntf.raw_ops.ReaderRestoreState\ntf.raw_ops.ReaderRestoreStateV2\ntf.raw_ops.ReaderSerializeState\ntf.raw_ops.ReaderSerializeStateV2\ntf.raw_ops.Real\ntf.raw_ops.RealDiv\ntf.raw_ops.RebatchDataset\ntf.raw_ops.RebatchDatasetV2\ntf.raw_ops.Reciprocal\ntf.raw_ops.ReciprocalGrad\ntf.raw_ops.RecordInput\ntf.raw_ops.Recv\ntf.raw_ops.RecvTPUEmbeddingActivations\ntf.raw_ops.ReduceDataset\ntf.raw_ops.ReduceJoin\ntf.raw_ops.RefEnter\ntf.raw_ops.RefExit\ntf.raw_ops.RefIdentity\ntf.raw_ops.RefMerge\ntf.raw_ops.RefNextIteration\ntf.raw_ops.RefSelect\ntf.raw_ops.RefSwitch\ntf.raw_ops.RegexFullMatch\ntf.raw_ops.RegexReplace\ntf.raw_ops.RegisterDataset\ntf.raw_ops.RegisterDatasetV2\ntf.raw_ops.Relu\ntf.raw_ops.Relu6\ntf.raw_ops.Relu6Grad\ntf.raw_ops.ReluGrad\ntf.raw_ops.RemoteCall\ntf.raw_ops.RepeatDataset\ntf.raw_ops.RequantizationRange\ntf.raw_ops.RequantizationRangePerChannel\ntf.raw_ops.Requantize\ntf.raw_ops.RequantizePerChannel\ntf.raw_ops.Reshape\ntf.raw_ops.ResizeArea\ntf.raw_ops.ResizeBicubic\ntf.raw_ops.ResizeBicubicGrad\ntf.raw_ops.ResizeBilinear\ntf.raw_ops.ResizeBilinearGrad\ntf.raw_ops.ResizeNearestNeighbor\ntf.raw_ops.ResizeNearestNeighborGrad\ntf.raw_ops.ResourceAccumulatorApplyGradient\ntf.raw_ops.ResourceAccumulatorNumAccumulated\ntf.raw_ops.ResourceAccumulatorSetGlobalStep\ntf.raw_ops.ResourceAccumulatorTakeGradient\ntf.raw_ops.ResourceApplyAdaMax\ntf.raw_ops.ResourceApplyAdadelta\ntf.raw_ops.ResourceApplyAdagrad\ntf.raw_ops.ResourceApplyAdagradDA\ntf.raw_ops.ResourceApplyAdagradV2\ntf.raw_ops.ResourceApplyAdam\ntf.raw_ops.ResourceApplyAdamWithAmsgrad\ntf.raw_ops.ResourceApplyAddSign\ntf.raw_ops.ResourceApplyCenteredRMSProp\ntf.raw_ops.ResourceApplyFtrl\ntf.raw_ops.ResourceApplyFtrlV2\ntf.raw_ops.ResourceApplyGradientDescent\ntf.raw_ops.ResourceApplyKerasMomentum\ntf.raw_ops.ResourceApplyMomentum\ntf.raw_ops.ResourceApplyPowerSign\ntf.raw_ops.ResourceApplyProximalAdagrad\ntf.raw_ops.ResourceApplyProximalGradientDescent\ntf.raw_ops.ResourceApplyRMSProp\ntf.raw_ops.ResourceConditionalAccumulator\ntf.raw_ops.ResourceCountUpTo\ntf.raw_ops.ResourceGather\ntf.raw_ops.ResourceGatherNd\ntf.raw_ops.ResourceScatterAdd\ntf.raw_ops.ResourceScatterDiv\ntf.raw_ops.ResourceScatterMax\ntf.raw_ops.ResourceScatterMin\ntf.raw_ops.ResourceScatterMul\ntf.raw_ops.ResourceScatterNdAdd\ntf.raw_ops.ResourceScatterNdMax\ntf.raw_ops.ResourceScatterNdMin\ntf.raw_ops.ResourceScatterNdSub\ntf.raw_ops.ResourceScatterNdUpdate\ntf.raw_ops.ResourceScatterSub\ntf.raw_ops.ResourceScatterUpdate\ntf.raw_ops.ResourceSparseApplyAdadelta\ntf.raw_ops.ResourceSparseApplyAdagrad\ntf.raw_ops.ResourceSparseApplyAdagradDA\ntf.raw_ops.ResourceSparseApplyAdagradV2\ntf.raw_ops.ResourceSparseApplyCenteredRMSProp\ntf.raw_ops.ResourceSparseApplyFtrl\ntf.raw_ops.ResourceSparseApplyFtrlV2\ntf.raw_ops.ResourceSparseApplyKerasMomentum\ntf.raw_ops.ResourceSparseApplyMomentum\ntf.raw_ops.ResourceSparseApplyProximalAdagrad\ntf.raw_ops.ResourceSparseApplyProximalGradientDescent\ntf.raw_ops.ResourceSparseApplyRMSProp\ntf.raw_ops.ResourceStridedSliceAssign\ntf.raw_ops.Restore\ntf.raw_ops.RestoreSlice\ntf.raw_ops.RestoreV2\ntf.raw_ops.RetrieveTPUEmbeddingADAMParameters\ntf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters\ntf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters\ntf.raw_ops.RetrieveTPUEmbeddingAdagradParameters\ntf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters\ntf.raw_ops.RetrieveTPUEmbeddingFTRLParameters\ntf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters\ntf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters\ntf.raw_ops.RetrieveTPUEmbeddingMomentumParameters\ntf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters\ntf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters\ntf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters\ntf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters\ntf.raw_ops.Reverse\ntf.raw_ops.ReverseSequence\ntf.raw_ops.ReverseV2\ntf.raw_ops.RewriteDataset\ntf.raw_ops.RightShift\ntf.raw_ops.Rint\ntf.raw_ops.RngReadAndSkip\ntf.raw_ops.RngSkip\ntf.raw_ops.Roll\ntf.raw_ops.Round\ntf.raw_ops.Rsqrt\ntf.raw_ops.RsqrtGrad\ntf.raw_ops.SampleDistortedBoundingBox\ntf.raw_ops.SampleDistortedBoundingBoxV2\ntf.raw_ops.SamplingDataset\ntf.raw_ops.Save\ntf.raw_ops.SaveDataset\ntf.raw_ops.SaveDatasetV2\ntf.raw_ops.SaveSlices\ntf.raw_ops.SaveV2\ntf.raw_ops.ScalarSummary\ntf.raw_ops.ScaleAndTranslate\ntf.raw_ops.ScaleAndTranslateGrad\ntf.raw_ops.ScanDataset\ntf.raw_ops.ScatterAdd\ntf.raw_ops.ScatterDiv\ntf.raw_ops.ScatterMax\ntf.raw_ops.ScatterMin\ntf.raw_ops.ScatterMul\ntf.raw_ops.ScatterNd\ntf.raw_ops.ScatterNdAdd\ntf.raw_ops.ScatterNdMax\ntf.raw_ops.ScatterNdMin\ntf.raw_ops.ScatterNdNonAliasingAdd\ntf.raw_ops.ScatterNdSub\ntf.raw_ops.ScatterNdUpdate\ntf.raw_ops.ScatterSub\ntf.raw_ops.ScatterUpdate\ntf.raw_ops.SdcaFprint\ntf.raw_ops.SdcaOptimizer\ntf.raw_ops.SdcaOptimizerV2\ntf.raw_ops.SdcaShrinkL1\ntf.raw_ops.SegmentMax\ntf.raw_ops.SegmentMaxV2\ntf.raw_ops.SegmentMean\ntf.raw_ops.SegmentMin\ntf.raw_ops.SegmentMinV2\ntf.raw_ops.SegmentProd\ntf.raw_ops.SegmentProdV2\ntf.raw_ops.SegmentSum\ntf.raw_ops.SegmentSumV2\ntf.raw_ops.Select\ntf.raw_ops.SelectV2\ntf.raw_ops.SelfAdjointEig\ntf.raw_ops.SelfAdjointEigV2\ntf.raw_ops.Selu\ntf.raw_ops.SeluGrad\ntf.raw_ops.Send\ntf.raw_ops.SendTPUEmbeddingGradients\ntf.raw_ops.SerializeIterator\ntf.raw_ops.SerializeManySparse\ntf.raw_ops.SerializeSparse\ntf.raw_ops.SerializeTensor\ntf.raw_ops.SetSize\ntf.raw_ops.SetStatsAggregatorDataset\ntf.raw_ops.Shape\ntf.raw_ops.ShapeN\ntf.raw_ops.ShardDataset\ntf.raw_ops.ShardedFilename\ntf.raw_ops.ShardedFilespec\ntf.raw_ops.ShuffleAndRepeatDataset\ntf.raw_ops.ShuffleAndRepeatDatasetV2\ntf.raw_ops.ShuffleDataset\ntf.raw_ops.ShuffleDatasetV2\ntf.raw_ops.ShuffleDatasetV3\ntf.raw_ops.ShutdownDistributedTPU\ntf.raw_ops.Sigmoid\ntf.raw_ops.SigmoidGrad\ntf.raw_ops.Sign\ntf.raw_ops.Sin\ntf.raw_ops.Sinh\ntf.raw_ops.Size\ntf.raw_ops.SkipDataset\ntf.raw_ops.SleepDataset\ntf.raw_ops.Slice\ntf.raw_ops.SlidingWindowDataset\ntf.raw_ops.Snapshot\ntf.raw_ops.SnapshotChunkDataset\ntf.raw_ops.SnapshotDataset\ntf.raw_ops.SnapshotDatasetReader\ntf.raw_ops.SnapshotDatasetV2\ntf.raw_ops.SnapshotNestedDatasetReader\ntf.raw_ops.SobolSample\ntf.raw_ops.Softmax\ntf.raw_ops.SoftmaxCrossEntropyWithLogits\ntf.raw_ops.Softplus\ntf.raw_ops.SoftplusGrad\ntf.raw_ops.Softsign\ntf.raw_ops.SoftsignGrad\ntf.raw_ops.SpaceToBatch\ntf.raw_ops.SpaceToBatchND\ntf.raw_ops.SpaceToDepth\ntf.raw_ops.SparseAccumulatorApplyGradient\ntf.raw_ops.SparseAccumulatorTakeGradient\ntf.raw_ops.SparseAdd\ntf.raw_ops.SparseAddGrad\ntf.raw_ops.SparseApplyAdadelta\ntf.raw_ops.SparseApplyAdagrad\ntf.raw_ops.SparseApplyAdagradDA\ntf.raw_ops.SparseApplyAdagradV2\ntf.raw_ops.SparseApplyCenteredRMSProp\ntf.raw_ops.SparseApplyFtrl\ntf.raw_ops.SparseApplyFtrlV2\ntf.raw_ops.SparseApplyMomentum\ntf.raw_ops.SparseApplyProximalAdagrad\ntf.raw_ops.SparseApplyProximalGradientDescent\ntf.raw_ops.SparseApplyRMSProp\ntf.raw_ops.SparseBincount\ntf.raw_ops.SparseConcat\ntf.raw_ops.SparseConditionalAccumulator\ntf.raw_ops.SparseCountSparseOutput\ntf.raw_ops.SparseCross\ntf.raw_ops.SparseCrossHashed\ntf.raw_ops.SparseCrossV2\ntf.raw_ops.SparseDenseCwiseAdd\ntf.raw_ops.SparseDenseCwiseDiv\ntf.raw_ops.SparseDenseCwiseMul\ntf.raw_ops.SparseFillEmptyRows\ntf.raw_ops.SparseFillEmptyRowsGrad\ntf.raw_ops.SparseMatMul\ntf.raw_ops.SparseMatrixAdd\ntf.raw_ops.SparseMatrixMatMul\ntf.raw_ops.SparseMatrixMul\ntf.raw_ops.SparseMatrixNNZ\ntf.raw_ops.SparseMatrixOrderingAMD\ntf.raw_ops.SparseMatrixSoftmax\ntf.raw_ops.SparseMatrixSoftmaxGrad\ntf.raw_ops.SparseMatrixSparseCholesky\ntf.raw_ops.SparseMatrixSparseMatMul\ntf.raw_ops.SparseMatrixTranspose\ntf.raw_ops.SparseMatrixZeros\ntf.raw_ops.SparseReduceMax\ntf.raw_ops.SparseReduceMaxSparse\ntf.raw_ops.SparseReduceSum\ntf.raw_ops.SparseReduceSumSparse\ntf.raw_ops.SparseReorder\ntf.raw_ops.SparseReshape\ntf.raw_ops.SparseSegmentMean\ntf.raw_ops.SparseSegmentMeanGrad\ntf.raw_ops.SparseSegmentMeanGradV2\ntf.raw_ops.SparseSegmentMeanWithNumSegments\ntf.raw_ops.SparseSegmentSqrtN\ntf.raw_ops.SparseSegmentSqrtNGrad\ntf.raw_ops.SparseSegmentSqrtNGradV2\ntf.raw_ops.SparseSegmentSqrtNWithNumSegments\ntf.raw_ops.SparseSegmentSum\ntf.raw_ops.SparseSegmentSumGrad\ntf.raw_ops.SparseSegmentSumGradV2\ntf.raw_ops.SparseSegmentSumWithNumSegments\ntf.raw_ops.SparseSlice\ntf.raw_ops.SparseSliceGrad\ntf.raw_ops.SparseSoftmax\ntf.raw_ops.SparseSoftmaxCrossEntropyWithLogits\ntf.raw_ops.SparseSparseMaximum\ntf.raw_ops.SparseSparseMinimum\ntf.raw_ops.SparseSplit\ntf.raw_ops.SparseTensorDenseAdd\ntf.raw_ops.SparseTensorDenseMatMul\ntf.raw_ops.SparseTensorSliceDataset\ntf.raw_ops.SparseTensorToCSRSparseMatrix\ntf.raw_ops.SparseToDense\ntf.raw_ops.SparseToSparseSetOperation\ntf.raw_ops.Spence\ntf.raw_ops.Split\ntf.raw_ops.SplitV\ntf.raw_ops.SqlDataset\ntf.raw_ops.Sqrt\ntf.raw_ops.SqrtGrad\ntf.raw_ops.Square\ntf.raw_ops.SquaredDifference\ntf.raw_ops.Squeeze\ntf.raw_ops.Stack\ntf.raw_ops.StackClose\ntf.raw_ops.StackCloseV2\ntf.raw_ops.StackPop\ntf.raw_ops.StackPopV2\ntf.raw_ops.StackPush\ntf.raw_ops.StackPushV2\ntf.raw_ops.StackV2\ntf.raw_ops.Stage\ntf.raw_ops.StageClear\ntf.raw_ops.StagePeek\ntf.raw_ops.StageSize\ntf.raw_ops.StatefulPartitionedCall\ntf.raw_ops.StatefulRandomBinomial\ntf.raw_ops.StatefulStandardNormal\ntf.raw_ops.StatefulStandardNormalV2\ntf.raw_ops.StatefulTruncatedNormal\ntf.raw_ops.StatefulUniform\ntf.raw_ops.StatefulUniformFullInt\ntf.raw_ops.StatefulUniformInt\ntf.raw_ops.StatelessCase\ntf.raw_ops.StatelessIf\ntf.raw_ops.StatelessMultinomial\ntf.raw_ops.StatelessParameterizedTruncatedNormal\ntf.raw_ops.StatelessRandomBinomial\ntf.raw_ops.StatelessRandomGammaV2\ntf.raw_ops.StatelessRandomGammaV3\ntf.raw_ops.StatelessRandomGetAlg\ntf.raw_ops.StatelessRandomGetKeyCounter\ntf.raw_ops.StatelessRandomGetKeyCounterAlg\ntf.raw_ops.StatelessRandomNormal\ntf.raw_ops.StatelessRandomNormalV2\ntf.raw_ops.StatelessRandomPoisson\ntf.raw_ops.StatelessRandomUniform\ntf.raw_ops.StatelessRandomUniformFullInt\ntf.raw_ops.StatelessRandomUniformFullIntV2\ntf.raw_ops.StatelessRandomUniformInt\ntf.raw_ops.StatelessRandomUniformIntV2\ntf.raw_ops.StatelessRandomUniformV2\ntf.raw_ops.StatelessSampleDistortedBoundingBox\ntf.raw_ops.StatelessShuffle\ntf.raw_ops.StatelessTruncatedNormal\ntf.raw_ops.StatelessTruncatedNormalV2\ntf.raw_ops.StatelessWhile\ntf.raw_ops.StaticRegexFullMatch\ntf.raw_ops.StaticRegexReplace\ntf.raw_ops.StatsAggregatorHandle\ntf.raw_ops.StatsAggregatorHandleV2\ntf.raw_ops.StatsAggregatorSetSummaryWriter\ntf.raw_ops.StatsAggregatorSummary\ntf.raw_ops.StopGradient\ntf.raw_ops.StoreMinibatchStatisticsInFdo\ntf.raw_ops.StridedSlice\ntf.raw_ops.StridedSliceAssign\ntf.raw_ops.StridedSliceGrad\ntf.raw_ops.StringFormat\ntf.raw_ops.StringJoin\ntf.raw_ops.StringLength\ntf.raw_ops.StringLower\ntf.raw_ops.StringNGrams\ntf.raw_ops.StringSplit\ntf.raw_ops.StringSplitV2\ntf.raw_ops.StringStrip\ntf.raw_ops.StringToHashBucket\ntf.raw_ops.StringToHashBucketFast\ntf.raw_ops.StringToHashBucketStrong\ntf.raw_ops.StringToNumber\ntf.raw_ops.StringUpper\ntf.raw_ops.Sub\ntf.raw_ops.Substr\ntf.raw_ops.Sum\ntf.raw_ops.SummaryWriter\ntf.raw_ops.Svd\ntf.raw_ops.Switch\ntf.raw_ops.SymbolicGradient\ntf.raw_ops.SyncDevice\ntf.raw_ops.TFRecordDataset\ntf.raw_ops.TFRecordDatasetV2\ntf.raw_ops.TFRecordReader\ntf.raw_ops.TFRecordReaderV2\ntf.raw_ops.TPUAnnotateTensorsWithDynamicShape\ntf.raw_ops.TPUCompilationResult\ntf.raw_ops.TPUCopyWithDynamicShape\ntf.raw_ops.TPUEmbeddingActivations\ntf.raw_ops.TPUOrdinalSelector\ntf.raw_ops.TPUPartitionedCall\ntf.raw_ops.TPUPartitionedInput\ntf.raw_ops.TPUPartitionedInputV2\ntf.raw_ops.TPUPartitionedOutput\ntf.raw_ops.TPUPartitionedOutputV2\ntf.raw_ops.TPUReplicateMetadata\ntf.raw_ops.TPUReplicatedInput\ntf.raw_ops.TPUReplicatedOutput\ntf.raw_ops.TakeDataset\ntf.raw_ops.TakeManySparseFromTensorsMap\ntf.raw_ops.TakeWhileDataset\ntf.raw_ops.Tan\ntf.raw_ops.Tanh\ntf.raw_ops.TanhGrad\ntf.raw_ops.TemporaryVariable\ntf.raw_ops.TensorArray\ntf.raw_ops.TensorArrayClose\ntf.raw_ops.TensorArrayCloseV2\ntf.raw_ops.TensorArrayCloseV3\ntf.raw_ops.TensorArrayConcat\ntf.raw_ops.TensorArrayConcatV2\ntf.raw_ops.TensorArrayConcatV3\ntf.raw_ops.TensorArrayGather\ntf.raw_ops.TensorArrayGatherV2\ntf.raw_ops.TensorArrayGatherV3\ntf.raw_ops.TensorArrayGrad\ntf.raw_ops.TensorArrayGradV2\ntf.raw_ops.TensorArrayGradV3\ntf.raw_ops.TensorArrayGradWithShape\ntf.raw_ops.TensorArrayPack\ntf.raw_ops.TensorArrayRead\ntf.raw_ops.TensorArrayReadV2\ntf.raw_ops.TensorArrayReadV3\ntf.raw_ops.TensorArrayScatter\ntf.raw_ops.TensorArrayScatterV2\ntf.raw_ops.TensorArrayScatterV3\ntf.raw_ops.TensorArraySize\ntf.raw_ops.TensorArraySizeV2\ntf.raw_ops.TensorArraySizeV3\ntf.raw_ops.TensorArraySplit\ntf.raw_ops.TensorArraySplitV2\ntf.raw_ops.TensorArraySplitV3\ntf.raw_ops.TensorArrayUnpack\ntf.raw_ops.TensorArrayV2\ntf.raw_ops.TensorArrayV3\ntf.raw_ops.TensorArrayWrite\ntf.raw_ops.TensorArrayWriteV2\ntf.raw_ops.TensorArrayWriteV3\ntf.raw_ops.TensorDataset\ntf.raw_ops.TensorListConcat\ntf.raw_ops.TensorListConcatLists\ntf.raw_ops.TensorListConcatV2\ntf.raw_ops.TensorListElementShape\ntf.raw_ops.TensorListFromTensor\ntf.raw_ops.TensorListGather\ntf.raw_ops.TensorListGetItem\ntf.raw_ops.TensorListLength\ntf.raw_ops.TensorListPopBack\ntf.raw_ops.TensorListPushBack\ntf.raw_ops.TensorListPushBackBatch\ntf.raw_ops.TensorListReserve\ntf.raw_ops.TensorListResize\ntf.raw_ops.TensorListScatter\ntf.raw_ops.TensorListScatterIntoExistingList\ntf.raw_ops.TensorListScatterV2\ntf.raw_ops.TensorListSetItem\ntf.raw_ops.TensorListSplit\ntf.raw_ops.TensorListStack\ntf.raw_ops.TensorMapErase\ntf.raw_ops.TensorMapHasKey\ntf.raw_ops.TensorMapInsert\ntf.raw_ops.TensorMapLookup\ntf.raw_ops.TensorMapSize\ntf.raw_ops.TensorMapStackKeys\ntf.raw_ops.TensorScatterAdd\ntf.raw_ops.TensorScatterMax\ntf.raw_ops.TensorScatterMin\ntf.raw_ops.TensorScatterSub\ntf.raw_ops.TensorScatterUpdate\ntf.raw_ops.TensorSliceDataset\ntf.raw_ops.TensorStridedSliceUpdate\ntf.raw_ops.TensorSummary\ntf.raw_ops.TensorSummaryV2\ntf.raw_ops.TextLineDataset\ntf.raw_ops.TextLineReader\ntf.raw_ops.TextLineReaderV2\ntf.raw_ops.ThreadPoolDataset\ntf.raw_ops.ThreadPoolHandle\ntf.raw_ops.ThreadUnsafeUnigramCandidateSampler\ntf.raw_ops.Tile\ntf.raw_ops.TileGrad\ntf.raw_ops.Timestamp\ntf.raw_ops.ToBool\ntf.raw_ops.TopK\ntf.raw_ops.TopKV2\ntf.raw_ops.Transpose\ntf.raw_ops.TridiagonalMatMul\ntf.raw_ops.TridiagonalSolve\ntf.raw_ops.TruncateDiv\ntf.raw_ops.TruncateMod\ntf.raw_ops.TruncatedNormal\ntf.raw_ops.Unbatch\ntf.raw_ops.UnbatchDataset\ntf.raw_ops.UnbatchGrad\ntf.raw_ops.UncompressElement\ntf.raw_ops.UnicodeDecode\ntf.raw_ops.UnicodeDecodeWithOffsets\ntf.raw_ops.UnicodeEncode\ntf.raw_ops.UnicodeScript\ntf.raw_ops.UnicodeTranscode\ntf.raw_ops.UniformCandidateSampler\ntf.raw_ops.UniformDequantize\ntf.raw_ops.UniformQuantize\ntf.raw_ops.UniformQuantizedAdd\ntf.raw_ops.UniformQuantizedClipByValue\ntf.raw_ops.UniformQuantizedConvolution\ntf.raw_ops.UniformQuantizedConvolutionHybrid\ntf.raw_ops.UniformQuantizedDot\ntf.raw_ops.UniformQuantizedDotHybrid\ntf.raw_ops.UniformRequantize\ntf.raw_ops.Unique\ntf.raw_ops.UniqueDataset\ntf.raw_ops.UniqueV2\ntf.raw_ops.UniqueWithCounts\ntf.raw_ops.UniqueWithCountsV2\ntf.raw_ops.Unpack\ntf.raw_ops.UnravelIndex\ntf.raw_ops.UnsortedSegmentJoin\ntf.raw_ops.UnsortedSegmentMax\ntf.raw_ops.UnsortedSegmentMin\ntf.raw_ops.UnsortedSegmentProd\ntf.raw_ops.UnsortedSegmentSum\ntf.raw_ops.Unstage\ntf.raw_ops.UnwrapDatasetVariant\ntf.raw_ops.UpperBound\ntf.raw_ops.VarHandleOp\ntf.raw_ops.VarIsInitializedOp\ntf.raw_ops.Variable\ntf.raw_ops.VariableShape\ntf.raw_ops.VariableV2\ntf.raw_ops.Where\ntf.raw_ops.While\ntf.raw_ops.WholeFileReader\ntf.raw_ops.WholeFileReaderV2\ntf.raw_ops.WindowDataset\ntf.raw_ops.WindowOp\ntf.raw_ops.WorkerHeartbeat\ntf.raw_ops.WrapDatasetVariant\ntf.raw_ops.WriteAudioSummary\ntf.raw_ops.WriteFile\ntf.raw_ops.WriteGraphSummary\ntf.raw_ops.WriteHistogramSummary\ntf.raw_ops.WriteImageSummary\ntf.raw_ops.WriteRawProtoSummary\ntf.raw_ops.WriteScalarSummary\ntf.raw_ops.WriteSummary\ntf.raw_ops.Xdivy\ntf.raw_ops.XlaConcatND\ntf.raw_ops.XlaSparseCoreAdagrad\ntf.raw_ops.XlaSparseCoreAdagradMomentum\ntf.raw_ops.XlaSparseCoreAdam\ntf.raw_ops.XlaSparseCoreFtrl\ntf.raw_ops.XlaSparseCoreSgd\ntf.raw_ops.XlaSparseDenseMatmul\ntf.raw_ops.XlaSparseDenseMatmulGradWithAdagradAndCsrInput\ntf.raw_ops.XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput\ntf.raw_ops.XlaSparseDenseMatmulGradWithAdamAndCsrInput\ntf.raw_ops.XlaSparseDenseMatmulGradWithFtrlAndCsrInput\ntf.raw_ops.XlaSparseDenseMatmulGradWithSgdAndCsrInput\ntf.raw_ops.XlaSparseDenseMatmulWithCsrInput\ntf.raw_ops.XlaSplitND\ntf.raw_ops.Xlog1py\ntf.raw_ops.Xlogy\ntf.raw_ops.ZerosLike\ntf.raw_ops.Zeta\ntf.raw_ops.ZipDataset\ntf.realdiv\ntf.recompute_grad\ntf.reduce_all\ntf.reduce_any\ntf.reduce_logsumexp\ntf.reduce_max\ntf.reduce_mean\ntf.reduce_min\ntf.reduce_prod\ntf.reduce_sum\ntf.register_tensor_conversion_function\ntf.repeat\ntf.required_space_to_batch_paddings\ntf.reshape\ntf.reverse\ntf.reverse_sequence\ntf.rfftnd\ntf.roll\ntf.round\ntf.saturate_cast\ntf.saved_model\ntf.saved_model.Asset\ntf.saved_model.LoadOptions\ntf.saved_model.SaveOptions\ntf.saved_model.contains_saved_model\ntf.saved_model.experimental\ntf.saved_model.experimental.Fingerprint\ntf.saved_model.experimental.TrackableResource\ntf.saved_model.experimental.VariablePolicy\ntf.saved_model.experimental.read_fingerprint\ntf.saved_model.load\ntf.saved_model.save\ntf.scalar_mul\ntf.scan\ntf.scatter_nd\ntf.searchsorted\ntf.sequence_mask\ntf.sets\ntf.sets.difference\ntf.sets.intersection\ntf.sets.size\ntf.sets.union\ntf.shape\ntf.shape_n\ntf.sigmoid\ntf.sign\ntf.signal\ntf.signal.dct\ntf.signal.fft\ntf.signal.fft2d\ntf.signal.fft3d\ntf.signal.fftnd\ntf.signal.fftshift\ntf.signal.frame\ntf.signal.hamming_window\ntf.signal.hann_window\ntf.signal.idct\ntf.signal.ifft\ntf.signal.ifft2d\ntf.signal.ifft3d\ntf.signal.ifftnd\ntf.signal.ifftshift\ntf.signal.inverse_mdct\ntf.signal.inverse_stft\ntf.signal.inverse_stft_window_fn\ntf.signal.irfft\ntf.signal.irfft2d\ntf.signal.irfft3d\ntf.signal.irfftnd\ntf.signal.kaiser_bessel_derived_window\ntf.signal.kaiser_window\ntf.signal.linear_to_mel_weight_matrix\ntf.signal.mdct\ntf.signal.mfccs_from_log_mel_spectrograms\ntf.signal.overlap_and_add\ntf.signal.rfft\ntf.signal.rfft2d\ntf.signal.rfft3d\ntf.signal.rfftnd\ntf.signal.stft\ntf.signal.vorbis_window\ntf.sin\ntf.sinh\ntf.size\ntf.slice\ntf.sort\ntf.space_to_batch\ntf.space_to_batch_nd\ntf.sparse\ntf.sparse.SparseTensor\ntf.sparse.add\ntf.sparse.bincount\ntf.sparse.concat\ntf.sparse.cross\ntf.sparse.cross_hashed\ntf.sparse.expand_dims\ntf.sparse.eye\ntf.sparse.fill_empty_rows\ntf.sparse.from_dense\ntf.sparse.map_values\ntf.sparse.mask\ntf.sparse.maximum\ntf.sparse.minimum\ntf.sparse.reduce_max\ntf.sparse.reduce_sum\ntf.sparse.reorder\ntf.sparse.reset_shape\ntf.sparse.reshape\ntf.sparse.retain\ntf.sparse.segment_mean\ntf.sparse.segment_sqrt_n\ntf.sparse.segment_sum\ntf.sparse.slice\ntf.sparse.softmax\ntf.sparse.sparse_dense_matmul\ntf.sparse.split\ntf.sparse.to_dense\ntf.sparse.to_indicator\ntf.sparse.transpose\ntf.split\ntf.sqrt\ntf.square\ntf.squeeze\ntf.stack\ntf.stop_gradient\ntf.strided_slice\ntf.strings\ntf.strings.as_string\ntf.strings.bytes_split\ntf.strings.format\ntf.strings.join\ntf.strings.length\ntf.strings.lower\ntf.strings.ngrams\ntf.strings.reduce_join\ntf.strings.regex_full_match\ntf.strings.regex_replace\ntf.strings.split\ntf.strings.strip\ntf.strings.substr\ntf.strings.to_hash_bucket\ntf.strings.to_hash_bucket_fast\ntf.strings.to_hash_bucket_strong\ntf.strings.to_number\ntf.strings.unicode_decode\ntf.strings.unicode_decode_with_offsets\ntf.strings.unicode_encode\ntf.strings.unicode_script\ntf.strings.unicode_split\ntf.strings.unicode_split_with_offsets\ntf.strings.unicode_transcode\ntf.strings.unsorted_segment_join\ntf.strings.upper\ntf.subtract\ntf.summary\ntf.summary.SummaryWriter\ntf.summary.audio\ntf.summary.create_file_writer\ntf.summary.create_noop_writer\ntf.summary.experimental\ntf.summary.experimental.get_step\ntf.summary.experimental.set_step\ntf.summary.experimental.summary_scope\ntf.summary.experimental.write_raw_pb\ntf.summary.flush\ntf.summary.graph\ntf.summary.histogram\ntf.summary.image\ntf.summary.record_if\ntf.summary.scalar\ntf.summary.should_record_summaries\ntf.summary.text\ntf.summary.trace_export\ntf.summary.trace_off\ntf.summary.trace_on\ntf.summary.write\ntf.switch_case\ntf.sysconfig\ntf.sysconfig.get_build_info\ntf.sysconfig.get_compile_flags\ntf.sysconfig.get_include\ntf.sysconfig.get_lib\ntf.sysconfig.get_link_flags\ntf.tan\ntf.tanh\ntf.tensor_scatter_nd_add\ntf.tensor_scatter_nd_max\ntf.tensor_scatter_nd_min\ntf.tensor_scatter_nd_sub\ntf.tensor_scatter_nd_update\ntf.tensordot\ntf.test\ntf.test.Benchmark\ntf.test.TestCase\ntf.test.TestCase.failureException\ntf.test.assert_equal_graph_def\ntf.test.benchmark_config\ntf.test.compute_gradient\ntf.test.create_local_cluster\ntf.test.disable_with_predicate\ntf.test.experimental\ntf.test.experimental.sync_devices\ntf.test.gpu_device_name\ntf.test.is_built_with_cuda\ntf.test.is_built_with_gpu_support\ntf.test.is_built_with_rocm\ntf.test.is_built_with_xla\ntf.test.is_gpu_available\ntf.test.main\ntf.test.with_eager_op_as_function\ntf.tile\ntf.timestamp\ntf.tpu\ntf.tpu.XLAOptions\ntf.tpu.experimental\ntf.tpu.experimental.DeviceAssignment\ntf.tpu.experimental.DeviceOrderMode\ntf.tpu.experimental.HardwareFeature\ntf.tpu.experimental.HardwareFeature.EmbeddingFeature\ntf.tpu.experimental.TPUSystemMetadata\ntf.tpu.experimental.Topology\ntf.tpu.experimental.embedding\ntf.tpu.experimental.embedding.Adagrad\ntf.tpu.experimental.embedding.AdagradMomentum\ntf.tpu.experimental.embedding.Adam\ntf.tpu.experimental.embedding.FTRL\ntf.tpu.experimental.embedding.FeatureConfig\ntf.tpu.experimental.embedding.QuantizationConfig\ntf.tpu.experimental.embedding.RowIdInitializer\ntf.tpu.experimental.embedding.SGD\ntf.tpu.experimental.embedding.TPUEmbedding\ntf.tpu.experimental.embedding.TPUEmbeddingForServing\ntf.tpu.experimental.embedding.TPUEmbeddingV0\ntf.tpu.experimental.embedding.TPUEmbeddingV2\ntf.tpu.experimental.embedding.TableConfig\ntf.tpu.experimental.embedding.serving_embedding_lookup\ntf.tpu.experimental.initialize_tpu_system\ntf.tpu.experimental.shutdown_tpu_system\ntf.train\ntf.train.BytesList\ntf.train.Checkpoint\ntf.train.CheckpointManager\ntf.train.CheckpointOptions\ntf.train.CheckpointView\ntf.train.ClusterDef\ntf.train.ClusterSpec\ntf.train.Coordinator\ntf.train.Example\ntf.train.ExponentialMovingAverage\ntf.train.Feature\ntf.train.FeatureList\ntf.train.FeatureLists\ntf.train.FeatureLists.FeatureListEntry\ntf.train.Features\ntf.train.Features.FeatureEntry\ntf.train.FloatList\ntf.train.Int64List\ntf.train.JobDef\ntf.train.JobDef.TasksEntry\ntf.train.SequenceExample\ntf.train.ServerDef\ntf.train.TrackableView\ntf.train.checkpoints_iterator\ntf.train.experimental\ntf.train.experimental.MaxShardSizePolicy\ntf.train.experimental.PythonState\ntf.train.experimental.ShardByTaskPolicy\ntf.train.experimental.ShardableTensor\ntf.train.experimental.ShardingCallback\ntf.train.get_checkpoint_state\ntf.train.latest_checkpoint\ntf.train.list_variables\ntf.train.load_checkpoint\ntf.train.load_variable\ntf.transpose\ntf.truediv\ntf.truncatediv\ntf.truncatemod\ntf.tuple\ntf.type_spec_from_value\ntf.types\ntf.types.experimental\ntf.types.experimental.AtomicFunction\ntf.types.experimental.Callable\ntf.types.experimental.ConcreteFunction\ntf.types.experimental.FunctionType\ntf.types.experimental.FunctionType.empty\ntf.types.experimental.GenericFunction\ntf.types.experimental.PolymorphicFunction\ntf.types.experimental.SupportsTracingProtocol\ntf.types.experimental.TensorLike\ntf.types.experimental.TraceType\ntf.types.experimental.distributed\ntf.types.experimental.distributed.Mirrored\ntf.types.experimental.distributed.PerReplica\ntf.unique\ntf.unique_with_counts\ntf.unravel_index\ntf.unstack\ntf.variable_creator_scope\ntf.vectorized_map\ntf.version\ntf.where\ntf.while_loop\ntf.xla\ntf.xla.experimental\ntf.xla.experimental.compile\ntf.xla.experimental.jit_scope\ntf.zeros\ntf.zeros_initializer\ntf.zeros_like\nCompat v1 symbols\ntf.compat.v1\ntf.compat.v1.AggregationMethod\ntf.compat.v1.Assert\ntf.compat.v1.AttrValue\ntf.compat.v1.AttrValue.ListValue\ntf.compat.v1.ConditionalAccumulator\ntf.compat.v1.ConditionalAccumulatorBase\ntf.compat.v1.ConfigProto\ntf.compat.v1.ConfigProto.DeviceCountEntry\ntf.compat.v1.ConfigProto.Experimental\ntf.compat.v1.CriticalSection\ntf.compat.v1.DType\ntf.compat.v1.DeviceSpec\ntf.compat.v1.Dimension\ntf.compat.v1.Event\ntf.compat.v1.FIFOQueue\ntf.compat.v1.FixedLenFeature\ntf.compat.v1.FixedLenSequenceFeature\ntf.compat.v1.FixedLengthRecordReader\ntf.compat.v1.GPUOptions\ntf.compat.v1.GPUOptions.Experimental\ntf.compat.v1.GPUOptions.Experimental.VirtualDevices\ntf.compat.v1.GradientTape\ntf.compat.v1.Graph\ntf.compat.v1.GraphDef\ntf.compat.v1.GraphKeys\ntf.compat.v1.GraphOptions\ntf.compat.v1.HistogramProto\ntf.compat.v1.IdentityReader\ntf.compat.v1.IndexedSlices\ntf.compat.v1.IndexedSlicesSpec\ntf.compat.v1.InteractiveSession\ntf.compat.v1.LMDBReader\ntf.compat.v1.LogMessage\ntf.compat.v1.MetaGraphDef\ntf.compat.v1.MetaGraphDef.CollectionDefEntry\ntf.compat.v1.MetaGraphDef.MetaInfoDef\ntf.compat.v1.MetaGraphDef.MetaInfoDef.FunctionAliasesEntry\ntf.compat.v1.MetaGraphDef.SignatureDefEntry\ntf.compat.v1.Module\ntf.compat.v1.NameAttrList\ntf.compat.v1.NameAttrList.AttrEntry\ntf.compat.v1.NoGradient\ntf.compat.v1.NodeDef\ntf.compat.v1.NodeDef.AttrEntry\ntf.compat.v1.NodeDef.ExperimentalDebugInfo\ntf.compat.v1.NotDifferentiable\ntf.compat.v1.OpError\ntf.compat.v1.Operation\ntf.compat.v1.OptimizerOptions\ntf.compat.v1.OptionalSpec\ntf.compat.v1.PaddingFIFOQueue\ntf.compat.v1.Print\ntf.compat.v1.PriorityQueue\ntf.compat.v1.QueueBase\ntf.compat.v1.RaggedTensor\ntf.compat.v1.RaggedTensorSpec\ntf.compat.v1.RandomShuffleQueue\ntf.compat.v1.ReaderBase\ntf.compat.v1.RegisterGradient\ntf.compat.v1.RunMetadata\ntf.compat.v1.RunMetadata.FunctionGraphs\ntf.compat.v1.RunOptions\ntf.compat.v1.RunOptions.Experimental\ntf.compat.v1.RunOptions.Experimental.RunHandlerPoolOptions\ntf.compat.v1.Session\ntf.compat.v1.SessionLog\ntf.compat.v1.SparseConditionalAccumulator\ntf.compat.v1.SparseFeature\ntf.compat.v1.SparseTensor\ntf.compat.v1.SparseTensorSpec\ntf.compat.v1.SparseTensorValue\ntf.compat.v1.Summary\ntf.compat.v1.Summary.Audio\ntf.compat.v1.Summary.Image\ntf.compat.v1.Summary.Value\ntf.compat.v1.SummaryMetadata\ntf.compat.v1.SummaryMetadata.PluginData\ntf.compat.v1.TFRecordReader\ntf.compat.v1.Tensor\ntf.compat.v1.TensorArray\ntf.compat.v1.TensorArraySpec\ntf.compat.v1.TensorInfo\ntf.compat.v1.TensorInfo.CompositeTensor\ntf.compat.v1.TensorInfo.CooSparse\ntf.compat.v1.TensorShape\ntf.compat.v1.TensorSpec\ntf.compat.v1.TextLineReader\ntf.compat.v1.TypeSpec\ntf.compat.v1.UnconnectedGradients\ntf.compat.v1.VarLenFeature\ntf.compat.v1.Variable\ntf.compat.v1.Variable.SaveSliceInfo\ntf.compat.v1.VariableAggregation\ntf.compat.v1.VariableScope\ntf.compat.v1.VariableSynchronization\ntf.compat.v1.WholeFileReader\ntf.compat.v1.abs\ntf.compat.v1.accumulate_n\ntf.compat.v1.acos\ntf.compat.v1.acosh\ntf.compat.v1.add\ntf.compat.v1.add_check_numerics_ops\ntf.compat.v1.add_n\ntf.compat.v1.add_to_collection\ntf.compat.v1.add_to_collections\ntf.compat.v1.all_variables\ntf.compat.v1.angle\ntf.compat.v1.app\ntf.compat.v1.app.run\ntf.compat.v1.approx_top_k\ntf.compat.v1.arg_max\ntf.compat.v1.arg_min\ntf.compat.v1.argmax\ntf.compat.v1.argmin\ntf.compat.v1.argsort\ntf.compat.v1.as_dtype\ntf.compat.v1.as_string\ntf.compat.v1.asin\ntf.compat.v1.asinh\ntf.compat.v1.assert_equal\ntf.compat.v1.assert_greater\ntf.compat.v1.assert_greater_equal\ntf.compat.v1.assert_integer\ntf.compat.v1.assert_less\ntf.compat.v1.assert_less_equal\ntf.compat.v1.assert_near\ntf.compat.v1.assert_negative\ntf.compat.v1.assert_non_negative\ntf.compat.v1.assert_non_positive\ntf.compat.v1.assert_none_equal\ntf.compat.v1.assert_positive\ntf.compat.v1.assert_proper_iterable\ntf.compat.v1.assert_rank\ntf.compat.v1.assert_rank_at_least\ntf.compat.v1.assert_rank_in\ntf.compat.v1.assert_same_float_dtype\ntf.compat.v1.assert_scalar\ntf.compat.v1.assert_type\ntf.compat.v1.assert_variables_initialized\ntf.compat.v1.assign\ntf.compat.v1.assign_add\ntf.compat.v1.assign_sub\ntf.compat.v1.atan\ntf.compat.v1.atan2\ntf.compat.v1.atanh\ntf.compat.v1.audio\ntf.compat.v1.audio.decode_wav\ntf.compat.v1.audio.encode_wav\ntf.compat.v1.autograph\ntf.compat.v1.autograph.experimental\ntf.compat.v1.autograph.experimental.Feature\ntf.compat.v1.autograph.experimental.do_not_convert\ntf.compat.v1.autograph.experimental.set_loop_options\ntf.compat.v1.autograph.set_verbosity\ntf.compat.v1.autograph.to_code\ntf.compat.v1.autograph.to_graph\ntf.compat.v1.autograph.trace\ntf.compat.v1.batch_gather\ntf.compat.v1.batch_scatter_update\ntf.compat.v1.batch_to_space\ntf.compat.v1.batch_to_space_nd\ntf.compat.v1.betainc\ntf.compat.v1.bincount\ntf.compat.v1.bitcast\ntf.compat.v1.bitwise\ntf.compat.v1.bitwise.bitwise_and\ntf.compat.v1.bitwise.bitwise_or\ntf.compat.v1.bitwise.bitwise_xor\ntf.compat.v1.bitwise.invert\ntf.compat.v1.bitwise.left_shift\ntf.compat.v1.bitwise.right_shift\ntf.compat.v1.boolean_mask\ntf.compat.v1.broadcast_dynamic_shape\ntf.compat.v1.broadcast_static_shape\ntf.compat.v1.broadcast_to\ntf.compat.v1.case\ntf.compat.v1.cast\ntf.compat.v1.ceil\ntf.compat.v1.check_numerics\ntf.compat.v1.cholesky\ntf.compat.v1.cholesky_solve\ntf.compat.v1.clip_by_average_norm\ntf.compat.v1.clip_by_global_norm\ntf.compat.v1.clip_by_norm\ntf.compat.v1.clip_by_value\ntf.compat.v1.colocate_with\ntf.compat.v1.compat\ntf.compat.v1.compat.as_bytes\ntf.compat.v1.compat.as_str\ntf.compat.v1.compat.as_str_any\ntf.compat.v1.compat.as_text\ntf.compat.v1.compat.dimension_at_index\ntf.compat.v1.compat.dimension_value\ntf.compat.v1.compat.forward_compatibility_horizon\ntf.compat.v1.compat.forward_compatible\ntf.compat.v1.compat.path_to_str\ntf.compat.v1.complex\ntf.compat.v1.concat\ntf.compat.v1.cond\ntf.compat.v1.config\ntf.compat.v1.config.LogicalDevice\ntf.compat.v1.config.LogicalDeviceConfiguration\ntf.compat.v1.config.PhysicalDevice\ntf.compat.v1.config.experimental\ntf.compat.v1.config.experimental.ClusterDeviceFilters\ntf.compat.v1.config.experimental.VirtualDeviceConfiguration\ntf.compat.v1.config.experimental.disable_mlir_bridge\ntf.compat.v1.config.experimental.enable_mlir_bridge\ntf.compat.v1.config.experimental.enable_tensor_float_32_execution\ntf.compat.v1.config.experimental.get_device_details\ntf.compat.v1.config.experimental.get_device_policy\ntf.compat.v1.config.experimental.get_memory_growth\ntf.compat.v1.config.experimental.get_memory_info\ntf.compat.v1.config.experimental.get_memory_usage\ntf.compat.v1.config.experimental.get_synchronous_execution\ntf.compat.v1.config.experimental.get_virtual_device_configuration\ntf.compat.v1.config.experimental.get_visible_devices\ntf.compat.v1.config.experimental.list_logical_devices\ntf.compat.v1.config.experimental.list_physical_devices\ntf.compat.v1.config.experimental.reset_memory_stats\ntf.compat.v1.config.experimental.set_device_policy\ntf.compat.v1.config.experimental.set_memory_growth\ntf.compat.v1.config.experimental.set_synchronous_execution\ntf.compat.v1.config.experimental.set_virtual_device_configuration\ntf.compat.v1.config.experimental.set_visible_devices\ntf.compat.v1.config.experimental.tensor_float_32_execution_enabled\ntf.compat.v1.config.experimental_connect_to_cluster\ntf.compat.v1.config.experimental_connect_to_host\ntf.compat.v1.config.experimental_functions_run_eagerly\ntf.compat.v1.config.experimental_run_functions_eagerly\ntf.compat.v1.config.functions_run_eagerly\ntf.compat.v1.config.get_logical_device_configuration\ntf.compat.v1.config.get_soft_device_placement\ntf.compat.v1.config.get_visible_devices\ntf.compat.v1.config.list_logical_devices\ntf.compat.v1.config.list_physical_devices\ntf.compat.v1.config.optimizer\ntf.compat.v1.config.optimizer.get_experimental_options\ntf.compat.v1.config.optimizer.get_jit\ntf.compat.v1.config.optimizer.set_experimental_options\ntf.compat.v1.config.optimizer.set_jit\ntf.compat.v1.config.run_functions_eagerly\ntf.compat.v1.config.set_logical_device_configuration\ntf.compat.v1.config.set_soft_device_placement\ntf.compat.v1.config.set_visible_devices\ntf.compat.v1.config.threading\ntf.compat.v1.config.threading.get_inter_op_parallelism_threads\ntf.compat.v1.config.threading.get_intra_op_parallelism_threads\ntf.compat.v1.config.threading.set_inter_op_parallelism_threads\ntf.compat.v1.config.threading.set_intra_op_parallelism_threads\ntf.compat.v1.confusion_matrix\ntf.compat.v1.conj\ntf.compat.v1.constant\ntf.compat.v1.constant_initializer\ntf.compat.v1.container\ntf.compat.v1.control_dependencies\ntf.compat.v1.control_flow_v2_enabled\ntf.compat.v1.conv\ntf.compat.v1.conv2d_backprop_filter_v2\ntf.compat.v1.conv2d_backprop_input_v2\ntf.compat.v1.convert_to_tensor\ntf.compat.v1.convert_to_tensor_or_indexed_slices\ntf.compat.v1.convert_to_tensor_or_sparse_tensor\ntf.compat.v1.cos\ntf.compat.v1.cosh\ntf.compat.v1.count_nonzero\ntf.compat.v1.count_up_to\ntf.compat.v1.create_partitioned_variables\ntf.compat.v1.cross\ntf.compat.v1.cumprod\ntf.compat.v1.cumsum\ntf.compat.v1.custom_gradient\ntf.compat.v1.data\ntf.compat.v1.data.Dataset\ntf.compat.v1.data.DatasetSpec\ntf.compat.v1.data.FixedLengthRecordDataset\ntf.compat.v1.data.Iterator\ntf.compat.v1.data.NumpyIterator\ntf.compat.v1.data.Options\ntf.compat.v1.data.TFRecordDataset\ntf.compat.v1.data.TextLineDataset\ntf.compat.v1.data.ThreadingOptions\ntf.compat.v1.data.experimental\ntf.compat.v1.data.experimental.AutoShardPolicy\ntf.compat.v1.data.experimental.AutotuneAlgorithm\ntf.compat.v1.data.experimental.AutotuneOptions\ntf.compat.v1.data.experimental.Counter\ntf.compat.v1.data.experimental.CsvDataset\ntf.compat.v1.data.experimental.DatasetInitializer\ntf.compat.v1.data.experimental.DatasetStructure\ntf.compat.v1.data.experimental.DistributeOptions\ntf.compat.v1.data.experimental.ExternalStatePolicy\ntf.compat.v1.data.experimental.OptimizationOptions\ntf.compat.v1.data.experimental.Optional\ntf.compat.v1.data.experimental.OptionalStructure\ntf.compat.v1.data.experimental.RaggedTensorStructure\ntf.compat.v1.data.experimental.RandomDataset\ntf.compat.v1.data.experimental.Reducer\ntf.compat.v1.data.experimental.SparseTensorStructure\ntf.compat.v1.data.experimental.SqlDataset\ntf.compat.v1.data.experimental.Structure\ntf.compat.v1.data.experimental.TFRecordWriter\ntf.compat.v1.data.experimental.TensorArrayStructure\ntf.compat.v1.data.experimental.TensorStructure\ntf.compat.v1.data.experimental.ThreadingOptions\ntf.compat.v1.data.experimental.assert_cardinality\ntf.compat.v1.data.experimental.bucket_by_sequence_length\ntf.compat.v1.data.experimental.cardinality\ntf.compat.v1.data.experimental.choose_from_datasets\ntf.compat.v1.data.experimental.copy_to_device\ntf.compat.v1.data.experimental.dense_to_ragged_batch\ntf.compat.v1.data.experimental.dense_to_sparse_batch\ntf.compat.v1.data.experimental.enable_debug_mode\ntf.compat.v1.data.experimental.enumerate_dataset\ntf.compat.v1.data.experimental.from_list\ntf.compat.v1.data.experimental.from_variant\ntf.compat.v1.data.experimental.get_next_as_optional\ntf.compat.v1.data.experimental.get_single_element\ntf.compat.v1.data.experimental.get_structure\ntf.compat.v1.data.experimental.group_by_reducer\ntf.compat.v1.data.experimental.group_by_window\ntf.compat.v1.data.experimental.ignore_errors\ntf.compat.v1.data.experimental.index_table_from_dataset\ntf.compat.v1.data.experimental.make_batched_features_dataset\ntf.compat.v1.data.experimental.make_csv_dataset\ntf.compat.v1.data.experimental.make_saveable_from_iterator\ntf.compat.v1.data.experimental.map_and_batch\ntf.compat.v1.data.experimental.map_and_batch_with_legacy_function\ntf.compat.v1.data.experimental.pad_to_cardinality\ntf.compat.v1.data.experimental.parallel_interleave\ntf.compat.v1.data.experimental.parse_example_dataset\ntf.compat.v1.data.experimental.prefetch_to_device\ntf.compat.v1.data.experimental.rejection_resample\ntf.compat.v1.data.experimental.sample_from_datasets\ntf.compat.v1.data.experimental.scan\ntf.compat.v1.data.experimental.service\ntf.compat.v1.data.experimental.service.CrossTrainerCache\ntf.compat.v1.data.experimental.service.DispatcherConfig\ntf.compat.v1.data.experimental.service.ShardingPolicy\ntf.compat.v1.data.experimental.service.WorkerConfig\ntf.compat.v1.data.experimental.service.distribute\ntf.compat.v1.data.experimental.service.from_dataset_id\ntf.compat.v1.data.experimental.service.register_dataset\ntf.compat.v1.data.experimental.shuffle_and_repeat\ntf.compat.v1.data.experimental.snapshot\ntf.compat.v1.data.experimental.table_from_dataset\ntf.compat.v1.data.experimental.take_while\ntf.compat.v1.data.experimental.to_variant\ntf.compat.v1.data.experimental.unbatch\ntf.compat.v1.data.experimental.unique\ntf.compat.v1.data.get_output_classes\ntf.compat.v1.data.get_output_shapes\ntf.compat.v1.data.get_output_types\ntf.compat.v1.data.make_initializable_iterator\ntf.compat.v1.data.make_one_shot_iterator\ntf.compat.v1.debugging\ntf.compat.v1.debugging.Assert\ntf.compat.v1.debugging.assert_all_finite\ntf.compat.v1.debugging.assert_equal\ntf.compat.v1.debugging.assert_greater\ntf.compat.v1.debugging.assert_greater_equal\ntf.compat.v1.debugging.assert_integer\ntf.compat.v1.debugging.assert_less\ntf.compat.v1.debugging.assert_less_equal\ntf.compat.v1.debugging.assert_near\ntf.compat.v1.debugging.assert_negative\ntf.compat.v1.debugging.assert_non_negative\ntf.compat.v1.debugging.assert_non_positive\ntf.compat.v1.debugging.assert_none_equal\ntf.compat.v1.debugging.assert_positive\ntf.compat.v1.debugging.assert_proper_iterable\ntf.compat.v1.debugging.assert_rank\ntf.compat.v1.debugging.assert_rank_at_least\ntf.compat.v1.debugging.assert_rank_in\ntf.compat.v1.debugging.assert_same_float_dtype\ntf.compat.v1.debugging.assert_scalar\ntf.compat.v1.debugging.assert_shapes\ntf.compat.v1.debugging.assert_type\ntf.compat.v1.debugging.check_numerics\ntf.compat.v1.debugging.disable_check_numerics\ntf.compat.v1.debugging.disable_traceback_filtering\ntf.compat.v1.debugging.enable_check_numerics\ntf.compat.v1.debugging.enable_traceback_filtering\ntf.compat.v1.debugging.experimental\ntf.compat.v1.debugging.experimental.disable_dump_debug_info\ntf.compat.v1.debugging.experimental.enable_dump_debug_info\ntf.compat.v1.debugging.get_log_device_placement\ntf.compat.v1.debugging.is_finite\ntf.compat.v1.debugging.is_inf\ntf.compat.v1.debugging.is_nan\ntf.compat.v1.debugging.is_non_decreasing\ntf.compat.v1.debugging.is_numeric_tensor\ntf.compat.v1.debugging.is_strictly_increasing\ntf.compat.v1.debugging.is_traceback_filtering_enabled\ntf.compat.v1.debugging.set_log_device_placement\ntf.compat.v1.decode_base64\ntf.compat.v1.decode_compressed\ntf.compat.v1.decode_csv\ntf.compat.v1.decode_json_example\ntf.compat.v1.decode_raw\ntf.compat.v1.delete_session_tensor\ntf.compat.v1.depth_to_space\ntf.compat.v1.dequantize\ntf.compat.v1.deserialize_many_sparse\ntf.compat.v1.device\ntf.compat.v1.diag\ntf.compat.v1.diag_part\ntf.compat.v1.digamma\ntf.compat.v1.dimension_at_index\ntf.compat.v1.dimension_value\ntf.compat.v1.disable_control_flow_v2\ntf.compat.v1.disable_eager_execution\ntf.compat.v1.disable_resource_variables\ntf.compat.v1.disable_tensor_equality\ntf.compat.v1.disable_v2_behavior\ntf.compat.v1.disable_v2_tensorshape\ntf.compat.v1.distribute\ntf.compat.v1.distribute.CrossDeviceOps\ntf.compat.v1.distribute.HierarchicalCopyAllReduce\ntf.compat.v1.distribute.InputContext\ntf.compat.v1.distribute.InputReplicationMode\ntf.compat.v1.distribute.MirroredStrategy\ntf.compat.v1.distribute.NcclAllReduce\ntf.compat.v1.distribute.OneDeviceStrategy\ntf.compat.v1.distribute.ReduceOp\ntf.compat.v1.distribute.ReductionToOneDevice\ntf.compat.v1.distribute.ReplicaContext\ntf.compat.v1.distribute.RunOptions\ntf.compat.v1.distribute.Server\ntf.compat.v1.distribute.Strategy\ntf.compat.v1.distribute.StrategyExtended\ntf.compat.v1.distribute.cluster_resolver\ntf.compat.v1.distribute.cluster_resolver.ClusterResolver\ntf.compat.v1.distribute.cluster_resolver.GCEClusterResolver\ntf.compat.v1.distribute.cluster_resolver.KubernetesClusterResolver\ntf.compat.v1.distribute.cluster_resolver.SimpleClusterResolver\ntf.compat.v1.distribute.cluster_resolver.SlurmClusterResolver\ntf.compat.v1.distribute.cluster_resolver.TFConfigClusterResolver\ntf.compat.v1.distribute.cluster_resolver.TPUClusterResolver\ntf.compat.v1.distribute.cluster_resolver.UnionResolver\ntf.compat.v1.distribute.experimental\ntf.compat.v1.distribute.experimental.CentralStorageStrategy\ntf.compat.v1.distribute.experimental.CollectiveCommunication\ntf.compat.v1.distribute.experimental.CollectiveHints\ntf.compat.v1.distribute.experimental.CommunicationImplementation\ntf.compat.v1.distribute.experimental.CommunicationOptions\ntf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy\ntf.compat.v1.distribute.experimental.ParameterServerStrategy\ntf.compat.v1.distribute.experimental.TPUStrategy\ntf.compat.v1.distribute.experimental_set_strategy\ntf.compat.v1.distribute.get_loss_reduction\ntf.compat.v1.distribute.get_replica_context\ntf.compat.v1.distribute.get_strategy\ntf.compat.v1.distribute.has_strategy\ntf.compat.v1.distribute.in_cross_replica_context\ntf.compat.v1.distributions\ntf.compat.v1.distributions.Bernoulli\ntf.compat.v1.distributions.Beta\ntf.compat.v1.distributions.Categorical\ntf.compat.v1.distributions.Dirichlet\ntf.compat.v1.distributions.DirichletMultinomial\ntf.compat.v1.distributions.Distribution\ntf.compat.v1.distributions.Exponential\ntf.compat.v1.distributions.Gamma\ntf.compat.v1.distributions.Laplace\ntf.compat.v1.distributions.Multinomial\ntf.compat.v1.distributions.Normal\ntf.compat.v1.distributions.RegisterKL\ntf.compat.v1.distributions.ReparameterizationType\ntf.compat.v1.distributions.StudentT\ntf.compat.v1.distributions.Uniform\ntf.compat.v1.distributions.kl_divergence\ntf.compat.v1.div\ntf.compat.v1.div_no_nan\ntf.compat.v1.divide\ntf.compat.v1.dtypes\ntf.compat.v1.dtypes.DType\ntf.compat.v1.dtypes.as_dtype\ntf.compat.v1.dtypes.as_string\ntf.compat.v1.dtypes.cast\ntf.compat.v1.dtypes.complex\ntf.compat.v1.dtypes.experimental\ntf.compat.v1.dtypes.saturate_cast\ntf.compat.v1.dynamic_partition\ntf.compat.v1.dynamic_stitch\ntf.compat.v1.edit_distance\ntf.compat.v1.einsum\ntf.compat.v1.enable_control_flow_v2\ntf.compat.v1.enable_eager_execution\ntf.compat.v1.enable_resource_variables\ntf.compat.v1.enable_tensor_equality\ntf.compat.v1.enable_v2_behavior\ntf.compat.v1.enable_v2_tensorshape\ntf.compat.v1.encode_base64\ntf.compat.v1.ensure_shape\ntf.compat.v1.equal\ntf.compat.v1.erf\ntf.compat.v1.erfc\ntf.compat.v1.errors\ntf.compat.v1.errors.AbortedError\ntf.compat.v1.errors.AlreadyExistsError\ntf.compat.v1.errors.CancelledError\ntf.compat.v1.errors.DataLossError\ntf.compat.v1.errors.DeadlineExceededError\ntf.compat.v1.errors.FailedPreconditionError\ntf.compat.v1.errors.InternalError\ntf.compat.v1.errors.InvalidArgumentError\ntf.compat.v1.errors.NotFoundError\ntf.compat.v1.errors.OpError\ntf.compat.v1.errors.OutOfRangeError\ntf.compat.v1.errors.PermissionDeniedError\ntf.compat.v1.errors.ResourceExhaustedError\ntf.compat.v1.errors.UnauthenticatedError\ntf.compat.v1.errors.UnavailableError\ntf.compat.v1.errors.UnimplementedError\ntf.compat.v1.errors.UnknownError\ntf.compat.v1.errors.error_code_from_exception_type\ntf.compat.v1.errors.exception_type_from_error_code\ntf.compat.v1.errors.raise_exception_on_not_ok_status\ntf.compat.v1.executing_eagerly\ntf.compat.v1.executing_eagerly_outside_functions\ntf.compat.v1.exp\ntf.compat.v1.expand_dims\ntf.compat.v1.experimental\ntf.compat.v1.experimental.BatchableExtensionType\ntf.compat.v1.experimental.DynamicRaggedShape\ntf.compat.v1.experimental.DynamicRaggedShape.Spec\ntf.compat.v1.experimental.ExtensionType\ntf.compat.v1.experimental.ExtensionTypeBatchEncoder\ntf.compat.v1.experimental.ExtensionTypeSpec\ntf.compat.v1.experimental.Optional\ntf.compat.v1.experimental.RowPartition\ntf.compat.v1.experimental.StructuredTensor\ntf.compat.v1.experimental.StructuredTensor.FieldName\ntf.compat.v1.experimental.StructuredTensor.Spec\ntf.compat.v1.experimental.async_clear_error\ntf.compat.v1.experimental.async_scope\ntf.compat.v1.experimental.dispatch_for_api\ntf.compat.v1.experimental.dispatch_for_binary_elementwise_apis\ntf.compat.v1.experimental.dispatch_for_binary_elementwise_assert_apis\ntf.compat.v1.experimental.dispatch_for_unary_elementwise_apis\ntf.compat.v1.experimental.enable_strict_mode\ntf.compat.v1.experimental.extension_type\ntf.compat.v1.experimental.extension_type.as_dict\ntf.compat.v1.experimental.function_executor_type\ntf.compat.v1.experimental.output_all_intermediates\ntf.compat.v1.experimental.register_filesystem_plugin\ntf.compat.v1.experimental.unregister_dispatch_for\ntf.compat.v1.expm1\ntf.compat.v1.extract_image_patches\ntf.compat.v1.extract_volume_patches\ntf.compat.v1.eye\ntf.compat.v1.fake_quant_with_min_max_args\ntf.compat.v1.fake_quant_with_min_max_args_gradient\ntf.compat.v1.fake_quant_with_min_max_vars\ntf.compat.v1.fake_quant_with_min_max_vars_gradient\ntf.compat.v1.fake_quant_with_min_max_vars_per_channel\ntf.compat.v1.fake_quant_with_min_max_vars_per_channel_gradient\ntf.compat.v1.feature_column\ntf.compat.v1.feature_column.bucketized_column\ntf.compat.v1.feature_column.categorical_column_with_hash_bucket\ntf.compat.v1.feature_column.categorical_column_with_identity\ntf.compat.v1.feature_column.categorical_column_with_vocabulary_file\ntf.compat.v1.feature_column.categorical_column_with_vocabulary_list\ntf.compat.v1.feature_column.crossed_column\ntf.compat.v1.feature_column.embedding_column\ntf.compat.v1.feature_column.indicator_column\ntf.compat.v1.feature_column.input_layer\ntf.compat.v1.feature_column.linear_model\ntf.compat.v1.feature_column.make_parse_example_spec\ntf.compat.v1.feature_column.numeric_column\ntf.compat.v1.feature_column.sequence_categorical_column_with_hash_bucket\ntf.compat.v1.feature_column.sequence_categorical_column_with_identity\ntf.compat.v1.feature_column.sequence_categorical_column_with_vocabulary_file\ntf.compat.v1.feature_column.sequence_categorical_column_with_vocabulary_list\ntf.compat.v1.feature_column.sequence_numeric_column\ntf.compat.v1.feature_column.shared_embedding_columns\ntf.compat.v1.feature_column.weighted_categorical_column\ntf.compat.v1.fft\ntf.compat.v1.fft2d\ntf.compat.v1.fft3d\ntf.compat.v1.fftnd\ntf.compat.v1.fill\ntf.compat.v1.fingerprint\ntf.compat.v1.fixed_size_partitioner\ntf.compat.v1.flags\ntf.compat.v1.flags.ArgumentParser\ntf.compat.v1.flags.ArgumentSerializer\ntf.compat.v1.flags.BaseListParser\ntf.compat.v1.flags.BooleanFlag\ntf.compat.v1.flags.BooleanParser\ntf.compat.v1.flags.CantOpenFlagFileError\ntf.compat.v1.flags.CsvListSerializer\ntf.compat.v1.flags.DEFINE\ntf.compat.v1.flags.DEFINE_alias\ntf.compat.v1.flags.DEFINE_bool\ntf.compat.v1.flags.DEFINE_boolean\ntf.compat.v1.flags.DEFINE_enum\ntf.compat.v1.flags.DEFINE_enum_class\ntf.compat.v1.flags.DEFINE_flag\ntf.compat.v1.flags.DEFINE_float\ntf.compat.v1.flags.DEFINE_integer\ntf.compat.v1.flags.DEFINE_list\ntf.compat.v1.flags.DEFINE_multi\ntf.compat.v1.flags.DEFINE_multi_enum\ntf.compat.v1.flags.DEFINE_multi_enum_class\ntf.compat.v1.flags.DEFINE_multi_float\ntf.compat.v1.flags.DEFINE_multi_integer\ntf.compat.v1.flags.DEFINE_multi_string\ntf.compat.v1.flags.DEFINE_spaceseplist\ntf.compat.v1.flags.DEFINE_string\ntf.compat.v1.flags.DuplicateFlagError\ntf.compat.v1.flags.EnumClassFlag\ntf.compat.v1.flags.EnumClassListSerializer\ntf.compat.v1.flags.EnumClassParser\ntf.compat.v1.flags.EnumClassSerializer\ntf.compat.v1.flags.EnumFlag\ntf.compat.v1.flags.EnumParser\ntf.compat.v1.flags.Error\ntf.compat.v1.flags.FLAGS\ntf.compat.v1.flags.Flag\ntf.compat.v1.flags.FlagHolder\ntf.compat.v1.flags.FlagNameConflictsWithMethodError\ntf.compat.v1.flags.FlagValues\ntf.compat.v1.flags.FloatParser\ntf.compat.v1.flags.IllegalFlagValueError\ntf.compat.v1.flags.IntegerParser\ntf.compat.v1.flags.ListParser\ntf.compat.v1.flags.ListSerializer\ntf.compat.v1.flags.MultiEnumClassFlag\ntf.compat.v1.flags.MultiFlag\ntf.compat.v1.flags.UnparsedFlagAccessError\ntf.compat.v1.flags.UnrecognizedFlagError\ntf.compat.v1.flags.ValidationError\ntf.compat.v1.flags.WhitespaceSeparatedListParser\ntf.compat.v1.flags.adopt_module_key_flags\ntf.compat.v1.flags.declare_key_flag\ntf.compat.v1.flags.disclaim_key_flags\ntf.compat.v1.flags.doc_to_help\ntf.compat.v1.flags.flag_dict_to_args\ntf.compat.v1.flags.get_help_width\ntf.compat.v1.flags.mark_bool_flags_as_mutual_exclusive\ntf.compat.v1.flags.mark_flag_as_required\ntf.compat.v1.flags.mark_flags_as_mutual_exclusive\ntf.compat.v1.flags.mark_flags_as_required\ntf.compat.v1.flags.multi_flags_validator\ntf.compat.v1.flags.override_value\ntf.compat.v1.flags.register_multi_flags_validator\ntf.compat.v1.flags.register_validator\ntf.compat.v1.flags.set_default\ntf.compat.v1.flags.text_wrap\ntf.compat.v1.flags.validator\ntf.compat.v1.floor\ntf.compat.v1.floor_div\ntf.compat.v1.floordiv\ntf.compat.v1.floormod\ntf.compat.v1.foldl\ntf.compat.v1.foldr\ntf.compat.v1.function\ntf.compat.v1.gather\ntf.compat.v1.gather_nd\ntf.compat.v1.get_collection\ntf.compat.v1.get_collection_ref\ntf.compat.v1.get_default_graph\ntf.compat.v1.get_default_session\ntf.compat.v1.get_local_variable\ntf.compat.v1.get_logger\ntf.compat.v1.get_seed\ntf.compat.v1.get_session_handle\ntf.compat.v1.get_session_tensor\ntf.compat.v1.get_static_value\ntf.compat.v1.get_variable\ntf.compat.v1.get_variable_scope\ntf.compat.v1.gfile\ntf.compat.v1.gfile.Copy\ntf.compat.v1.gfile.DeleteRecursively\ntf.compat.v1.gfile.Exists\ntf.compat.v1.gfile.FastGFile\ntf.compat.v1.gfile.GFile\ntf.compat.v1.gfile.Glob\ntf.compat.v1.gfile.IsDirectory\ntf.compat.v1.gfile.ListDirectory\ntf.compat.v1.gfile.MakeDirs\ntf.compat.v1.gfile.MkDir\ntf.compat.v1.gfile.Open\ntf.compat.v1.gfile.Remove\ntf.compat.v1.gfile.Rename\ntf.compat.v1.gfile.Stat\ntf.compat.v1.gfile.Walk\ntf.compat.v1.global_norm\ntf.compat.v1.global_variables\ntf.compat.v1.global_variables_initializer\ntf.compat.v1.glorot_normal_initializer\ntf.compat.v1.glorot_uniform_initializer\ntf.compat.v1.grad_pass_through\ntf.compat.v1.gradients\ntf.compat.v1.graph_util\ntf.compat.v1.graph_util.convert_variables_to_constants\ntf.compat.v1.graph_util.extract_sub_graph\ntf.compat.v1.graph_util.import_graph_def\ntf.compat.v1.graph_util.must_run_on_cpu\ntf.compat.v1.graph_util.remove_training_nodes\ntf.compat.v1.graph_util.tensor_shape_from_node_def_name\ntf.compat.v1.greater\ntf.compat.v1.greater_equal\ntf.compat.v1.group\ntf.compat.v1.guarantee_const\ntf.compat.v1.hessians\ntf.compat.v1.histogram_fixed_width\ntf.compat.v1.histogram_fixed_width_bins\ntf.compat.v1.identity\ntf.compat.v1.identity_n\ntf.compat.v1.ifft\ntf.compat.v1.ifft2d\ntf.compat.v1.ifft3d\ntf.compat.v1.ifftnd\ntf.compat.v1.igamma\ntf.compat.v1.igammac\ntf.compat.v1.imag\ntf.compat.v1.image\ntf.compat.v1.image.ResizeMethod\ntf.compat.v1.image.adjust_brightness\ntf.compat.v1.image.adjust_contrast\ntf.compat.v1.image.adjust_gamma\ntf.compat.v1.image.adjust_hue\ntf.compat.v1.image.adjust_jpeg_quality\ntf.compat.v1.image.adjust_saturation\ntf.compat.v1.image.central_crop\ntf.compat.v1.image.combined_non_max_suppression\ntf.compat.v1.image.convert_image_dtype\ntf.compat.v1.image.crop_and_resize\ntf.compat.v1.image.crop_to_bounding_box\ntf.compat.v1.image.decode_and_crop_jpeg\ntf.compat.v1.image.decode_bmp\ntf.compat.v1.image.decode_gif\ntf.compat.v1.image.decode_image\ntf.compat.v1.image.decode_jpeg\ntf.compat.v1.image.decode_png\ntf.compat.v1.image.draw_bounding_boxes\ntf.compat.v1.image.encode_jpeg\ntf.compat.v1.image.encode_png\ntf.compat.v1.image.extract_glimpse\ntf.compat.v1.image.extract_image_patches\ntf.compat.v1.image.extract_jpeg_shape\ntf.compat.v1.image.extract_patches\ntf.compat.v1.image.flip_left_right\ntf.compat.v1.image.flip_up_down\ntf.compat.v1.image.generate_bounding_box_proposals\ntf.compat.v1.image.grayscale_to_rgb\ntf.compat.v1.image.hsv_to_rgb\ntf.compat.v1.image.image_gradients\ntf.compat.v1.image.is_jpeg\ntf.compat.v1.image.non_max_suppression\ntf.compat.v1.image.non_max_suppression_overlaps\ntf.compat.v1.image.non_max_suppression_padded\ntf.compat.v1.image.non_max_suppression_with_scores\ntf.compat.v1.image.pad_to_bounding_box\ntf.compat.v1.image.per_image_standardization\ntf.compat.v1.image.psnr\ntf.compat.v1.image.random_brightness\ntf.compat.v1.image.random_contrast\ntf.compat.v1.image.random_crop\ntf.compat.v1.image.random_flip_left_right\ntf.compat.v1.image.random_flip_up_down\ntf.compat.v1.image.random_hue\ntf.compat.v1.image.random_jpeg_quality\ntf.compat.v1.image.random_saturation\ntf.compat.v1.image.resize\ntf.compat.v1.image.resize_area\ntf.compat.v1.image.resize_bicubic\ntf.compat.v1.image.resize_bilinear\ntf.compat.v1.image.resize_image_with_crop_or_pad\ntf.compat.v1.image.resize_image_with_pad\ntf.compat.v1.image.resize_images\ntf.compat.v1.image.resize_nearest_neighbor\ntf.compat.v1.image.resize_with_crop_or_pad\ntf.compat.v1.image.rgb_to_grayscale\ntf.compat.v1.image.rgb_to_hsv\ntf.compat.v1.image.rgb_to_yiq\ntf.compat.v1.image.rgb_to_yuv\ntf.compat.v1.image.rot90\ntf.compat.v1.image.sample_distorted_bounding_box\ntf.compat.v1.image.sobel_edges\ntf.compat.v1.image.ssim\ntf.compat.v1.image.ssim_multiscale\ntf.compat.v1.image.total_variation\ntf.compat.v1.image.transpose\ntf.compat.v1.image.transpose_image\ntf.compat.v1.image.yiq_to_rgb\ntf.compat.v1.image.yuv_to_rgb\ntf.compat.v1.import_graph_def\ntf.compat.v1.init_scope\ntf.compat.v1.initialize_all_tables\ntf.compat.v1.initialize_all_variables\ntf.compat.v1.initialize_local_variables\ntf.compat.v1.initialize_variables\ntf.compat.v1.initializers\ntf.compat.v1.initializers.constant\ntf.compat.v1.initializers.global_variables\ntf.compat.v1.initializers.glorot_normal\ntf.compat.v1.initializers.glorot_uniform\ntf.compat.v1.initializers.he_normal\ntf.compat.v1.initializers.he_uniform\ntf.compat.v1.initializers.identity\ntf.compat.v1.initializers.lecun_normal\ntf.compat.v1.initializers.lecun_uniform\ntf.compat.v1.initializers.local_variables\ntf.compat.v1.initializers.ones\ntf.compat.v1.initializers.orthogonal\ntf.compat.v1.initializers.random_normal\ntf.compat.v1.initializers.random_uniform\ntf.compat.v1.initializers.tables_initializer\ntf.compat.v1.initializers.truncated_normal\ntf.compat.v1.initializers.uniform_unit_scaling\ntf.compat.v1.initializers.variables\ntf.compat.v1.initializers.variance_scaling\ntf.compat.v1.initializers.zeros\ntf.compat.v1.invert_permutation\ntf.compat.v1.io\ntf.compat.v1.io.FixedLenFeature\ntf.compat.v1.io.FixedLenSequenceFeature\ntf.compat.v1.io.PaddingFIFOQueue\ntf.compat.v1.io.PriorityQueue\ntf.compat.v1.io.QueueBase\ntf.compat.v1.io.RaggedFeature\ntf.compat.v1.io.RaggedFeature.RowLengths\ntf.compat.v1.io.RaggedFeature.RowLimits\ntf.compat.v1.io.RaggedFeature.RowSplits\ntf.compat.v1.io.RaggedFeature.RowStarts\ntf.compat.v1.io.RaggedFeature.UniformRowLength\ntf.compat.v1.io.RaggedFeature.ValueRowIds\ntf.compat.v1.io.RandomShuffleQueue\ntf.compat.v1.io.SparseFeature\ntf.compat.v1.io.TFRecordCompressionType\ntf.compat.v1.io.TFRecordOptions\ntf.compat.v1.io.TFRecordWriter\ntf.compat.v1.io.VarLenFeature\ntf.compat.v1.io.decode_and_crop_jpeg\ntf.compat.v1.io.decode_base64\ntf.compat.v1.io.decode_bmp\ntf.compat.v1.io.decode_compressed\ntf.compat.v1.io.decode_csv\ntf.compat.v1.io.decode_gif\ntf.compat.v1.io.decode_image\ntf.compat.v1.io.decode_jpeg\ntf.compat.v1.io.decode_json_example\ntf.compat.v1.io.decode_png\ntf.compat.v1.io.decode_proto\ntf.compat.v1.io.decode_raw\ntf.compat.v1.io.deserialize_many_sparse\ntf.compat.v1.io.encode_base64\ntf.compat.v1.io.encode_jpeg\ntf.compat.v1.io.encode_png\ntf.compat.v1.io.encode_proto\ntf.compat.v1.io.extract_jpeg_shape\ntf.compat.v1.io.gfile\ntf.compat.v1.io.gfile.GFile\ntf.compat.v1.io.gfile.copy\ntf.compat.v1.io.gfile.exists\ntf.compat.v1.io.gfile.get_registered_schemes\ntf.compat.v1.io.gfile.glob\ntf.compat.v1.io.gfile.isdir\ntf.compat.v1.io.gfile.join\ntf.compat.v1.io.gfile.listdir\ntf.compat.v1.io.gfile.makedirs\ntf.compat.v1.io.gfile.mkdir\ntf.compat.v1.io.gfile.remove\ntf.compat.v1.io.gfile.rename\ntf.compat.v1.io.gfile.rmtree\ntf.compat.v1.io.gfile.stat\ntf.compat.v1.io.gfile.walk\ntf.compat.v1.io.is_jpeg\ntf.compat.v1.io.match_filenames_once\ntf.compat.v1.io.matching_files\ntf.compat.v1.io.parse_example\ntf.compat.v1.io.parse_sequence_example\ntf.compat.v1.io.parse_single_example\ntf.compat.v1.io.parse_single_sequence_example\ntf.compat.v1.io.parse_tensor\ntf.compat.v1.io.read_file\ntf.compat.v1.io.serialize_many_sparse\ntf.compat.v1.io.serialize_sparse\ntf.compat.v1.io.serialize_tensor\ntf.compat.v1.io.tf_record_iterator\ntf.compat.v1.io.write_file\ntf.compat.v1.io.write_graph\ntf.compat.v1.irfftnd\ntf.compat.v1.is_finite\ntf.compat.v1.is_inf\ntf.compat.v1.is_nan\ntf.compat.v1.is_non_decreasing\ntf.compat.v1.is_numeric_tensor\ntf.compat.v1.is_strictly_increasing\ntf.compat.v1.is_symbolic_tensor\ntf.compat.v1.is_tensor\ntf.compat.v1.is_variable_initialized\ntf.compat.v1.keras\ntf.compat.v1.keras.AbsMaxQuantizer\ntf.compat.v1.keras.DTypePolicy\ntf.compat.v1.keras.FloatDTypePolicy\ntf.compat.v1.keras.Function\ntf.compat.v1.keras.Initializer\ntf.compat.v1.keras.Input\ntf.compat.v1.keras.InputSpec\ntf.compat.v1.keras.KerasTensor\ntf.compat.v1.keras.Layer\ntf.compat.v1.keras.Loss\ntf.compat.v1.keras.Metric\ntf.compat.v1.keras.Model\ntf.compat.v1.keras.Operation\ntf.compat.v1.keras.Optimizer\ntf.compat.v1.keras.QuantizedDTypePolicy\ntf.compat.v1.keras.Quantizer\ntf.compat.v1.keras.Regularizer\ntf.compat.v1.keras.Sequential\ntf.compat.v1.keras.StatelessScope\ntf.compat.v1.keras.Variable\ntf.compat.v1.keras.device\ntf.compat.v1.keras.name_scope\ntf.compat.v1.keras.version\ntf.compat.v1.layers\ntf.compat.v1.lbeta\ntf.compat.v1.less\ntf.compat.v1.less_equal\ntf.compat.v1.lgamma\ntf.compat.v1.lin_space\ntf.compat.v1.linalg\ntf.compat.v1.linalg.LinearOperator\ntf.compat.v1.linalg.LinearOperatorAdjoint\ntf.compat.v1.linalg.LinearOperatorBlockDiag\ntf.compat.v1.linalg.LinearOperatorBlockLowerTriangular\ntf.compat.v1.linalg.LinearOperatorCirculant\ntf.compat.v1.linalg.LinearOperatorCirculant2D\ntf.compat.v1.linalg.LinearOperatorCirculant3D\ntf.compat.v1.linalg.LinearOperatorComposition\ntf.compat.v1.linalg.LinearOperatorDiag\ntf.compat.v1.linalg.LinearOperatorFullMatrix\ntf.compat.v1.linalg.LinearOperatorHouseholder\ntf.compat.v1.linalg.LinearOperatorIdentity\ntf.compat.v1.linalg.LinearOperatorInversion\ntf.compat.v1.linalg.LinearOperatorKronecker\ntf.compat.v1.linalg.LinearOperatorLowRankUpdate\ntf.compat.v1.linalg.LinearOperatorLowerTriangular\ntf.compat.v1.linalg.LinearOperatorPermutation\ntf.compat.v1.linalg.LinearOperatorScaledIdentity\ntf.compat.v1.linalg.LinearOperatorToeplitz\ntf.compat.v1.linalg.LinearOperatorTridiag\ntf.compat.v1.linalg.LinearOperatorZeros\ntf.compat.v1.linalg.adjoint\ntf.compat.v1.linalg.band_part\ntf.compat.v1.linalg.cholesky\ntf.compat.v1.linalg.cholesky_solve\ntf.compat.v1.linalg.cross\ntf.compat.v1.linalg.det\ntf.compat.v1.linalg.diag\ntf.compat.v1.linalg.diag_part\ntf.compat.v1.linalg.eigh\ntf.compat.v1.linalg.eigh_tridiagonal\ntf.compat.v1.linalg.eigvalsh\ntf.compat.v1.linalg.einsum\ntf.compat.v1.linalg.experimental\ntf.compat.v1.linalg.experimental.conjugate_gradient\ntf.compat.v1.linalg.expm\ntf.compat.v1.linalg.eye\ntf.compat.v1.linalg.global_norm\ntf.compat.v1.linalg.inv\ntf.compat.v1.linalg.l2_normalize\ntf.compat.v1.linalg.logdet\ntf.compat.v1.linalg.logm\ntf.compat.v1.linalg.lstsq\ntf.compat.v1.linalg.lu\ntf.compat.v1.linalg.lu_matrix_inverse\ntf.compat.v1.linalg.lu_reconstruct\ntf.compat.v1.linalg.lu_solve\ntf.compat.v1.linalg.matmul\ntf.compat.v1.linalg.matrix_rank\ntf.compat.v1.linalg.matrix_transpose\ntf.compat.v1.linalg.matvec\ntf.compat.v1.linalg.norm\ntf.compat.v1.linalg.normalize\ntf.compat.v1.linalg.pinv\ntf.compat.v1.linalg.qr\ntf.compat.v1.linalg.set_diag\ntf.compat.v1.linalg.slogdet\ntf.compat.v1.linalg.solve\ntf.compat.v1.linalg.sqrtm\ntf.compat.v1.linalg.svd\ntf.compat.v1.linalg.tensor_diag\ntf.compat.v1.linalg.tensor_diag_part\ntf.compat.v1.linalg.tensordot\ntf.compat.v1.linalg.trace\ntf.compat.v1.linalg.transpose\ntf.compat.v1.linalg.triangular_solve\ntf.compat.v1.linalg.tridiagonal_matmul\ntf.compat.v1.linalg.tridiagonal_solve\ntf.compat.v1.linspace\ntf.compat.v1.lite\ntf.compat.v1.lite.Interpreter\ntf.compat.v1.lite.OpHint\ntf.compat.v1.lite.OpHint.OpHintArgumentTracker\ntf.compat.v1.lite.OpsSet\ntf.compat.v1.lite.Optimize\ntf.compat.v1.lite.RepresentativeDataset\ntf.compat.v1.lite.TFLiteConverter\ntf.compat.v1.lite.TargetSpec\ntf.compat.v1.lite.TocoConverter\ntf.compat.v1.lite.constants\ntf.compat.v1.lite.experimental\ntf.compat.v1.lite.experimental.Analyzer\ntf.compat.v1.lite.experimental.OpResolverType\ntf.compat.v1.lite.experimental.QuantizationDebugOptions\ntf.compat.v1.lite.experimental.QuantizationDebugger\ntf.compat.v1.lite.experimental.authoring\ntf.compat.v1.lite.experimental.authoring.compatible\ntf.compat.v1.lite.experimental.convert_op_hints_to_stubs\ntf.compat.v1.lite.experimental.load_delegate\ntf.compat.v1.lite.toco_convert\ntf.compat.v1.load_file_system_library\ntf.compat.v1.load_library\ntf.compat.v1.load_op_library\ntf.compat.v1.local_variables\ntf.compat.v1.local_variables_initializer\ntf.compat.v1.log\ntf.compat.v1.log1p\ntf.compat.v1.log_sigmoid\ntf.compat.v1.logging\ntf.compat.v1.logging.TaskLevelStatusMessage\ntf.compat.v1.logging.debug\ntf.compat.v1.logging.error\ntf.compat.v1.logging.fatal\ntf.compat.v1.logging.flush\ntf.compat.v1.logging.get_verbosity\ntf.compat.v1.logging.info\ntf.compat.v1.logging.log\ntf.compat.v1.logging.log_every_n\ntf.compat.v1.logging.log_first_n\ntf.compat.v1.logging.log_if\ntf.compat.v1.logging.set_verbosity\ntf.compat.v1.logging.vlog\ntf.compat.v1.logging.warn\ntf.compat.v1.logging.warning\ntf.compat.v1.logical_and\ntf.compat.v1.logical_not\ntf.compat.v1.logical_or\ntf.compat.v1.logical_xor\ntf.compat.v1.lookup\ntf.compat.v1.lookup.KeyValueTensorInitializer\ntf.compat.v1.lookup.StaticHashTable\ntf.compat.v1.lookup.StaticVocabularyTable\ntf.compat.v1.lookup.TextFileIndex\ntf.compat.v1.lookup.TextFileInitializer\ntf.compat.v1.lookup.experimental\ntf.compat.v1.lookup.experimental.DenseHashTable\ntf.compat.v1.lookup.experimental.MutableHashTable\ntf.compat.v1.losses\ntf.compat.v1.losses.Reduction\ntf.compat.v1.losses.absolute_difference\ntf.compat.v1.losses.add_loss\ntf.compat.v1.losses.compute_weighted_loss\ntf.compat.v1.losses.cosine_distance\ntf.compat.v1.losses.get_losses\ntf.compat.v1.losses.get_regularization_loss\ntf.compat.v1.losses.get_regularization_losses\ntf.compat.v1.losses.get_total_loss\ntf.compat.v1.losses.hinge_loss\ntf.compat.v1.losses.huber_loss\ntf.compat.v1.losses.log_loss\ntf.compat.v1.losses.mean_pairwise_squared_error\ntf.compat.v1.losses.mean_squared_error\ntf.compat.v1.losses.sigmoid_cross_entropy\ntf.compat.v1.losses.softmax_cross_entropy\ntf.compat.v1.losses.sparse_softmax_cross_entropy\ntf.compat.v1.make_ndarray\ntf.compat.v1.make_template\ntf.compat.v1.make_tensor_proto\ntf.compat.v1.manip\ntf.compat.v1.manip.batch_to_space_nd\ntf.compat.v1.manip.gather_nd\ntf.compat.v1.manip.reshape\ntf.compat.v1.manip.reverse\ntf.compat.v1.manip.roll\ntf.compat.v1.manip.scatter_nd\ntf.compat.v1.manip.space_to_batch_nd\ntf.compat.v1.manip.tile\ntf.compat.v1.map_fn\ntf.compat.v1.matching_files\ntf.compat.v1.math\ntf.compat.v1.math.abs\ntf.compat.v1.math.accumulate_n\ntf.compat.v1.math.acos\ntf.compat.v1.math.acosh\ntf.compat.v1.math.add\ntf.compat.v1.math.add_n\ntf.compat.v1.math.angle\ntf.compat.v1.math.approx_max_k\ntf.compat.v1.math.approx_min_k\ntf.compat.v1.math.argmax\ntf.compat.v1.math.argmin\ntf.compat.v1.math.asin\ntf.compat.v1.math.asinh\ntf.compat.v1.math.atan\ntf.compat.v1.math.atan2\ntf.compat.v1.math.atanh\ntf.compat.v1.math.bessel_i0\ntf.compat.v1.math.bessel_i0e\ntf.compat.v1.math.bessel_i1\ntf.compat.v1.math.bessel_i1e\ntf.compat.v1.math.betainc\ntf.compat.v1.math.bincount\ntf.compat.v1.math.ceil\ntf.compat.v1.math.confusion_matrix\ntf.compat.v1.math.conj\ntf.compat.v1.math.cos\ntf.compat.v1.math.cosh\ntf.compat.v1.math.count_nonzero\ntf.compat.v1.math.cumprod\ntf.compat.v1.math.cumsum\ntf.compat.v1.math.cumulative_logsumexp\ntf.compat.v1.math.digamma\ntf.compat.v1.math.divide\ntf.compat.v1.math.divide_no_nan\ntf.compat.v1.math.equal\ntf.compat.v1.math.erf\ntf.compat.v1.math.erfc\ntf.compat.v1.math.erfcinv\ntf.compat.v1.math.erfinv\ntf.compat.v1.math.exp\ntf.compat.v1.math.expm1\ntf.compat.v1.math.floor\ntf.compat.v1.math.floordiv\ntf.compat.v1.math.floormod\ntf.compat.v1.math.greater\ntf.compat.v1.math.greater_equal\ntf.compat.v1.math.igamma\ntf.compat.v1.math.igammac\ntf.compat.v1.math.imag\ntf.compat.v1.math.in_top_k\ntf.compat.v1.math.invert_permutation\ntf.compat.v1.math.is_finite\ntf.compat.v1.math.is_inf\ntf.compat.v1.math.is_nan\ntf.compat.v1.math.is_non_decreasing\ntf.compat.v1.math.is_strictly_increasing\ntf.compat.v1.math.l2_normalize\ntf.compat.v1.math.lbeta\ntf.compat.v1.math.less\ntf.compat.v1.math.less_equal\ntf.compat.v1.math.lgamma\ntf.compat.v1.math.log\ntf.compat.v1.math.log1p\ntf.compat.v1.math.log_sigmoid\ntf.compat.v1.math.log_softmax\ntf.compat.v1.math.logical_and\ntf.compat.v1.math.logical_not\ntf.compat.v1.math.logical_or\ntf.compat.v1.math.logical_xor\ntf.compat.v1.math.maximum\ntf.compat.v1.math.minimum\ntf.compat.v1.math.mod\ntf.compat.v1.math.multiply\ntf.compat.v1.math.multiply_no_nan\ntf.compat.v1.math.ndtri\ntf.compat.v1.math.negative\ntf.compat.v1.math.nextafter\ntf.compat.v1.math.not_equal\ntf.compat.v1.math.polygamma\ntf.compat.v1.math.polyval\ntf.compat.v1.math.pow\ntf.compat.v1.math.real\ntf.compat.v1.math.reciprocal\ntf.compat.v1.math.reciprocal_no_nan\ntf.compat.v1.math.reduce_all\ntf.compat.v1.math.reduce_any\ntf.compat.v1.math.reduce_euclidean_norm\ntf.compat.v1.math.reduce_logsumexp\ntf.compat.v1.math.reduce_max\ntf.compat.v1.math.reduce_mean\ntf.compat.v1.math.reduce_min\ntf.compat.v1.math.reduce_prod\ntf.compat.v1.math.reduce_std\ntf.compat.v1.math.reduce_sum\ntf.compat.v1.math.reduce_variance\ntf.compat.v1.math.rint\ntf.compat.v1.math.round\ntf.compat.v1.math.rsqrt\ntf.compat.v1.math.scalar_mul\ntf.compat.v1.math.segment_max\ntf.compat.v1.math.segment_mean\ntf.compat.v1.math.segment_min\ntf.compat.v1.math.segment_prod\ntf.compat.v1.math.segment_sum\ntf.compat.v1.math.sigmoid\ntf.compat.v1.math.sign\ntf.compat.v1.math.sin\ntf.compat.v1.math.sinh\ntf.compat.v1.math.sobol_sample\ntf.compat.v1.math.softmax\ntf.compat.v1.math.softplus\ntf.compat.v1.math.softsign\ntf.compat.v1.math.special\ntf.compat.v1.math.special.bessel_i0\ntf.compat.v1.math.special.bessel_i0e\ntf.compat.v1.math.special.bessel_i1\ntf.compat.v1.math.special.bessel_i1e\ntf.compat.v1.math.special.bessel_j0\ntf.compat.v1.math.special.bessel_j1\ntf.compat.v1.math.special.bessel_k0\ntf.compat.v1.math.special.bessel_k0e\ntf.compat.v1.math.special.bessel_k1\ntf.compat.v1.math.special.bessel_k1e\ntf.compat.v1.math.special.bessel_y0\ntf.compat.v1.math.special.bessel_y1\ntf.compat.v1.math.special.dawsn\ntf.compat.v1.math.special.expint\ntf.compat.v1.math.special.fresnel_cos\ntf.compat.v1.math.special.fresnel_sin\ntf.compat.v1.math.special.spence\ntf.compat.v1.math.sqrt\ntf.compat.v1.math.square\ntf.compat.v1.math.squared_difference\ntf.compat.v1.math.subtract\ntf.compat.v1.math.tan\ntf.compat.v1.math.tanh\ntf.compat.v1.math.top_k\ntf.compat.v1.math.truediv\ntf.compat.v1.math.unsorted_segment_max\ntf.compat.v1.math.unsorted_segment_mean\ntf.compat.v1.math.unsorted_segment_min\ntf.compat.v1.math.unsorted_segment_prod\ntf.compat.v1.math.unsorted_segment_sqrt_n\ntf.compat.v1.math.unsorted_segment_sum\ntf.compat.v1.math.xdivy\ntf.compat.v1.math.xlog1py\ntf.compat.v1.math.xlogy\ntf.compat.v1.math.zero_fraction\ntf.compat.v1.math.zeta\ntf.compat.v1.matmul\ntf.compat.v1.matrix_band_part\ntf.compat.v1.matrix_determinant\ntf.compat.v1.matrix_diag\ntf.compat.v1.matrix_diag_part\ntf.compat.v1.matrix_inverse\ntf.compat.v1.matrix_set_diag\ntf.compat.v1.matrix_solve\ntf.compat.v1.matrix_solve_ls\ntf.compat.v1.matrix_square_root\ntf.compat.v1.matrix_transpose\ntf.compat.v1.matrix_triangular_solve\ntf.compat.v1.maximum\ntf.compat.v1.meshgrid\ntf.compat.v1.metrics\ntf.compat.v1.metrics.accuracy\ntf.compat.v1.metrics.auc\ntf.compat.v1.metrics.average_precision_at_k\ntf.compat.v1.metrics.false_negatives\ntf.compat.v1.metrics.false_negatives_at_thresholds\ntf.compat.v1.metrics.false_positives\ntf.compat.v1.metrics.false_positives_at_thresholds\ntf.compat.v1.metrics.mean\ntf.compat.v1.metrics.mean_absolute_error\ntf.compat.v1.metrics.mean_cosine_distance\ntf.compat.v1.metrics.mean_iou\ntf.compat.v1.metrics.mean_per_class_accuracy\ntf.compat.v1.metrics.mean_relative_error\ntf.compat.v1.metrics.mean_squared_error\ntf.compat.v1.metrics.mean_tensor\ntf.compat.v1.metrics.percentage_below\ntf.compat.v1.metrics.precision\ntf.compat.v1.metrics.precision_at_k\ntf.compat.v1.metrics.precision_at_thresholds\ntf.compat.v1.metrics.precision_at_top_k\ntf.compat.v1.metrics.recall\ntf.compat.v1.metrics.recall_at_k\ntf.compat.v1.metrics.recall_at_thresholds\ntf.compat.v1.metrics.recall_at_top_k\ntf.compat.v1.metrics.root_mean_squared_error\ntf.compat.v1.metrics.sensitivity_at_specificity\ntf.compat.v1.metrics.sparse_average_precision_at_k\ntf.compat.v1.metrics.sparse_precision_at_k\ntf.compat.v1.metrics.specificity_at_sensitivity\ntf.compat.v1.metrics.true_negatives\ntf.compat.v1.metrics.true_negatives_at_thresholds\ntf.compat.v1.metrics.true_positives\ntf.compat.v1.metrics.true_positives_at_thresholds\ntf.compat.v1.min_max_variable_partitioner\ntf.compat.v1.minimum\ntf.compat.v1.mixed_precision\ntf.compat.v1.mixed_precision.DynamicLossScale\ntf.compat.v1.mixed_precision.FixedLossScale\ntf.compat.v1.mixed_precision.LossScale\ntf.compat.v1.mixed_precision.MixedPrecisionLossScaleOptimizer\ntf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite\ntf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite\ntf.compat.v1.mixed_precision.experimental\ntf.compat.v1.mixed_precision.experimental.DynamicLossScale\ntf.compat.v1.mixed_precision.experimental.FixedLossScale\ntf.compat.v1.mixed_precision.experimental.LossScale\ntf.compat.v1.mlir\ntf.compat.v1.mlir.experimental\ntf.compat.v1.mlir.experimental.convert_function\ntf.compat.v1.mlir.experimental.convert_graph_def\ntf.compat.v1.mlir.experimental.convert_saved_model\ntf.compat.v1.mlir.experimental.convert_saved_model_v1\ntf.compat.v1.mlir.experimental.run_pass_pipeline\ntf.compat.v1.mlir.experimental.tflite_to_tosa_bytecode\ntf.compat.v1.mlir.experimental.write_bytecode\ntf.compat.v1.mod\ntf.compat.v1.model_variables\ntf.compat.v1.moving_average_variables\ntf.compat.v1.multinomial\ntf.compat.v1.multiply\ntf.compat.v1.name_scope\ntf.compat.v1.negative\ntf.compat.v1.nest\ntf.compat.v1.nest.assert_same_structure\ntf.compat.v1.nest.flatten\ntf.compat.v1.nest.is_nested\ntf.compat.v1.nest.map_structure\ntf.compat.v1.nest.pack_sequence_as\ntf.compat.v1.nn\ntf.compat.v1.nn.all_candidate_sampler\ntf.compat.v1.nn.approx_max_k\ntf.compat.v1.nn.approx_min_k\ntf.compat.v1.nn.atrous_conv2d\ntf.compat.v1.nn.atrous_conv2d_transpose\ntf.compat.v1.nn.avg_pool\ntf.compat.v1.nn.avg_pool1d\ntf.compat.v1.nn.avg_pool2d\ntf.compat.v1.nn.avg_pool3d\ntf.compat.v1.nn.avg_pool_v2\ntf.compat.v1.nn.batch_norm_with_global_normalization\ntf.compat.v1.nn.batch_normalization\ntf.compat.v1.nn.bias_add\ntf.compat.v1.nn.bidirectional_dynamic_rnn\ntf.compat.v1.nn.collapse_repeated\ntf.compat.v1.nn.compute_accidental_hits\ntf.compat.v1.nn.compute_average_loss\ntf.compat.v1.nn.conv1d\ntf.compat.v1.nn.conv1d_transpose\ntf.compat.v1.nn.conv2d\ntf.compat.v1.nn.conv2d_backprop_filter\ntf.compat.v1.nn.conv2d_backprop_input\ntf.compat.v1.nn.conv2d_transpose\ntf.compat.v1.nn.conv3d\ntf.compat.v1.nn.conv3d_backprop_filter\ntf.compat.v1.nn.conv3d_backprop_filter_v2\ntf.compat.v1.nn.conv3d_transpose\ntf.compat.v1.nn.conv_transpose\ntf.compat.v1.nn.convolution\ntf.compat.v1.nn.crelu\ntf.compat.v1.nn.ctc_beam_search_decoder\ntf.compat.v1.nn.ctc_beam_search_decoder_v2\ntf.compat.v1.nn.ctc_greedy_decoder\ntf.compat.v1.nn.ctc_loss\ntf.compat.v1.nn.ctc_loss_v2\ntf.compat.v1.nn.ctc_unique_labels\ntf.compat.v1.nn.depth_to_space\ntf.compat.v1.nn.depthwise_conv2d\ntf.compat.v1.nn.depthwise_conv2d_backprop_filter\ntf.compat.v1.nn.depthwise_conv2d_backprop_input\ntf.compat.v1.nn.depthwise_conv2d_native\ntf.compat.v1.nn.depthwise_conv2d_native_backprop_filter\ntf.compat.v1.nn.depthwise_conv2d_native_backprop_input\ntf.compat.v1.nn.dilation2d\ntf.compat.v1.nn.dropout\ntf.compat.v1.nn.dynamic_rnn\ntf.compat.v1.nn.elu\ntf.compat.v1.nn.embedding_lookup\ntf.compat.v1.nn.embedding_lookup_sparse\ntf.compat.v1.nn.erosion2d\ntf.compat.v1.nn.experimental\ntf.compat.v1.nn.experimental.general_dropout\ntf.compat.v1.nn.experimental.stateless_dropout\ntf.compat.v1.nn.fixed_unigram_candidate_sampler\ntf.compat.v1.nn.fractional_avg_pool\ntf.compat.v1.nn.fractional_max_pool\ntf.compat.v1.nn.fused_batch_norm\ntf.compat.v1.nn.in_top_k\ntf.compat.v1.nn.l2_loss\ntf.compat.v1.nn.l2_normalize\ntf.compat.v1.nn.leaky_relu\ntf.compat.v1.nn.learned_unigram_candidate_sampler\ntf.compat.v1.nn.local_response_normalization\ntf.compat.v1.nn.log_poisson_loss\ntf.compat.v1.nn.log_softmax\ntf.compat.v1.nn.log_uniform_candidate_sampler\ntf.compat.v1.nn.lrn\ntf.compat.v1.nn.max_pool\ntf.compat.v1.nn.max_pool1d\ntf.compat.v1.nn.max_pool2d\ntf.compat.v1.nn.max_pool3d\ntf.compat.v1.nn.max_pool_v2\ntf.compat.v1.nn.max_pool_with_argmax\ntf.compat.v1.nn.moments\ntf.compat.v1.nn.nce_loss\ntf.compat.v1.nn.normalize_moments\ntf.compat.v1.nn.pool\ntf.compat.v1.nn.quantized_avg_pool\ntf.compat.v1.nn.quantized_conv2d\ntf.compat.v1.nn.quantized_max_pool\ntf.compat.v1.nn.quantized_relu_x\ntf.compat.v1.nn.raw_rnn\ntf.compat.v1.nn.relu\ntf.compat.v1.nn.relu6\ntf.compat.v1.nn.relu_layer\ntf.compat.v1.nn.rnn_cell\ntf.compat.v1.nn.safe_embedding_lookup_sparse\ntf.compat.v1.nn.sampled_softmax_loss\ntf.compat.v1.nn.scale_regularization_loss\ntf.compat.v1.nn.selu\ntf.compat.v1.nn.separable_conv2d\ntf.compat.v1.nn.sigmoid\ntf.compat.v1.nn.sigmoid_cross_entropy_with_logits\ntf.compat.v1.nn.silu\ntf.compat.v1.nn.softmax\ntf.compat.v1.nn.softmax_cross_entropy_with_logits\ntf.compat.v1.nn.softmax_cross_entropy_with_logits_v2\ntf.compat.v1.nn.softplus\ntf.compat.v1.nn.softsign\ntf.compat.v1.nn.space_to_batch\ntf.compat.v1.nn.space_to_depth\ntf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits\ntf.compat.v1.nn.static_bidirectional_rnn\ntf.compat.v1.nn.static_rnn\ntf.compat.v1.nn.static_state_saving_rnn\ntf.compat.v1.nn.sufficient_statistics\ntf.compat.v1.nn.swish\ntf.compat.v1.nn.tanh\ntf.compat.v1.nn.top_k\ntf.compat.v1.nn.uniform_candidate_sampler\ntf.compat.v1.nn.weighted_cross_entropy_with_logits\ntf.compat.v1.nn.weighted_moments\ntf.compat.v1.nn.with_space_to_batch\ntf.compat.v1.nn.xw_plus_b\ntf.compat.v1.nn.zero_fraction\ntf.compat.v1.no_gradient\ntf.compat.v1.no_op\ntf.compat.v1.no_regularizer\ntf.compat.v1.nondifferentiable_batch_function\ntf.compat.v1.norm\ntf.compat.v1.not_equal\ntf.compat.v1.numpy_function\ntf.compat.v1.one_hot\ntf.compat.v1.ones\ntf.compat.v1.ones_initializer\ntf.compat.v1.ones_like\ntf.compat.v1.op_scope\ntf.compat.v1.orthogonal_initializer\ntf.compat.v1.pad\ntf.compat.v1.parallel_stack\ntf.compat.v1.parse_example\ntf.compat.v1.parse_single_example\ntf.compat.v1.parse_single_sequence_example\ntf.compat.v1.parse_tensor\ntf.compat.v1.placeholder\ntf.compat.v1.placeholder_with_default\ntf.compat.v1.polygamma\ntf.compat.v1.pow\ntf.compat.v1.print\ntf.compat.v1.profiler\ntf.compat.v1.profiler.AdviceProto\ntf.compat.v1.profiler.AdviceProto.Checker\ntf.compat.v1.profiler.AdviceProto.CheckersEntry\ntf.compat.v1.profiler.GraphNodeProto\ntf.compat.v1.profiler.GraphNodeProto.InputShapesEntry\ntf.compat.v1.profiler.MultiGraphNodeProto\ntf.compat.v1.profiler.OpLogProto\ntf.compat.v1.profiler.OpLogProto.IdToStringEntry\ntf.compat.v1.profiler.ProfileOptionBuilder\ntf.compat.v1.profiler.Profiler\ntf.compat.v1.profiler.advise\ntf.compat.v1.profiler.profile\ntf.compat.v1.profiler.write_op_log\ntf.compat.v1.py_func\ntf.compat.v1.py_function\ntf.compat.v1.python_io\ntf.compat.v1.python_io.TFRecordCompressionType\ntf.compat.v1.python_io.TFRecordOptions\ntf.compat.v1.python_io.TFRecordWriter\ntf.compat.v1.python_io.tf_record_iterator\ntf.compat.v1.qr\ntf.compat.v1.quantization\ntf.compat.v1.quantization.dequantize\ntf.compat.v1.quantization.experimental\ntf.compat.v1.quantization.experimental.QuantizationComponentSpec\ntf.compat.v1.quantization.experimental.QuantizationMethod\ntf.compat.v1.quantization.experimental.QuantizationOptions\ntf.compat.v1.quantization.experimental.QuantizationOptions.RepresentativeDatasetsEntry\ntf.compat.v1.quantization.experimental.TfRecordRepresentativeDatasetSaver\ntf.compat.v1.quantization.experimental.UnitWiseQuantizationSpec\ntf.compat.v1.quantization.experimental.UnitWiseQuantizationSpec.QuantizationUnit\ntf.compat.v1.quantization.experimental.quantize_saved_model\ntf.compat.v1.quantization.fake_quant_with_min_max_args\ntf.compat.v1.quantization.fake_quant_with_min_max_args_gradient\ntf.compat.v1.quantization.fake_quant_with_min_max_vars\ntf.compat.v1.quantization.fake_quant_with_min_max_vars_gradient\ntf.compat.v1.quantization.fake_quant_with_min_max_vars_per_channel\ntf.compat.v1.quantization.fake_quant_with_min_max_vars_per_channel_gradient\ntf.compat.v1.quantization.quantize\ntf.compat.v1.quantization.quantize_and_dequantize\ntf.compat.v1.quantization.quantize_and_dequantize_v2\ntf.compat.v1.quantization.quantized_concat\ntf.compat.v1.quantize\ntf.compat.v1.quantize_v2\ntf.compat.v1.quantized_concat\ntf.compat.v1.queue\ntf.compat.v1.queue.FIFOQueue\ntf.compat.v1.queue.PaddingFIFOQueue\ntf.compat.v1.queue.PriorityQueue\ntf.compat.v1.queue.QueueBase\ntf.compat.v1.queue.RandomShuffleQueue\ntf.compat.v1.ragged\ntf.compat.v1.ragged.RaggedTensorValue\ntf.compat.v1.ragged.boolean_mask\ntf.compat.v1.ragged.constant\ntf.compat.v1.ragged.constant_value\ntf.compat.v1.ragged.cross\ntf.compat.v1.ragged.cross_hashed\ntf.compat.v1.ragged.map_flat_values\ntf.compat.v1.ragged.placeholder\ntf.compat.v1.ragged.range\ntf.compat.v1.ragged.row_splits_to_segment_ids\ntf.compat.v1.ragged.segment_ids_to_row_splits\ntf.compat.v1.ragged.stack\ntf.compat.v1.ragged.stack_dynamic_partitions\ntf.compat.v1.ragged_fill_empty_rows\ntf.compat.v1.ragged_fill_empty_rows_grad\ntf.compat.v1.random\ntf.compat.v1.random.Algorithm\ntf.compat.v1.random.Generator\ntf.compat.v1.random.all_candidate_sampler\ntf.compat.v1.random.categorical\ntf.compat.v1.random.create_rng_state\ntf.compat.v1.random.experimental\ntf.compat.v1.random.experimental.Algorithm\ntf.compat.v1.random.experimental.Generator\ntf.compat.v1.random.experimental.create_rng_state\ntf.compat.v1.random.experimental.get_global_generator\ntf.compat.v1.random.experimental.index_shuffle\ntf.compat.v1.random.experimental.set_global_generator\ntf.compat.v1.random.experimental.stateless_fold_in\ntf.compat.v1.random.experimental.stateless_shuffle\ntf.compat.v1.random.experimental.stateless_split\ntf.compat.v1.random.fixed_unigram_candidate_sampler\ntf.compat.v1.random.fold_in\ntf.compat.v1.random.gamma\ntf.compat.v1.random.get_global_generator\ntf.compat.v1.random.get_seed\ntf.compat.v1.random.learned_unigram_candidate_sampler\ntf.compat.v1.random.log_uniform_candidate_sampler\ntf.compat.v1.random.multinomial\ntf.compat.v1.random.normal\ntf.compat.v1.random.poisson\ntf.compat.v1.random.set_global_generator\ntf.compat.v1.random.set_random_seed\ntf.compat.v1.random.shuffle\ntf.compat.v1.random.split\ntf.compat.v1.random.stateless_binomial\ntf.compat.v1.random.stateless_categorical\ntf.compat.v1.random.stateless_gamma\ntf.compat.v1.random.stateless_multinomial\ntf.compat.v1.random.stateless_normal\ntf.compat.v1.random.stateless_parameterized_truncated_normal\ntf.compat.v1.random.stateless_poisson\ntf.compat.v1.random.stateless_truncated_normal\ntf.compat.v1.random.stateless_uniform\ntf.compat.v1.random.truncated_normal\ntf.compat.v1.random.uniform\ntf.compat.v1.random.uniform_candidate_sampler\ntf.compat.v1.random_crop\ntf.compat.v1.random_gamma\ntf.compat.v1.random_index_shuffle\ntf.compat.v1.random_normal\ntf.compat.v1.random_normal_initializer\ntf.compat.v1.random_poisson\ntf.compat.v1.random_shuffle\ntf.compat.v1.random_uniform\ntf.compat.v1.random_uniform_initializer\ntf.compat.v1.range\ntf.compat.v1.rank\ntf.compat.v1.read_file\ntf.compat.v1.real\ntf.compat.v1.realdiv\ntf.compat.v1.reciprocal\ntf.compat.v1.recompute_grad\ntf.compat.v1.reduce_all\ntf.compat.v1.reduce_any\ntf.compat.v1.reduce_join\ntf.compat.v1.reduce_logsumexp\ntf.compat.v1.reduce_max\ntf.compat.v1.reduce_mean\ntf.compat.v1.reduce_min\ntf.compat.v1.reduce_prod\ntf.compat.v1.reduce_sum\ntf.compat.v1.regex_replace\ntf.compat.v1.register_tensor_conversion_function\ntf.compat.v1.repeat\ntf.compat.v1.report_uninitialized_variables\ntf.compat.v1.required_space_to_batch_paddings\ntf.compat.v1.reset_default_graph\ntf.compat.v1.reshape\ntf.compat.v1.resource_loader\ntf.compat.v1.resource_loader.get_data_files_path\ntf.compat.v1.resource_loader.get_path_to_datafile\ntf.compat.v1.resource_loader.get_root_dir_with_all_resources\ntf.compat.v1.resource_loader.load_resource\ntf.compat.v1.resource_loader.readahead_file_path\ntf.compat.v1.resource_variables_enabled\ntf.compat.v1.reverse\ntf.compat.v1.reverse_sequence\ntf.compat.v1.reverse_v2\ntf.compat.v1.rfftnd\ntf.compat.v1.rint\ntf.compat.v1.roll\ntf.compat.v1.round\ntf.compat.v1.rsqrt\ntf.compat.v1.saturate_cast\ntf.compat.v1.saved_model\ntf.compat.v1.saved_model.Asset\ntf.compat.v1.saved_model.Builder\ntf.compat.v1.saved_model.SaveOptions\ntf.compat.v1.saved_model.build_signature_def\ntf.compat.v1.saved_model.build_tensor_info\ntf.compat.v1.saved_model.builder\ntf.compat.v1.saved_model.builder.SavedModelBuilder\ntf.compat.v1.saved_model.classification_signature_def\ntf.compat.v1.saved_model.constants\ntf.compat.v1.saved_model.contains_saved_model\ntf.compat.v1.saved_model.experimental\ntf.compat.v1.saved_model.experimental.TrackableResource\ntf.compat.v1.saved_model.experimental.VariablePolicy\ntf.compat.v1.saved_model.experimental.save\ntf.compat.v1.saved_model.get_tensor_from_tensor_info\ntf.compat.v1.saved_model.is_valid_signature\ntf.compat.v1.saved_model.load\ntf.compat.v1.saved_model.load_v2\ntf.compat.v1.saved_model.loader\ntf.compat.v1.saved_model.loader.load\ntf.compat.v1.saved_model.loader.maybe_saved_model_directory\ntf.compat.v1.saved_model.main_op\ntf.compat.v1.saved_model.main_op.main_op\ntf.compat.v1.saved_model.main_op.main_op_with_restore\ntf.compat.v1.saved_model.main_op_with_restore\ntf.compat.v1.saved_model.maybe_saved_model_directory\ntf.compat.v1.saved_model.predict_signature_def\ntf.compat.v1.saved_model.regression_signature_def\ntf.compat.v1.saved_model.save\ntf.compat.v1.saved_model.signature_constants\ntf.compat.v1.saved_model.signature_def_utils\ntf.compat.v1.saved_model.signature_def_utils.MethodNameUpdater\ntf.compat.v1.saved_model.signature_def_utils.build_signature_def\ntf.compat.v1.saved_model.signature_def_utils.classification_signature_def\ntf.compat.v1.saved_model.signature_def_utils.is_valid_signature\ntf.compat.v1.saved_model.signature_def_utils.predict_signature_def\ntf.compat.v1.saved_model.signature_def_utils.regression_signature_def\ntf.compat.v1.saved_model.simple_save\ntf.compat.v1.saved_model.tag_constants\ntf.compat.v1.saved_model.utils\ntf.compat.v1.saved_model.utils.build_tensor_info\ntf.compat.v1.saved_model.utils.get_tensor_from_tensor_info\ntf.compat.v1.scalar_mul\ntf.compat.v1.scan\ntf.compat.v1.scatter_add\ntf.compat.v1.scatter_div\ntf.compat.v1.scatter_max\ntf.compat.v1.scatter_min\ntf.compat.v1.scatter_mul\ntf.compat.v1.scatter_nd\ntf.compat.v1.scatter_nd_add\ntf.compat.v1.scatter_nd_sub\ntf.compat.v1.scatter_nd_update\ntf.compat.v1.scatter_sub\ntf.compat.v1.scatter_update\ntf.compat.v1.searchsorted\ntf.compat.v1.segment_max\ntf.compat.v1.segment_mean\ntf.compat.v1.segment_min\ntf.compat.v1.segment_prod\ntf.compat.v1.segment_sum\ntf.compat.v1.self_adjoint_eig\ntf.compat.v1.self_adjoint_eigvals\ntf.compat.v1.sequence_mask\ntf.compat.v1.serialize_many_sparse\ntf.compat.v1.serialize_sparse\ntf.compat.v1.serialize_tensor\ntf.compat.v1.set_random_seed\ntf.compat.v1.setdiff1d\ntf.compat.v1.sets\ntf.compat.v1.sets.difference\ntf.compat.v1.sets.intersection\ntf.compat.v1.sets.set_difference\ntf.compat.v1.sets.set_intersection\ntf.compat.v1.sets.set_size\ntf.compat.v1.sets.set_union\ntf.compat.v1.sets.size\ntf.compat.v1.sets.union\ntf.compat.v1.shape\ntf.compat.v1.shape_n\ntf.compat.v1.sigmoid\ntf.compat.v1.sign\ntf.compat.v1.signal\ntf.compat.v1.signal.dct\ntf.compat.v1.signal.fft\ntf.compat.v1.signal.fft2d\ntf.compat.v1.signal.fft3d\ntf.compat.v1.signal.fftnd\ntf.compat.v1.signal.fftshift\ntf.compat.v1.signal.frame\ntf.compat.v1.signal.hamming_window\ntf.compat.v1.signal.hann_window\ntf.compat.v1.signal.idct\ntf.compat.v1.signal.ifft\ntf.compat.v1.signal.ifft2d\ntf.compat.v1.signal.ifft3d\ntf.compat.v1.signal.ifftnd\ntf.compat.v1.signal.ifftshift\ntf.compat.v1.signal.inverse_mdct\ntf.compat.v1.signal.inverse_stft\ntf.compat.v1.signal.inverse_stft_window_fn\ntf.compat.v1.signal.irfft\ntf.compat.v1.signal.irfft2d\ntf.compat.v1.signal.irfft3d\ntf.compat.v1.signal.irfftnd\ntf.compat.v1.signal.kaiser_bessel_derived_window\ntf.compat.v1.signal.kaiser_window\ntf.compat.v1.signal.linear_to_mel_weight_matrix\ntf.compat.v1.signal.mdct\ntf.compat.v1.signal.mfccs_from_log_mel_spectrograms\ntf.compat.v1.signal.overlap_and_add\ntf.compat.v1.signal.rfft\ntf.compat.v1.signal.rfft2d\ntf.compat.v1.signal.rfft3d\ntf.compat.v1.signal.rfftnd\ntf.compat.v1.signal.stft\ntf.compat.v1.signal.vorbis_window\ntf.compat.v1.sin\ntf.compat.v1.sinh\ntf.compat.v1.size\ntf.compat.v1.slice\ntf.compat.v1.sort\ntf.compat.v1.space_to_batch\ntf.compat.v1.space_to_batch_nd\ntf.compat.v1.space_to_depth\ntf.compat.v1.sparse\ntf.compat.v1.sparse.SparseConditionalAccumulator\ntf.compat.v1.sparse.SparseTensor\ntf.compat.v1.sparse.add\ntf.compat.v1.sparse.bincount\ntf.compat.v1.sparse.concat\ntf.compat.v1.sparse.cross\ntf.compat.v1.sparse.cross_hashed\ntf.compat.v1.sparse.expand_dims\ntf.compat.v1.sparse.eye\ntf.compat.v1.sparse.fill_empty_rows\ntf.compat.v1.sparse.from_dense\ntf.compat.v1.sparse.mask\ntf.compat.v1.sparse.matmul\ntf.compat.v1.sparse.maximum\ntf.compat.v1.sparse.merge\ntf.compat.v1.sparse.minimum\ntf.compat.v1.sparse.placeholder\ntf.compat.v1.sparse.reduce_max\ntf.compat.v1.sparse.reduce_max_sparse\ntf.compat.v1.sparse.reduce_sum\ntf.compat.v1.sparse.reduce_sum_sparse\ntf.compat.v1.sparse.reorder\ntf.compat.v1.sparse.reset_shape\ntf.compat.v1.sparse.reshape\ntf.compat.v1.sparse.retain\ntf.compat.v1.sparse.segment_mean\ntf.compat.v1.sparse.segment_sqrt_n\ntf.compat.v1.sparse.segment_sum\ntf.compat.v1.sparse.slice\ntf.compat.v1.sparse.softmax\ntf.compat.v1.sparse.sparse_dense_matmul\ntf.compat.v1.sparse.split\ntf.compat.v1.sparse.to_dense\ntf.compat.v1.sparse.to_indicator\ntf.compat.v1.sparse.transpose\ntf.compat.v1.sparse_add\ntf.compat.v1.sparse_concat\ntf.compat.v1.sparse_fill_empty_rows\ntf.compat.v1.sparse_mask\ntf.compat.v1.sparse_matmul\ntf.compat.v1.sparse_maximum\ntf.compat.v1.sparse_merge\ntf.compat.v1.sparse_minimum\ntf.compat.v1.sparse_placeholder\ntf.compat.v1.sparse_reduce_max\ntf.compat.v1.sparse_reduce_max_sparse\ntf.compat.v1.sparse_reduce_sum\ntf.compat.v1.sparse_reduce_sum_sparse\ntf.compat.v1.sparse_reorder\ntf.compat.v1.sparse_reset_shape\ntf.compat.v1.sparse_reshape\ntf.compat.v1.sparse_retain\ntf.compat.v1.sparse_segment_mean\ntf.compat.v1.sparse_segment_sqrt_n\ntf.compat.v1.sparse_segment_sum\ntf.compat.v1.sparse_slice\ntf.compat.v1.sparse_softmax\ntf.compat.v1.sparse_split\ntf.compat.v1.sparse_tensor_dense_matmul\ntf.compat.v1.sparse_tensor_to_dense\ntf.compat.v1.sparse_to_dense\ntf.compat.v1.sparse_to_indicator\ntf.compat.v1.sparse_transpose\ntf.compat.v1.spectral\ntf.compat.v1.spectral.dct\ntf.compat.v1.spectral.fft\ntf.compat.v1.spectral.fft2d\ntf.compat.v1.spectral.fft3d\ntf.compat.v1.spectral.idct\ntf.compat.v1.spectral.ifft\ntf.compat.v1.spectral.ifft2d\ntf.compat.v1.spectral.ifft3d\ntf.compat.v1.spectral.irfft\ntf.compat.v1.spectral.irfft2d\ntf.compat.v1.spectral.irfft3d\ntf.compat.v1.spectral.rfft\ntf.compat.v1.spectral.rfft2d\ntf.compat.v1.spectral.rfft3d\ntf.compat.v1.split\ntf.compat.v1.sqrt\ntf.compat.v1.square\ntf.compat.v1.squared_difference\ntf.compat.v1.squeeze\ntf.compat.v1.stack\ntf.compat.v1.stop_gradient\ntf.compat.v1.strided_slice\ntf.compat.v1.string_join\ntf.compat.v1.string_split\ntf.compat.v1.string_strip\ntf.compat.v1.string_to_hash_bucket\ntf.compat.v1.string_to_hash_bucket_fast\ntf.compat.v1.string_to_hash_bucket_strong\ntf.compat.v1.string_to_number\ntf.compat.v1.strings\ntf.compat.v1.strings.as_string\ntf.compat.v1.strings.bytes_split\ntf.compat.v1.strings.format\ntf.compat.v1.strings.join\ntf.compat.v1.strings.length\ntf.compat.v1.strings.lower\ntf.compat.v1.strings.ngrams\ntf.compat.v1.strings.reduce_join\ntf.compat.v1.strings.regex_full_match\ntf.compat.v1.strings.regex_replace\ntf.compat.v1.strings.split\ntf.compat.v1.strings.strip\ntf.compat.v1.strings.substr\ntf.compat.v1.strings.to_hash_bucket\ntf.compat.v1.strings.to_hash_bucket_fast\ntf.compat.v1.strings.to_hash_bucket_strong\ntf.compat.v1.strings.to_number\ntf.compat.v1.strings.unicode_decode\ntf.compat.v1.strings.unicode_decode_with_offsets\ntf.compat.v1.strings.unicode_encode\ntf.compat.v1.strings.unicode_script\ntf.compat.v1.strings.unicode_split\ntf.compat.v1.strings.unicode_split_with_offsets\ntf.compat.v1.strings.unicode_transcode\ntf.compat.v1.strings.unsorted_segment_join\ntf.compat.v1.strings.upper\ntf.compat.v1.substr\ntf.compat.v1.subtract\ntf.compat.v1.summary\ntf.compat.v1.summary.Event\ntf.compat.v1.summary.FileWriter\ntf.compat.v1.summary.FileWriterCache\ntf.compat.v1.summary.SessionLog\ntf.compat.v1.summary.Summary\ntf.compat.v1.summary.Summary.Audio\ntf.compat.v1.summary.Summary.Image\ntf.compat.v1.summary.Summary.Value\ntf.compat.v1.summary.SummaryDescription\ntf.compat.v1.summary.TaggedRunMetadata\ntf.compat.v1.summary.all_v2_summary_ops\ntf.compat.v1.summary.audio\ntf.compat.v1.summary.get_summary_description\ntf.compat.v1.summary.histogram\ntf.compat.v1.summary.image\ntf.compat.v1.summary.initialize\ntf.compat.v1.summary.merge\ntf.compat.v1.summary.merge_all\ntf.compat.v1.summary.scalar\ntf.compat.v1.summary.tensor_summary\ntf.compat.v1.summary.text\ntf.compat.v1.svd\ntf.compat.v1.switch_case\ntf.compat.v1.sysconfig\ntf.compat.v1.sysconfig.get_build_info\ntf.compat.v1.sysconfig.get_compile_flags\ntf.compat.v1.sysconfig.get_include\ntf.compat.v1.sysconfig.get_lib\ntf.compat.v1.sysconfig.get_link_flags\ntf.compat.v1.tables_initializer\ntf.compat.v1.tan\ntf.compat.v1.tanh\ntf.compat.v1.tensor_scatter_add\ntf.compat.v1.tensor_scatter_nd_add\ntf.compat.v1.tensor_scatter_nd_max\ntf.compat.v1.tensor_scatter_nd_min\ntf.compat.v1.tensor_scatter_nd_sub\ntf.compat.v1.tensor_scatter_nd_update\ntf.compat.v1.tensor_scatter_sub\ntf.compat.v1.tensor_scatter_update\ntf.compat.v1.tensordot\ntf.compat.v1.test\ntf.compat.v1.test.Benchmark\ntf.compat.v1.test.StubOutForTesting\ntf.compat.v1.test.TestCase\ntf.compat.v1.test.TestCase.failureException\ntf.compat.v1.test.assert_equal_graph_def\ntf.compat.v1.test.benchmark_config\ntf.compat.v1.test.compute_gradient\ntf.compat.v1.test.compute_gradient_error\ntf.compat.v1.test.create_local_cluster\ntf.compat.v1.test.disable_with_predicate\ntf.compat.v1.test.experimental\ntf.compat.v1.test.experimental.sync_devices\ntf.compat.v1.test.get_temp_dir\ntf.compat.v1.test.gpu_device_name\ntf.compat.v1.test.is_built_with_cuda\ntf.compat.v1.test.is_built_with_gpu_support\ntf.compat.v1.test.is_built_with_rocm\ntf.compat.v1.test.is_built_with_xla\ntf.compat.v1.test.is_gpu_available\ntf.compat.v1.test.main\ntf.compat.v1.test.test_src_dir_path\ntf.compat.v1.test.with_eager_op_as_function\ntf.compat.v1.tile\ntf.compat.v1.timestamp\ntf.compat.v1.to_bfloat16\ntf.compat.v1.to_complex128\ntf.compat.v1.to_complex64\ntf.compat.v1.to_double\ntf.compat.v1.to_float\ntf.compat.v1.to_int32\ntf.compat.v1.to_int64\ntf.compat.v1.tpu\ntf.compat.v1.tpu.CrossShardOptimizer\ntf.compat.v1.tpu.PaddingSpec\ntf.compat.v1.tpu.XLAOptions\ntf.compat.v1.tpu.batch_parallel\ntf.compat.v1.tpu.bfloat16_scope\ntf.compat.v1.tpu.core\ntf.compat.v1.tpu.cross_replica_sum\ntf.compat.v1.tpu.experimental\ntf.compat.v1.tpu.experimental.DeviceAssignment\ntf.compat.v1.tpu.experimental.DeviceOrderMode\ntf.compat.v1.tpu.experimental.HardwareFeature\ntf.compat.v1.tpu.experimental.HardwareFeature.EmbeddingFeature\ntf.compat.v1.tpu.experimental.TPUSystemMetadata\ntf.compat.v1.tpu.experimental.Topology\ntf.compat.v1.tpu.experimental.embedding\ntf.compat.v1.tpu.experimental.embedding.Adagrad\ntf.compat.v1.tpu.experimental.embedding.AdagradMomentum\ntf.compat.v1.tpu.experimental.embedding.Adam\ntf.compat.v1.tpu.experimental.embedding.FTRL\ntf.compat.v1.tpu.experimental.embedding.FeatureConfig\ntf.compat.v1.tpu.experimental.embedding.QuantizationConfig\ntf.compat.v1.tpu.experimental.embedding.RowIdInitializer\ntf.compat.v1.tpu.experimental.embedding.SGD\ntf.compat.v1.tpu.experimental.embedding.TPUEmbedding\ntf.compat.v1.tpu.experimental.embedding.TPUEmbeddingForServing\ntf.compat.v1.tpu.experimental.embedding.TPUEmbeddingV0\ntf.compat.v1.tpu.experimental.embedding.TPUEmbeddingV2\ntf.compat.v1.tpu.experimental.embedding.TableConfig\ntf.compat.v1.tpu.experimental.embedding.serving_embedding_lookup\ntf.compat.v1.tpu.experimental.embedding_column\ntf.compat.v1.tpu.experimental.initialize_tpu_system\ntf.compat.v1.tpu.experimental.shared_embedding_columns\ntf.compat.v1.tpu.experimental.shutdown_tpu_system\ntf.compat.v1.tpu.initialize_system\ntf.compat.v1.tpu.outside_compilation\ntf.compat.v1.tpu.replicate\ntf.compat.v1.tpu.rewrite\ntf.compat.v1.tpu.shard\ntf.compat.v1.tpu.shutdown_system\ntf.compat.v1.trace\ntf.compat.v1.train\ntf.compat.v1.train.AdadeltaOptimizer\ntf.compat.v1.train.AdagradDAOptimizer\ntf.compat.v1.train.AdagradOptimizer\ntf.compat.v1.train.AdamOptimizer\ntf.compat.v1.train.BytesList\ntf.compat.v1.train.Checkpoint\ntf.compat.v1.train.CheckpointManager\ntf.compat.v1.train.CheckpointOptions\ntf.compat.v1.train.CheckpointSaverHook\ntf.compat.v1.train.CheckpointSaverListener\ntf.compat.v1.train.ChiefSessionCreator\ntf.compat.v1.train.ClusterDef\ntf.compat.v1.train.ClusterSpec\ntf.compat.v1.train.Coordinator\ntf.compat.v1.train.Example\ntf.compat.v1.train.ExponentialMovingAverage\ntf.compat.v1.train.Feature\ntf.compat.v1.train.FeatureList\ntf.compat.v1.train.FeatureLists\ntf.compat.v1.train.FeatureLists.FeatureListEntry\ntf.compat.v1.train.Features\ntf.compat.v1.train.Features.FeatureEntry\ntf.compat.v1.train.FeedFnHook\ntf.compat.v1.train.FinalOpsHook\ntf.compat.v1.train.FloatList\ntf.compat.v1.train.FtrlOptimizer\ntf.compat.v1.train.GlobalStepWaiterHook\ntf.compat.v1.train.GradientDescentOptimizer\ntf.compat.v1.train.Int64List\ntf.compat.v1.train.JobDef\ntf.compat.v1.train.JobDef.TasksEntry\ntf.compat.v1.train.LoggingTensorHook\ntf.compat.v1.train.LooperThread\ntf.compat.v1.train.MomentumOptimizer\ntf.compat.v1.train.MonitoredSession\ntf.compat.v1.train.MonitoredSession.StepContext\ntf.compat.v1.train.MonitoredTrainingSession\ntf.compat.v1.train.NanLossDuringTrainingError\ntf.compat.v1.train.NanTensorHook\ntf.compat.v1.train.NewCheckpointReader\ntf.compat.v1.train.Optimizer\ntf.compat.v1.train.ProfilerHook\ntf.compat.v1.train.ProximalAdagradOptimizer\ntf.compat.v1.train.ProximalGradientDescentOptimizer\ntf.compat.v1.train.QueueRunner\ntf.compat.v1.train.RMSPropOptimizer\ntf.compat.v1.train.Saver\ntf.compat.v1.train.SaverDef\ntf.compat.v1.train.Scaffold\ntf.compat.v1.train.SecondOrStepTimer\ntf.compat.v1.train.SequenceExample\ntf.compat.v1.train.Server\ntf.compat.v1.train.ServerDef\ntf.compat.v1.train.SessionCreator\ntf.compat.v1.train.SessionManager\ntf.compat.v1.train.SessionRunArgs\ntf.compat.v1.train.SessionRunContext\ntf.compat.v1.train.SessionRunHook\ntf.compat.v1.train.SessionRunValues\ntf.compat.v1.train.SingularMonitoredSession\ntf.compat.v1.train.SingularMonitoredSession.StepContext\ntf.compat.v1.train.StepCounterHook\ntf.compat.v1.train.StopAtStepHook\ntf.compat.v1.train.SummarySaverHook\ntf.compat.v1.train.Supervisor\ntf.compat.v1.train.SyncReplicasOptimizer\ntf.compat.v1.train.VocabInfo\ntf.compat.v1.train.WorkerSessionCreator\ntf.compat.v1.train.add_queue_runner\ntf.compat.v1.train.assert_global_step\ntf.compat.v1.train.basic_train_loop\ntf.compat.v1.train.batch\ntf.compat.v1.train.batch_join\ntf.compat.v1.train.checkpoint_exists\ntf.compat.v1.train.checkpoints_iterator\ntf.compat.v1.train.cosine_decay\ntf.compat.v1.train.cosine_decay_restarts\ntf.compat.v1.train.create_global_step\ntf.compat.v1.train.do_quantize_training_on_graphdef\ntf.compat.v1.train.experimental\ntf.compat.v1.train.experimental.DynamicLossScale\ntf.compat.v1.train.experimental.FixedLossScale\ntf.compat.v1.train.experimental.LossScale\ntf.compat.v1.train.experimental.MaxShardSizePolicy\ntf.compat.v1.train.experimental.MixedPrecisionLossScaleOptimizer\ntf.compat.v1.train.experimental.PythonState\ntf.compat.v1.train.experimental.ShardByTaskPolicy\ntf.compat.v1.train.experimental.ShardableTensor\ntf.compat.v1.train.experimental.ShardingCallback\ntf.compat.v1.train.experimental.disable_mixed_precision_graph_rewrite\ntf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite\ntf.compat.v1.train.exponential_decay\ntf.compat.v1.train.export_meta_graph\ntf.compat.v1.train.generate_checkpoint_state_proto\ntf.compat.v1.train.get_checkpoint_mtimes\ntf.compat.v1.train.get_checkpoint_state\ntf.compat.v1.train.get_global_step\ntf.compat.v1.train.get_or_create_global_step\ntf.compat.v1.train.global_step\ntf.compat.v1.train.import_meta_graph\ntf.compat.v1.train.init_from_checkpoint\ntf.compat.v1.train.input_producer\ntf.compat.v1.train.inverse_time_decay\ntf.compat.v1.train.latest_checkpoint\ntf.compat.v1.train.limit_epochs\ntf.compat.v1.train.linear_cosine_decay\ntf.compat.v1.train.list_variables\ntf.compat.v1.train.load_checkpoint\ntf.compat.v1.train.load_variable\ntf.compat.v1.train.match_filenames_once\ntf.compat.v1.train.maybe_batch\ntf.compat.v1.train.maybe_batch_join\ntf.compat.v1.train.maybe_shuffle_batch\ntf.compat.v1.train.maybe_shuffle_batch_join\ntf.compat.v1.train.natural_exp_decay\ntf.compat.v1.train.noisy_linear_cosine_decay\ntf.compat.v1.train.piecewise_constant\ntf.compat.v1.train.piecewise_constant_decay\ntf.compat.v1.train.polynomial_decay\ntf.compat.v1.train.queue_runner\ntf.compat.v1.train.queue_runner.QueueRunner\ntf.compat.v1.train.queue_runner.add_queue_runner\ntf.compat.v1.train.queue_runner.start_queue_runners\ntf.compat.v1.train.range_input_producer\ntf.compat.v1.train.remove_checkpoint\ntf.compat.v1.train.replica_device_setter\ntf.compat.v1.train.sdca_fprint\ntf.compat.v1.train.sdca_optimizer\ntf.compat.v1.train.sdca_shrink_l1\ntf.compat.v1.train.shuffle_batch\ntf.compat.v1.train.shuffle_batch_join\ntf.compat.v1.train.slice_input_producer\ntf.compat.v1.train.start_queue_runners\ntf.compat.v1.train.string_input_producer\ntf.compat.v1.train.summary_iterator\ntf.compat.v1.train.update_checkpoint_state\ntf.compat.v1.train.warm_start\ntf.compat.v1.train.write_graph\ntf.compat.v1.trainable_variables\ntf.compat.v1.transpose\ntf.compat.v1.truediv\ntf.compat.v1.truncated_normal\ntf.compat.v1.truncated_normal_initializer\ntf.compat.v1.truncatediv\ntf.compat.v1.truncatemod\ntf.compat.v1.tuple\ntf.compat.v1.type_spec_from_value\ntf.compat.v1.types\ntf.compat.v1.types.experimental\ntf.compat.v1.types.experimental.FunctionType\ntf.compat.v1.types.experimental.FunctionType.empty\ntf.compat.v1.types.experimental.TensorLike\ntf.compat.v1.uniform_unit_scaling_initializer\ntf.compat.v1.unique\ntf.compat.v1.unique_with_counts\ntf.compat.v1.unravel_index\ntf.compat.v1.unsorted_segment_max\ntf.compat.v1.unsorted_segment_mean\ntf.compat.v1.unsorted_segment_min\ntf.compat.v1.unsorted_segment_prod\ntf.compat.v1.unsorted_segment_sqrt_n\ntf.compat.v1.unsorted_segment_sum\ntf.compat.v1.unstack\ntf.compat.v1.user_ops\ntf.compat.v1.user_ops.my_fact\ntf.compat.v1.variable_axis_size_partitioner\ntf.compat.v1.variable_creator_scope\ntf.compat.v1.variable_op_scope\ntf.compat.v1.variable_scope\ntf.compat.v1.variables_initializer\ntf.compat.v1.variance_scaling_initializer\ntf.compat.v1.vectorized_map\ntf.compat.v1.verify_tensor_all_finite\ntf.compat.v1.version\ntf.compat.v1.where\ntf.compat.v1.where_v2\ntf.compat.v1.while_loop\ntf.compat.v1.wrap_function\ntf.compat.v1.write_file\ntf.compat.v1.xla\ntf.compat.v1.xla.experimental\ntf.compat.v1.xla.experimental.compile\ntf.compat.v1.xla.experimental.jit_scope\ntf.compat.v1.zeros\ntf.compat.v1.zeros_initializer\ntf.compat.v1.zeros_like\ntf.compat.v1.zeta\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.IndexedSlices  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/IndexedSlices",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nAttributes\nMethods\nconsumers\n__neg__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.IndexedSlices \nbookmark_border\n\nView source on GitHub\n\nA sparse representation of a set of tensor slices at given indices.\n\nView aliases\ntf.IndexedSlices(\n    values, indices, dense_shape=None\n)\n\nUsed in the notebooks\nUsed in the tutorials\n\nClient-efficient large-model federated learning via `federated_select` and sparse aggregation\n\nThis class is a simple wrapper for a pair of Tensor objects:\n\nvalues: A Tensor of any dtype with shape [D0, D1, ..., Dn].\nindices: A 1-D integer Tensor with shape [D0].\n\nAn IndexedSlices is typically used to represent a subset of a larger tensor dense of shape [LARGE0, D1, .. , DN] where LARGE0 >> D0. The values in indices are the indices in the first dimension of the slices that have been extracted from the larger tensor.\n\nThe dense tensor dense represented by an IndexedSlices slices has\n\ndense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]\n\n\nThe IndexedSlices class is used principally in the definition of gradients for operations that have sparse gradients (e.g. tf.gather).\n\nv = tf.Variable([[0.,1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]])\nwith tf.GradientTape() as tape:\n  r = tf.gather(v, [1,3])\nindex_slices = tape.gradient(r,v)\nindex_slices\n<...IndexedSlices object ...>\nindex_slices.indices.numpy()\narray([1, 3], dtype=int32)\nindex_slices.values.numpy()\narray([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)\n\n\nContrast this representation with tf.sparse.SparseTensor, which uses multi-dimensional indices and scalar values.\n\nAttributes\n\ndense_shape\tA 1-D Tensor containing the shape of the corresponding dense tensor.\ndevice\tThe name of the device on which values will be produced, or None.\ndtype\tThe DType of elements in this tensor.\ngraph\tThe Graph that contains the values, indices, and shape tensors.\nindices\tA 1-D Tensor containing the indices of the slices.\nname\tThe name of this IndexedSlices.\nop\tThe Operation that produces values as an output.\nshape\tGets the tf.TensorShape representing the shape of the dense tensor.\nvalues\tA Tensor containing the values of the slices.\n\nMethods\nconsumers\n\nView source\n\nconsumers()\n\n__neg__\n\nView source\n\n__neg__()\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.GradientTape  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/GradientTape",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nUsed in the notebooks\nArgs\nMethods\nbatch_jacobian\ngradient\njacobian\nreset\nstop_recording\nwatch\nwatched_variables\n__enter__\n__exit__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.GradientTape \nbookmark_border\n\nView source on GitHub\n\nRecord operations for automatic differentiation.\n\nView aliases\ntf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n\nUsed in the notebooks\nUsed in the guide\tUsed in the tutorials\n\nIntroduction to gradients and automatic differentiation\nAdvanced automatic differentiation\nBetter performance with tf.function\nTensorFlow basics\nEffective Tensorflow 2\n\t\nDeep Convolutional Generative Adversarial Network\nDeepDream\npix2pix: Image-to-image translation with a conditional GAN\nNeural style transfer\nCustom training: walkthrough\n\nOperations are recorded if they are executed within this context manager and at least one of their inputs is being \"watched\".\n\nTrainable variables (created by tf.Variable or tf.compat.v1.get_variable, where trainable=True is default in both cases) are automatically watched. Tensors can be manually watched by invoking the watch method on this context manager.\n\nFor example, consider the function y = x * x. The gradient at x = 3.0 can be computed as:\n\nx = tf.constant(3.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ndy_dx = g.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\nGradientTapes can be nested to compute higher-order derivatives. For example,\n\nx = tf.constant(5.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  with tf.GradientTape() as gg:\n    gg.watch(x)\n    y = x * x\n  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\nd2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\nprint(dy_dx)\ntf.Tensor(10.0, shape=(), dtype=float32)\nprint(d2y_dx2)\ntf.Tensor(2.0, shape=(), dtype=float32)\n\n\nBy default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:\n\nx = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as g:\n  g.watch(x)\n  y = x * x\n  z = y * y\ndz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\nprint(dz_dx)\ntf.Tensor(108.0, shape=(), dtype=float32)\ndy_dx = g.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\nBy default GradientTape will automatically watch any trainable variables that are accessed inside the context. If you want fine grained control over which variables are watched you can disable automatic tracking by passing watch_accessed_variables=False to the tape constructor:\n\nx = tf.Variable(2.0)\nw = tf.Variable(5.0)\nwith tf.GradientTape(\n    watch_accessed_variables=False, persistent=True) as tape:\n  tape.watch(x)\n  y = x ** 2  # Gradients will be available for `x`.\n  z = w ** 3  # No gradients will be available as `w` isn't being watched.\ndy_dx = tape.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(4.0, shape=(), dtype=float32)\n# No gradients will be available as `w` isn't being watched.\ndz_dw = tape.gradient(z, w)\nprint(dz_dw)\nNone\n\n\nNote that when using models you should ensure that your variables exist when using watch_accessed_variables=False. Otherwise it's quite easy to make your first iteration not have any gradients:\n\na = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\n\n\nNote that only tensors with real or complex dtypes are differentiable.\n\nArgs\n\npersistent\tBoolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object.\nwatch_accessed_variables\tBoolean controlling whether the tape will automatically watch any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable Variable. If False users must explicitly watch any Variables they want to request gradients from.\n\nMethods\nbatch_jacobian\n\nView source\n\nbatch_jacobian(\n    target,\n    source,\n    unconnected_gradients=tf.UnconnectedGradients.NONE,\n    parallel_iterations=None,\n    experimental_use_pfor=True\n)\n\n\nComputes and stacks per-example jacobians.\n\nSee wikipedia article for the definition of a Jacobian. This function is essentially an efficient implementation of the following:\n\ntf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])]).\n\nNote that compared to GradientTape.jacobian which computes gradient of each output value w.r.t each input value, this function is useful when target[i,...] is independent of source[j,...] for j != i. This assumption allows more efficient computation as compared to GradientTape.jacobian. The output, as well as intermediate activations, are lower dimensional and avoid a bunch of redundant zeros which would result in the jacobian computation given the independence assumption.\n\nNote: Unless you set persistent=True a GradientTape can only be used to compute one set of gradients (or jacobians).\nNote: By default the batch_jacobian implementation uses parallel for (pfor), which creates a tf.function under the hood for each batch_jacobian call. For better performance, and to avoid recompilation and vectorization rewrites on each call, enclose GradientTape code in @tf.function.\nExample usage:\nwith tf.GradientTape() as g:\n  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n  g.watch(x)\n  y = x * x\nbatch_jacobian = g.batch_jacobian(y, x)\n# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n\n\nArgs\ntarget\tA tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. target[i,...] should only depend on source[i,...].\nsource\tA tensor with rank 2 or higher and with shape [b, x1, ..., x_m].\nunconnected_gradients\ta value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.\nparallel_iterations\tA knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage.\nexperimental_use_pfor\tIf true, uses pfor for computing the Jacobian. Else uses a tf.while_loop.\n\nReturns\nA tensor t with shape [b, y_1, ..., y_n, x1, ..., x_m] where t[i, ...] is the jacobian of target[i, ...] w.r.t. source[i, ...], i.e. stacked per-example jacobians.\n\nRaises\nRuntimeError\tIf called on a used, non-persistent tape.\nRuntimeError\tIf called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor.\nValueError\tIf vectorization of jacobian computation fails or if first dimension of target and source do not match.\n\ngradient\n\nView source\n\ngradient(\n    target,\n    sources,\n    output_gradients=None,\n    unconnected_gradients=tf.UnconnectedGradients.NONE\n)\n\n\nComputes the gradient using operations recorded in context of this tape.\n\nNote: Unless you set persistent=True a GradientTape can only be used to compute one set of gradients (or jacobians).\n\nIn addition to Tensors, gradient also supports RaggedTensors. For example,\n\nx = tf.ragged.constant([[1.0, 2.0], [3.0]])\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ng.gradient(y, x)\n<tf.RaggedTensor [[2.0, 4.0], [6.0]]>\n\n\nArgs\ntarget\ta list or nested structure of Tensors or Variables or CompositeTensors to be differentiated.\nsources\ta list or nested structure of Tensors or Variables or CompositeTensors. target will be differentiated against elements in sources.\noutput_gradients\ta list of gradients, one for each differentiable element of target. Defaults to None.\nunconnected_gradients\ta value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.\n\nReturns\na list or nested structure of Tensors (or IndexedSlices, or None, or CompositeTensor), one for each element in sources. Returned structure is the same as the structure of sources.\n\nRaises\nRuntimeError\tIf called on a used, non-persistent tape.\nRuntimeError\tIf called inside the context of the tape.\nTypeError\tIf the target is a None object.\nValueError\tIf the target is a variable or if unconnected gradients is called with an unknown value.\n\njacobian\n\nView source\n\njacobian(\n    target,\n    sources,\n    unconnected_gradients=tf.UnconnectedGradients.NONE,\n    parallel_iterations=None,\n    experimental_use_pfor=True\n)\n\n\nComputes the jacobian using operations recorded in context of this tape.\n\nNote: Unless you set persistent=True a GradientTape can only be used to compute one set of gradients (or jacobians).\nNote: By default the jacobian implementation uses parallel for (pfor), which creates a tf.function under the hood for each jacobian call. For better performance, and to avoid recompilation and vectorization rewrites on each call, enclose GradientTape code in @tf.function.\n\nSeewikipedia article for the definition of a Jacobian.\n\nExample usage:\nwith tf.GradientTape() as g:\n  x  = tf.constant([1.0, 2.0])\n  g.watch(x)\n  y = x * x\njacobian = g.jacobian(y, x)\n# jacobian value is [[2., 0.], [0., 4.]]\n\n\nArgs\ntarget\tTensor to be differentiated.\nsources\ta list or nested structure of Tensors or Variables. target will be differentiated against elements in sources.\nunconnected_gradients\ta value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.\nparallel_iterations\tA knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage.\nexperimental_use_pfor\tIf true, vectorizes the jacobian computation. Else falls back to a sequential while_loop. Vectorization can sometimes fail or lead to excessive memory usage. This option can be used to disable vectorization in such cases.\n\nReturns\nA list or nested structure of Tensors (or None), one for each element in sources. Returned structure is the same as the structure of sources. Note if any gradient is sparse (IndexedSlices), jacobian function currently makes it dense and returns a Tensor instead. This may change in the future.\n\nRaises\nRuntimeError\tIf called on a used, non-persistent tape.\nRuntimeError\tIf called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor.\nValueError\tIf vectorization of jacobian computation fails.\n\nreset\n\nView source\n\nreset()\n\n\nClears all information stored in this tape.\n\nEquivalent to exiting and reentering the tape context manager with a new tape. For example, the two following code blocks are equivalent:\n\nwith tf.GradientTape() as t:\n  loss = loss_fn()\nwith tf.GradientTape() as t:\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n\n\n# The following is equivalent to the above\nwith tf.GradientTape() as t:\n  loss = loss_fn()\n  t.reset()\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n\n\nThis is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:\n\nwith tf.GradientTape() as t:\n  loss = ...\n  if loss > k:\n    t.reset()\n\nstop_recording\n\nView source\n\n@tf_contextlib.contextmanager\nstop_recording()\n\n\nTemporarily stops recording operations on this tape.\n\nOperations executed while this context manager is active will not be recorded on the tape. This is useful for reducing the memory used by tracing all computations.\n\nFor example:\nx = tf.constant(4.0)\nwith tf.GradientTape() as tape:\n  with tape.stop_recording():\n    y = x ** 2\ndy_dx = tape.gradient(y, x)\nprint(dy_dx)\nNone\n\n\nYields\nNone\n\nRaises\nRuntimeError\tif the tape is not currently recording.\n\nwatch\n\nView source\n\nwatch(\n    tensor\n)\n\n\nEnsures that tensor is being traced by this tape.\n\nArgs\ntensor\ta Tensor/Variable or list of Tensors/Variables.\n\nRaises\nValueError\tif it encounters something that is not a tensor.\n\nwatched_variables\n\nView source\n\nwatched_variables()\n\n\nReturns variables watched by this tape in order of construction.\n\n__enter__\n\nView source\n\n__enter__()\n\n\nEnters a context inside which operations are recorded on this tape.\n\n__exit__\n\nView source\n\n__exit__(\n    typ, value, traceback\n)\n\n\nExits the recording context, no further operations are traced.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.DeviceSpec  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/DeviceSpec",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nArgs\nAttributes\nMethods\nfrom_string\nmake_merged_spec\nparse_from_string\nreplace\nto_string\n__eq__\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.DeviceSpec \nbookmark_border\n\nView source on GitHub\n\nRepresents a (possibly partial) specification for a TensorFlow device.\n\ntf.DeviceSpec(\n    job=None, replica=None, task=None, device_type=None, device_index=None\n)\n\n\nDeviceSpecs are used throughout TensorFlow to describe where state is stored and computations occur. Using DeviceSpec allows you to parse device spec strings to verify their validity, merge them or compose them programmatically.\n\nExample:\n# Place the operations on device \"GPU:0\" in the \"ps\" job.\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(device_spec.to_string()):\n  # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n\n\nWith eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used:\n\ntf.compat.v1.disable_eager_execution()\n\n# Same as previous\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n# No need of .to_string() method.\nwith tf.device(device_spec):\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n\n\nIf a DeviceSpec is partially specified, it will be merged with other DeviceSpecs according to the scope in which it is defined. DeviceSpec components defined in inner scopes take precedence over those defined in outer scopes.\n\ngpu0_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(DeviceSpec(job=\"train\").to_string()):\n  with tf.device(gpu0_spec.to_string()):\n    # Nodes created here will be assigned to /job:ps/device:GPU:0.\n  with tf.device(DeviceSpec(device_type=\"GPU\", device_index=1).to_string()):\n    # Nodes created here will be assigned to /job:train/device:GPU:1.\n\n\nA DeviceSpec consists of 5 components -- each of which is optionally specified:\n\nJob: The job name.\nReplica: The replica index.\nTask: The task index.\nDevice type: The device type string (e.g. \"CPU\" or \"GPU\").\nDevice index: The device index.\n\nArgs\n\njob\tstring. Optional job name.\nreplica\tint. Optional replica index.\ntask\tint. Optional task index.\ndevice_type\tOptional device type string (e.g. \"CPU\" or \"GPU\")\ndevice_index\tint. Optional device index. If left unspecified, device represents 'any' device_index.\n\nAttributes\n\ndevice_index\t\n\n\ndevice_type\t\n\n\njob\t\n\n\nreplica\t\n\n\ntask\t\n\nMethods\nfrom_string\n\nView source\n\n@classmethod\nfrom_string(\n    spec\n)\n\n\nConstruct a DeviceSpec from a string.\n\nArgs\nspec\ta string of the form /job:/replica:/task:/device:CPU: or /job:/replica:/task:/device:GPU: as cpu and gpu are mutually exclusive. All entries are optional.\n\nReturns\nA DeviceSpec.\n\nmake_merged_spec\n\nView source\n\nmake_merged_spec(\n    dev\n)\n\n\nReturns a new DeviceSpec which incorporates dev.\n\nWhen combining specs, dev will take precedence over the current spec. So for instance:\n\nfirst_spec = tf.DeviceSpec(job=0, device_type=\"CPU\")\nsecond_spec = tf.DeviceSpec(device_type=\"GPU\")\ncombined_spec = first_spec.make_merged_spec(second_spec)\n\n\nis equivalent to:\n\ncombined_spec = tf.DeviceSpec(job=0, device_type=\"GPU\")\n\n\nArgs\ndev\ta DeviceSpec\n\nReturns\nA new DeviceSpec which combines self and dev\n\nparse_from_string\n\nView source\n\nparse_from_string(\n    spec\n)\n\n\nParse a DeviceSpec name into its components.\n\n2.x behavior change:\n\nIn TensorFlow 1.x, this function mutates its own state and returns itself. In 2.x, DeviceSpecs are immutable, and this function will return a DeviceSpec which contains the spec.\n\nRecommended:\n\n# my_spec and my_updated_spec are unrelated.\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_updated_spec = tf.DeviceSpec.from_string(\"/GPU:0\")\nwith tf.device(my_updated_spec):\n  ...\n\n\nWill work in 1.x and 2.x (though deprecated in 2.x):\n\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_updated_spec = my_spec.parse_from_string(\"/GPU:0\")\nwith tf.device(my_updated_spec):\n  ...\n\n\nWill NOT work in 2.x:\n\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_spec.parse_from_string(\"/GPU:0\")  # <== Will not update my_spec\nwith tf.device(my_spec):\n  ...\n\n\nIn general, DeviceSpec.from_string should completely replace DeviceSpec.parse_from_string, and DeviceSpec.replace should completely replace setting attributes directly.\n\nArgs\nspec\tan optional string of the form /job:/replica:/task:/device:CPU: or /job:/replica:/task:/device:GPU: as cpu and gpu are mutually exclusive. All entries are optional.\n\nReturns\nThe DeviceSpec.\n\nRaises\nValueError\tif the spec was not valid.\n\nreplace\n\nView source\n\nreplace(\n    **kwargs\n)\n\n\nConvenience method for making a new DeviceSpec by overriding fields.\n\nFor instance:\nmy_spec = DeviceSpec=(job=\"my_job\", device=\"CPU\")\nmy_updated_spec = my_spec.replace(device=\"GPU\")\nmy_other_spec = my_spec.replace(device=None)\n\n\nArgs\n**kwargs\tThis method takes the same args as the DeviceSpec constructor\n\nReturns\nA DeviceSpec with the fields specified in kwargs overridden.\n\nto_string\n\nView source\n\nto_string()\n\n\nReturn a string representation of this DeviceSpec.\n\nReturns\na string of the form /job:/replica:/task:/device::.\n\n__eq__\n\nView source\n\n__eq__(\n    other\n)\n\n\nChecks if the other DeviceSpec is same as the current instance, eg have\n\nsame value for all the internal fields.\n\nArgs\nother\tAnother DeviceSpec\n\nReturns\nReturn True if other is also a DeviceSpec instance and has same value as the current instance. Return False otherwise.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.CriticalSection  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/CriticalSection",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nAttributes\nMethods\nexecute\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.CriticalSection \nbookmark_border\n\nView source on GitHub\n\nCritical section.\n\nView aliases\ntf.CriticalSection(\n    name=None, shared_name=None, critical_section_def=None, import_scope=None\n)\n\n\nA CriticalSection object is a resource in the graph which executes subgraphs in serial order. A common example of a subgraph one may wish to run exclusively is the one given by the following function:\n\nv = resource_variable_ops.ResourceVariable(0.0, name=\"v\")\n\ndef count():\n  value = v.read_value()\n  with tf.control_dependencies([value]):\n    with tf.control_dependencies([v.assign_add(1)]):\n      return tf.identity(value)\n\n\nHere, a snapshot of v is captured in value; and then v is updated. The snapshot value is returned.\n\nIf multiple workers or threads all execute count in parallel, there is no guarantee that access to the variable v is atomic at any point within any thread's calculation of count. In fact, even implementing an atomic counter that guarantees that the user will see each value 0, 1, ..., is currently impossible.\n\nThe solution is to ensure any access to the underlying resource v is only processed through a critical section:\n\ncs = CriticalSection()\nf1 = cs.execute(count)\nf2 = cs.execute(count)\noutput = f1 + f2\nsession.run(output)\n\n\nThe functions f1 and f2 will be executed serially, and updates to v will be atomic.\n\nNOTES\n\nAll resource objects, including the critical section and any captured variables of functions executed on that critical section, will be colocated to the same device (host and cpu/gpu).\n\nWhen using multiple critical sections on the same resources, there is no guarantee of exclusive access to those resources. This behavior is disallowed by default (but see the kwarg exclusive_resource_access).\n\nFor example, running the same function in two separate critical sections will not ensure serial execution:\n\nv = tf.compat.v1.get_variable(\"v\", initializer=0.0, use_resource=True)\ndef accumulate(up):\n  x = v.read_value()\n  with tf.control_dependencies([x]):\n    with tf.control_dependencies([v.assign_add(up)]):\n      return tf.identity(x)\nex1 = CriticalSection().execute(\n  accumulate, 1.0, exclusive_resource_access=False)\nex2 = CriticalSection().execute(\n  accumulate, 1.0, exclusive_resource_access=False)\nbad_sum = ex1 + ex2\nsess.run(v.initializer)\nsess.run(bad_sum)  # May return 0.0\n\n\nAttributes\n\nname\t\n\nMethods\nexecute\n\nView source\n\nexecute(\n    fn, exclusive_resource_access=True, name=None\n)\n\n\nExecute function fn() inside the critical section.\n\nfn should not accept any arguments. To add extra arguments to when calling fn in the critical section, create a lambda:\n\ncritical_section.execute(lambda: fn(*my_args, **my_kwargs))\n\n\nArgs\nfn\tThe function to execute. Must return at least one tensor.\nexclusive_resource_access\tWhether the resources required by fn should be exclusive to this CriticalSection. Default: True. You may want to set this to False if you will be accessing a resource in read-only mode in two different CriticalSections.\nname\tThe name to use when creating the execute operation.\n\nReturns\nThe tensors returned from fn().\n\nRaises\nValueError\tIf fn attempts to lock this CriticalSection in any nested or lazy way that may cause a deadlock.\nValueError\tIf exclusive_resource_access == True and another CriticalSection has an execution requesting the same resources as fn. Note, even ifexclusive_resource_accessisTrue, if another execution in anotherCriticalSectionwas created withoutexclusive_resource_access=True, aValueError` will be raised.\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "tf.AggregationMethod  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf/AggregationMethod",
    "html": "Install\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\nOverview\nAggregationMethod\nCriticalSection\nDeviceSpec\nGradientTape\nGraph\nIndexedSlices\nIndexedSlicesSpec\nModule\nOperation\nOptionalSpec\nRaggedTensor\nRaggedTensorSpec\nRegisterGradient\nSparseTensorSpec\nTensor\nTensorArray\nTensorArraySpec\nTensorShape\nTensorSpec\nTypeSpec\nUnconnectedGradients\nVariable\nVariable.SaveSliceInfo\nVariableAggregation\nVariableSynchronization\napprox_top_k\nargsort\nbatch_to_space\nbitcast\nboolean_mask\nbroadcast_dynamic_shape\nbroadcast_static_shape\nbroadcast_to\ncase\ncast\nclip_by_global_norm\nclip_by_norm\nclip_by_value\nconcat\ncond\nconstant\nconstant_initializer\ncontrol_dependencies\nconv\nconv2d_backprop_filter_v2\nconv2d_backprop_input_v2\nconvert_to_tensor\ncustom_gradient\ndevice\ndynamic_partition\ndynamic_stitch\nedit_distance\neinsum\nensure_shape\nexecuting_eagerly\nexpand_dims\nextract_volume_patches\neye\nfftnd\nfill\nfingerprint\nfoldl\nfoldr\nfunction\ngather\ngather_nd\nget_current_name_scope\nget_logger\nget_static_value\ngrad_pass_through\ngradients\ngroup\nguarantee_const\nhessians\nhistogram_fixed_width\nhistogram_fixed_width_bins\nidentity\nidentity_n\nifftnd\ninit_scope\ninside_function\nirfftnd\nis_symbolic_tensor\nis_tensor\nlinspace\nload_library\nload_op_library\nmake_ndarray\nmake_tensor_proto\nmap_fn\nmeshgrid\nname_scope\nno_gradient\nno_op\nnondifferentiable_batch_function\nnorm\nnumpy_function\none_hot\nones\nones_initializer\nones_like\npad\nparallel_stack\nprint\npy_function\nragged_fill_empty_rows\nragged_fill_empty_rows_grad\nrandom_index_shuffle\nrandom_normal_initializer\nrandom_uniform_initializer\nrange\nrank\nrealdiv\nrecompute_grad\nregister_tensor_conversion_function\nrepeat\nrequired_space_to_batch_paddings\nreshape\nreverse\nreverse_sequence\nrfftnd\nroll\nscan\nscatter_nd\nsearchsorted\nsequence_mask\nshape\nshape_n\nsize\nslice\nsort\nspace_to_batch\nspace_to_batch_nd\nsplit\nsqueeze\nstack\nstop_gradient\nstrided_slice\nswitch_case\ntensor_scatter_nd_add\ntensor_scatter_nd_max\ntensor_scatter_nd_min\ntensor_scatter_nd_sub\ntensor_scatter_nd_update\ntensordot\ntile\ntimestamp\ntranspose\ntruncatediv\ntruncatemod\ntuple\ntype_spec_from_value\nunique\nunique_with_counts\nunravel_index\nunstack\nvariable_creator_scope\nvectorized_map\nwhere\nwhile_loop\nzeros\nzeros_initializer\nzeros_like\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nClass Variables\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\ntf.AggregationMethod \nbookmark_border\n\nView source on GitHub\n\nA class listing aggregation methods used to combine gradients.\n\nView aliases\n\nComputing partial derivatives can require aggregating gradient contributions. This class lists the various methods that can be used to combine gradients in the graph.\n\nThe following aggregation methods are part of the stable API for aggregating gradients:\n\nADD_N: All of the gradient terms are summed as part of one operation using the \"AddN\" op (see tf.add_n). This method has the property that all gradients must be ready and buffered separately in memory before any aggregation is performed.\nDEFAULT: The system-chosen default aggregation method.\n\nThe following aggregation methods are experimental and may not be supported in future releases:\n\nEXPERIMENTAL_TREE: Gradient terms are summed in pairs using the \"AddN\" op. This method of summing gradients may reduce performance, but it can improve memory utilization because the gradients can be released earlier.\nEXPERIMENTAL_ACCUMULATE_N: Same as EXPERIMENTAL_TREE.\n\nExample usage when computing gradient:\n\n@tf.function\ndef example():\n  x = tf.constant(1.0)\n  y = x * 2.0\n  z = y + y + y + y\n  return tf.gradients(z, [x, y],\n    aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\nexample()\n[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=4.0>]\n\n\nClass Variables\n\nADD_N\t0\nDEFAULT\t0\nEXPERIMENTAL_ACCUMULATE_N\t2\nEXPERIMENTAL_TREE\t1\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  },
  {
    "title": "Module: tf  |  TensorFlow v2.16.1",
    "url": "https://www.tensorflow.org/api_docs/python/tf",
    "html": "Loading [MathJax]/jax/output/SVG/jax.js\nInstall\nLearn\nAPI\nEcosystem\nCommunity\nWhy TensorFlow\nGitHub\nSign in\nTensorFlow v2.16.1\nOverview\nPython\nC++\nJava\nMore\nOverview\nAll Symbols\nPython v2.16.1\ntf\ntf.audio\ntf.autodiff\ntf.autograph\ntf.bitwise\ntf.compat\ntf.config\ntf.data\ntf.debugging\ntf.distribute\ntf.dtypes\ntf.errors\ntf.experimental\ntf.feature_column\ntf.graph_util\ntf.image\ntf.io\ntf.keras\ntf.linalg\ntf.lite\ntf.lookup\ntf.math\ntf.mlir\ntf.nest\ntf.nn\ntf.profiler\ntf.quantization\ntf.queue\ntf.ragged\ntf.random\ntf.raw_ops\ntf.saved_model\ntf.sets\ntf.signal\ntf.sparse\ntf.strings\ntf.summary\ntf.sysconfig\ntf.test\ntf.tpu\ntf.train\ntf.types\ntf.version\ntf.xla\nOn this page\nTensorFlow\nModules\nClasses\nFunctions\nOther Members\nTensorFlow is back at Google I/O on May 14! Register now\nTensorFlow\nAPI\nTensorFlow v2.16.1\nPython\nWas this helpful?\nModule: tf \nbookmark_border\n\nView source on GitHub\nTensorFlow\npip install tensorflow\n\nModules\n\naudio module: Public API for tf._api.v2.audio namespace\n\nautodiff module: Public API for tf._api.v2.autodiff namespace\n\nautograph module: Public API for tf._api.v2.autograph namespace\n\nbitwise module: Public API for tf._api.v2.bitwise namespace\n\ncompat module: Public API for tf._api.v2.compat namespace\n\nconfig module: Public API for tf._api.v2.config namespace\n\ndata module: Public API for tf._api.v2.data namespace\n\ndebugging module: Public API for tf._api.v2.debugging namespace\n\ndistribute module: Public API for tf._api.v2.distribute namespace\n\ndtypes module: Public API for tf._api.v2.dtypes namespace\n\nerrors module: Public API for tf._api.v2.errors namespace\n\nexperimental module: Public API for tf._api.v2.experimental namespace\n\nfeature_column module: Public API for tf._api.v2.feature_column namespace\n\ngraph_util module: Public API for tf._api.v2.graph_util namespace\n\nimage module: Public API for tf._api.v2.image namespace\n\nio module: Public API for tf._api.v2.io namespace\n\nkeras module: DO NOT EDIT.\n\nlinalg module: Public API for tf._api.v2.linalg namespace\n\nlite module: Public API for tf._api.v2.lite namespace\n\nlookup module: Public API for tf._api.v2.lookup namespace\n\nmath module: Public API for tf._api.v2.math namespace\n\nmlir module: Public API for tf._api.v2.mlir namespace\n\nnest module: Public API for tf._api.v2.nest namespace\n\nnn module: Public API for tf._api.v2.nn namespace\n\nprofiler module: Public API for tf._api.v2.profiler namespace\n\nquantization module: Public API for tf._api.v2.quantization namespace\n\nqueue module: Public API for tf._api.v2.queue namespace\n\nragged module: Public API for tf._api.v2.ragged namespace\n\nrandom module: Public API for tf._api.v2.random namespace\n\nraw_ops module: Public API for tf._api.v2.raw_ops namespace\n\nsaved_model module: Public API for tf._api.v2.saved_model namespace\n\nsets module: Public API for tf._api.v2.sets namespace\n\nsignal module: Public API for tf._api.v2.signal namespace\n\nsparse module: Public API for tf._api.v2.sparse namespace\n\nstrings module: Public API for tf._api.v2.strings namespace\n\nsummary module: Public API for tf._api.v2.summary namespace\n\nsysconfig module: Public API for tf._api.v2.sysconfig namespace\n\ntest module: Public API for tf._api.v2.test namespace\n\ntpu module: Public API for tf._api.v2.tpu namespace\n\ntrain module: Public API for tf._api.v2.train namespace\n\ntypes module: Public API for tf._api.v2.types namespace\n\nversion module: Public API for tf._api.v2.version namespace\n\nxla module: Public API for tf._api.v2.xla namespace\n\nClasses\n\nclass AggregationMethod: A class listing aggregation methods used to combine gradients.\n\nclass CriticalSection: Critical section.\n\nclass DType: Represents the type of the elements in a Tensor.\n\nclass DeviceSpec: Represents a (possibly partial) specification for a TensorFlow device.\n\nclass GradientTape: Record operations for automatic differentiation.\n\nclass Graph: A TensorFlow computation, represented as a dataflow graph.\n\nclass IndexedSlices: A sparse representation of a set of tensor slices at given indices.\n\nclass IndexedSlicesSpec: Type specification for a tf.IndexedSlices.\n\nclass Module: Base neural network module class.\n\nclass Operation: Represents a graph node that performs computation on tensors.\n\nclass OptionalSpec: Type specification for tf.experimental.Optional.\n\nclass RaggedTensor: Represents a ragged tensor.\n\nclass RaggedTensorSpec: Type specification for a tf.RaggedTensor.\n\nclass RegisterGradient: A decorator for registering the gradient function for an op type.\n\nclass SparseTensor: Represents a sparse tensor.\n\nclass SparseTensorSpec: Type specification for a tf.sparse.SparseTensor.\n\nclass Tensor: A tf.Tensor represents a multidimensional array of elements.\n\nclass TensorArray: Class wrapping dynamic-sized, per-time-step, Tensor arrays.\n\nclass TensorArraySpec: Type specification for a tf.TensorArray.\n\nclass TensorShape: Represents the shape of a Tensor.\n\nclass TensorSpec: Describes the type of a tf.Tensor.\n\nclass TypeSpec: Specifies a TensorFlow value type.\n\nclass UnconnectedGradients: Controls how gradient computation behaves when y does not depend on x.\n\nclass Variable: See the variable guide.\n\nclass VariableAggregation: Indicates how a distributed variable will be aggregated.\n\nclass VariableSynchronization: Indicates when a distributed variable will be synced.\n\nclass constant_initializer: Initializer that generates tensors with constant values.\n\nclass name_scope: A context manager for use when defining a Python op.\n\nclass ones_initializer: Initializer that generates tensors initialized to 1.\n\nclass random_normal_initializer: Initializer that generates tensors with a normal distribution.\n\nclass random_uniform_initializer: Initializer that generates tensors with a uniform distribution.\n\nclass zeros_initializer: Initializer that generates tensors initialized to 0.\n\nFunctions\n\nAssert(...): Asserts that the given condition is true.\n\nabs(...): Computes the absolute value of a tensor.\n\nacos(...): Computes acos of x element-wise.\n\nacosh(...): Computes inverse hyperbolic cosine of x element-wise.\n\nadd(...): Returns x + y element-wise.\n\nadd_n(...): Returns the element-wise sum of a list of tensors.\n\napprox_top_k(...): Returns min/max k values and their indices of the input operand in an approximate manner.\n\nargmax(...): Returns the index with the largest value across axes of a tensor.\n\nargmin(...): Returns the index with the smallest value across axes of a tensor.\n\nargsort(...): Returns the indices of a tensor that give its sorted order along an axis.\n\nas_dtype(...): Converts the given type_value to a tf.DType.\n\nas_string(...): Converts each entry in the given tensor to strings.\n\nasin(...): Computes the trignometric inverse sine of x element-wise.\n\nasinh(...): Computes inverse hyperbolic sine of x element-wise.\n\nassert_equal(...): Assert the condition x == y holds element-wise.\n\nassert_greater(...): Assert the condition x > y holds element-wise.\n\nassert_less(...): Assert the condition x < y holds element-wise.\n\nassert_rank(...): Assert that x has rank equal to rank.\n\natan(...): Computes the trignometric inverse tangent of x element-wise.\n\natan2(...): Computes arctangent of y/x element-wise, respecting signs of the arguments.\n\natanh(...): Computes inverse hyperbolic tangent of x element-wise.\n\nbatch_to_space(...): BatchToSpace for N-D tensors of type T.\n\nbitcast(...): Bitcasts a tensor from one type to another without copying data.\n\nboolean_mask(...): Apply boolean mask to tensor.\n\nbroadcast_dynamic_shape(...): Computes the shape of a broadcast given symbolic shapes.\n\nbroadcast_static_shape(...): Computes the shape of a broadcast given known shapes.\n\nbroadcast_to(...): Broadcast an array for a compatible shape.\n\ncase(...): Create a case operation.\n\ncast(...): Casts a tensor to a new type.\n\nclip_by_global_norm(...): Clips values of multiple tensors by the ratio of the sum of their norms.\n\nclip_by_norm(...): Clips tensor values to a maximum L2-norm.\n\nclip_by_value(...): Clips tensor values to a specified min and max.\n\ncomplex(...): Converts two real numbers to a complex number.\n\nconcat(...): Concatenates tensors along one dimension.\n\ncond(...): Return true_fn() if the predicate pred is true else false_fn().\n\nconstant(...): Creates a constant tensor from a tensor-like object.\n\ncontrol_dependencies(...): Wrapper for Graph.control_dependencies() using the default graph.\n\nconv(...): Computes a N-D convolution given (N+1+batch_dims)-D input and (N+2)-D filter tensors.\n\nconv2d_backprop_filter_v2(...): Computes the gradients of convolution with respect to the filter.\n\nconv2d_backprop_input_v2(...): Computes the gradients of convolution with respect to the input.\n\nconvert_to_tensor(...): Converts the given value to a Tensor.\n\ncos(...): Computes cos of x element-wise.\n\ncosh(...): Computes hyperbolic cosine of x element-wise.\n\ncumsum(...): Compute the cumulative sum of the tensor x along axis.\n\ncustom_gradient(...): Decorator to define a function with a custom gradient.\n\ndevice(...): Specifies the device for ops created/executed in this context.\n\ndivide(...): Computes Python style division of x by y.\n\ndynamic_partition(...): Partitions data into num_partitions tensors using indices from partitions.\n\ndynamic_stitch(...): Interleave the values from the data tensors into a single tensor.\n\nedit_distance(...): Computes the Levenshtein distance between sequences.\n\neig(...): Computes the eigen decomposition of a batch of matrices.\n\neigvals(...): Computes the eigenvalues of one or more matrices.\n\neinsum(...): Tensor contraction over specified indices and outer product.\n\nensure_shape(...): Updates the shape of a tensor and checks at runtime that the shape holds.\n\nequal(...): Returns the truth value of (x == y) element-wise.\n\nexecuting_eagerly(...): Checks whether the current thread has eager execution enabled.\n\nexp(...): Computes exponential of x element-wise. y=ex.\n\nexpand_dims(...): Returns a tensor with a length 1 axis inserted at index axis.\n\nextract_volume_patches(...): Extract patches from input and put them in the \"depth\" output dimension. 3D extension of extract_image_patches.\n\neye(...): Construct an identity matrix, or a batch of matrices.\n\nfftnd(...): ND fast Fourier transform.\n\nfill(...): Creates a tensor filled with a scalar value.\n\nfingerprint(...): Generates fingerprint values.\n\nfloor(...): Returns element-wise largest integer not greater than x.\n\nfoldl(...): foldl on the list of tensors unpacked from elems on dimension 0. (deprecated argument values)\n\nfoldr(...): foldr on the list of tensors unpacked from elems on dimension 0. (deprecated argument values)\n\nfunction(...): Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments)\n\ngather(...): Gather slices from params axis axis according to indices. (deprecated arguments)\n\ngather_nd(...): Gather slices from params into a Tensor with shape specified by indices.\n\nget_current_name_scope(...): Returns current full name scope specified by tf.name_scope(...)s.\n\nget_logger(...): Return TF logger instance.\n\nget_static_value(...): Returns the constant value of the given tensor, if efficiently calculable.\n\ngrad_pass_through(...): Creates a grad-pass-through op with the forward behavior provided in f.\n\ngradients(...): Constructs symbolic derivatives of sum of ys w.r.t. x in xs.\n\ngreater(...): Returns the truth value of (x > y) element-wise.\n\ngreater_equal(...): Returns the truth value of (x >= y) element-wise.\n\ngroup(...): Create an op that groups multiple operations.\n\nguarantee_const(...): Promise to the TF runtime that the input tensor is a constant. (deprecated)\n\nhessians(...): Constructs the Hessian of sum of ys with respect to x in xs.\n\nhistogram_fixed_width(...): Return histogram of values.\n\nhistogram_fixed_width_bins(...): Bins the given values for use in a histogram.\n\nidentity(...): Return a Tensor with the same shape and contents as input.\n\nidentity_n(...): Returns a list of tensors with the same shapes and contents as the input\n\nifftnd(...): ND inverse fast Fourier transform.\n\nimport_graph_def(...): Imports the graph from graph_def into the current default Graph. (deprecated arguments)\n\ninit_scope(...): A context manager that lifts ops out of control-flow scopes and function-building graphs.\n\ninside_function(...): Indicates whether the caller code is executing inside a tf.function.\n\nirfftnd(...): ND inverse real fast Fourier transform.\n\nis_symbolic_tensor(...): Test if tensor is a symbolic Tensor.\n\nis_tensor(...): Checks whether x is a TF-native type that can be passed to many TF ops.\n\nless(...): Returns the truth value of (x < y) element-wise.\n\nless_equal(...): Returns the truth value of (x <= y) element-wise.\n\nlinspace(...): Generates evenly-spaced values in an interval along a given axis.\n\nload_library(...): Loads a TensorFlow plugin.\n\nload_op_library(...): Loads a TensorFlow plugin, containing custom ops and kernels.\n\nlogical_and(...): Returns the truth value of x AND y element-wise.\n\nlogical_not(...): Returns the truth value of NOT x element-wise.\n\nlogical_or(...): Returns the truth value of x OR y element-wise.\n\nmake_ndarray(...): Create a numpy ndarray from a tensor.\n\nmake_tensor_proto(...): Create a TensorProto.\n\nmap_fn(...): Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments)\n\nmatmul(...): Multiplies matrix a by matrix b, producing a * b.\n\nmatrix_square_root(...): Computes the matrix square root of one or more square matrices:\n\nmaximum(...): Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n\nmeshgrid(...): Broadcasts parameters for evaluation on an N-D grid.\n\nminimum(...): Returns the min of x and y (i.e. x < y ? x : y) element-wise.\n\nmultiply(...): Returns an element-wise x * y.\n\nnegative(...): Computes numerical negative value element-wise.\n\nno_gradient(...): Specifies that ops of type op_type is not differentiable.\n\nno_op(...): Does nothing. Only useful as a placeholder for control edges.\n\nnondifferentiable_batch_function(...): Batches the computation done by the decorated function.\n\nnorm(...): Computes the norm of vectors, matrices, and tensors.\n\nnot_equal(...): Returns the truth value of (x != y) element-wise.\n\nnumpy_function(...): Wraps a python function and uses it as a TensorFlow op.\n\none_hot(...): Returns a one-hot tensor.\n\nones(...): Creates a tensor with all elements set to one (1).\n\nones_like(...): Creates a tensor of all ones that has the same shape as the input.\n\npad(...): Pads a tensor.\n\nparallel_stack(...): Stacks a list of rank-R tensors into one rank-(R+1) tensor in parallel.\n\npow(...): Computes the power of one value to another.\n\nprint(...): Print the specified inputs.\n\npy_function(...): Wraps a python function into a TensorFlow op that executes it eagerly.\n\nragged_fill_empty_rows(...)\n\nragged_fill_empty_rows_grad(...)\n\nrandom_index_shuffle(...): Outputs the position of value in a permutation of [0, ..., max_index].\n\nrange(...): Creates a sequence of numbers.\n\nrank(...): Returns the rank of a tensor.\n\nrealdiv(...): Returns x / y element-wise for real types.\n\nrecompute_grad(...): Defines a function as a recompute-checkpoint for the tape auto-diff.\n\nreduce_all(...): Computes tf.math.logical_and of elements across dimensions of a tensor.\n\nreduce_any(...): Computes tf.math.logical_or of elements across dimensions of a tensor.\n\nreduce_logsumexp(...): Computes log(sum(exp(elements across dimensions of a tensor))).\n\nreduce_max(...): Computes tf.math.maximum of elements across dimensions of a tensor.\n\nreduce_mean(...): Computes the mean of elements across dimensions of a tensor.\n\nreduce_min(...): Computes the tf.math.minimum of elements across dimensions of a tensor.\n\nreduce_prod(...): Computes tf.math.multiply of elements across dimensions of a tensor.\n\nreduce_sum(...): Computes the sum of elements across dimensions of a tensor.\n\nregister_tensor_conversion_function(...): Registers a function for converting objects of base_type to Tensor.\n\nrepeat(...): Repeat elements of input.\n\nrequired_space_to_batch_paddings(...): Calculate padding required to make block_shape divide input_shape.\n\nreshape(...): Reshapes a tensor.\n\nreverse(...): Reverses specific dimensions of a tensor.\n\nreverse_sequence(...): Reverses variable length slices.\n\nrfftnd(...): ND fast real Fourier transform.\n\nroll(...): Rolls the elements of a tensor along an axis.\n\nround(...): Rounds the values of a tensor to the nearest integer, element-wise.\n\nsaturate_cast(...): Performs a safe saturating cast of value to dtype.\n\nscalar_mul(...): Multiplies a scalar times a Tensor or IndexedSlices object.\n\nscan(...): scan on the list of tensors unpacked from elems on dimension 0. (deprecated argument values)\n\nscatter_nd(...): Scatters updates into a tensor of shape shape according to indices.\n\nsearchsorted(...): Searches for where a value would go in a sorted sequence.\n\nsequence_mask(...): Returns a mask tensor representing the first N positions of each cell.\n\nshape(...): Returns a tensor containing the shape of the input tensor.\n\nshape_n(...): Returns shape of a list of tensors.\n\nsigmoid(...): Computes sigmoid of x element-wise.\n\nsign(...): Returns an element-wise indication of the sign of a number.\n\nsin(...): Computes sine of x element-wise.\n\nsinh(...): Computes hyperbolic sine of x element-wise.\n\nsize(...): Returns the size of a tensor.\n\nslice(...): Extracts a slice from a tensor.\n\nsort(...): Sorts a tensor.\n\nspace_to_batch(...): SpaceToBatch for N-D tensors of type T.\n\nspace_to_batch_nd(...): SpaceToBatch for N-D tensors of type T.\n\nsplit(...): Splits a tensor value into a list of sub tensors.\n\nsqrt(...): Computes element-wise square root of the input tensor.\n\nsquare(...): Computes square of x element-wise.\n\nsqueeze(...): Removes dimensions of size 1 from the shape of a tensor.\n\nstack(...): Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n\nstop_gradient(...): Stops gradient computation.\n\nstrided_slice(...): Extracts a strided slice of a tensor (generalized Python array indexing).\n\nsubtract(...): Returns x - y element-wise.\n\nswitch_case(...): Create a switch/case operation, i.e.\n\ntan(...): Computes tan of x element-wise.\n\ntanh(...): Computes hyperbolic tangent of x element-wise.\n\ntensor_scatter_nd_add(...): Adds sparse updates to an existing tensor according to indices.\n\ntensor_scatter_nd_max(...): Apply a sparse update to a tensor taking the element-wise maximum.\n\ntensor_scatter_nd_min(...)\n\ntensor_scatter_nd_sub(...): Subtracts sparse updates from an existing tensor according to indices.\n\ntensor_scatter_nd_update(...): Scatter updates into an existing tensor according to indices.\n\ntensordot(...): Tensor contraction of a and b along specified axes and outer product.\n\ntile(...): Constructs a tensor by tiling a given tensor.\n\ntimestamp(...): Provides the time since epoch in seconds.\n\ntranspose(...): Transposes a, where a is a Tensor.\n\ntruediv(...): Divides x / y elementwise (using Python 3 division operator semantics).\n\ntruncatediv(...): Returns x / y element-wise, rounded towards zero.\n\ntruncatemod(...): Returns element-wise remainder of division.\n\ntuple(...): Groups tensors together.\n\ntype_spec_from_value(...): Returns a tf.TypeSpec that represents the given value.\n\nunique(...): Finds unique elements in a 1-D tensor.\n\nunique_with_counts(...): Finds unique elements in a 1-D tensor.\n\nunravel_index(...): Converts an array of flat indices into a tuple of coordinate arrays.\n\nunstack(...): Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n\nvariable_creator_scope(...): Scope which defines a variable creation function to be used by variable().\n\nvectorized_map(...): Parallel map on the list of tensors unpacked from elems on dimension 0.\n\nwhere(...): Returns the indices of non-zero elements, or multiplexes x and y.\n\nwhile_loop(...): Repeat body while the condition cond is true. (deprecated argument values)\n\nzeros(...): Creates a tensor with all elements set to zero.\n\nzeros_like(...): Creates a tensor with all elements set to zero.\n\nOther Members\n\nversion\t'2.16.1'\nbfloat16\tInstance of tf.dtypes.DType\n\n16-bit bfloat (brain floating point).\n\n\nbool\tInstance of tf.dtypes.DType\n\nBoolean.\n\n\ncomplex128\tInstance of tf.dtypes.DType\n\n128-bit complex.\n\n\ncomplex64\tInstance of tf.dtypes.DType\n\n64-bit complex.\n\n\ndouble\tInstance of tf.dtypes.DType\n\n64-bit (double precision) floating-point.\n\n\nfloat16\tInstance of tf.dtypes.DType\n\n16-bit (half precision) floating-point.\n\n\nfloat32\tInstance of tf.dtypes.DType\n\n32-bit (single precision) floating-point.\n\n\nfloat64\tInstance of tf.dtypes.DType\n\n64-bit (double precision) floating-point.\n\n\nhalf\tInstance of tf.dtypes.DType\n\n16-bit (half precision) floating-point.\n\n\nint16\tInstance of tf.dtypes.DType\n\nSigned 16-bit integer.\n\n\nint32\tInstance of tf.dtypes.DType\n\nSigned 32-bit integer.\n\n\nint64\tInstance of tf.dtypes.DType\n\nSigned 64-bit integer.\n\n\nint8\tInstance of tf.dtypes.DType\n\nSigned 8-bit integer.\n\n\nnewaxis\tNone\nqint16\tInstance of tf.dtypes.DType\n\nSigned quantized 16-bit integer.\n\n\nqint32\tInstance of tf.dtypes.DType\n\nsigned quantized 32-bit integer.\n\n\nqint8\tInstance of tf.dtypes.DType\n\nSigned quantized 8-bit integer.\n\n\nquint16\tInstance of tf.dtypes.DType\n\nUnsigned quantized 16-bit integer.\n\n\nquint8\tInstance of tf.dtypes.DType\n\nUnsigned quantized 8-bit integer.\n\n\nresource\tInstance of tf.dtypes.DType\n\nHandle to a mutable, dynamically allocated resource.\n\n\nstring\tInstance of tf.dtypes.DType\n\nVariable-length string, represented as byte array.\n\n\nuint16\tInstance of tf.dtypes.DType\n\nUnsigned 16-bit (word) integer.\n\n\nuint32\tInstance of tf.dtypes.DType\n\nUnsigned 32-bit (dword) integer.\n\n\nuint64\tInstance of tf.dtypes.DType\n\nUnsigned 64-bit (qword) integer.\n\n\nuint8\tInstance of tf.dtypes.DType\n\nUnsigned 8-bit (byte) integer.\n\n\nvariant\tInstance of tf.dtypes.DType\n\nData of arbitrary type (known at runtime).\n\nWas this helpful?\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license.\n\nLast updated 2024-04-26 UTC.\n\nStay connected\nBlog\nForum\nGitHub\nTwitter\nYouTube\nSupport\nIssue tracker\nRelease notes\nStack Overflow\nBrand guidelines\nCite TensorFlow\nTerms\nPrivacy\nSign up for the TensorFlow newsletter\nSubscribe"
  }
]